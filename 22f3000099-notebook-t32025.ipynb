{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114451,"databundleVersionId":13718043,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sklearn IMPORTS","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, TimeSeriesSplit, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression,  Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\nimport seaborn as sns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dummy Reg","metadata":{}},{"cell_type":"code","source":"csv_path = {\n    \"booknow_booking.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_booking/booknow_booking.csv\",\n    \"booknow_theaters.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_theaters/booknow_theaters.csv\",\n    \"booknow_visits.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_visits/booknow_visits.csv\",\n    \"cinePOS_booking.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/cinePOS_booking/cinePOS_booking.csv\",\n    \"cinePOS_theaters.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/cinePOS_theaters/cinePOS_theaters.csv\",\n    \"date_info.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/date_info/date_info.csv\",\n    \"movie_theater_id_relation.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/movie_theater_id_relation/movie_theater_id_relation.csv\",\n    \"sample_submission.csv\": \"/kaggle/input/Cinema_Audience_Forecasting_challenge/sample_submission/sample_submission.csv\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1 = pd.read_csv(csv_path['booknow_booking.csv'])\ndf2 = pd.read_csv(csv_path['booknow_theaters.csv'])\ndf3 = pd.read_csv(csv_path['booknow_visits.csv'])\ndf4 = pd.read_csv(csv_path['cinePOS_booking.csv'])\ndf5 = pd.read_csv(csv_path['cinePOS_theaters.csv'])\ndf6 = pd.read_csv(csv_path['date_info.csv'])\ndf7 = pd.read_csv(csv_path['movie_theater_id_relation.csv'])\ndf8 = pd.read_csv(csv_path['sample_submission.csv'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CRESTION OF DF","metadata":{}},{"cell_type":"code","source":"# df6[\"show_date\"].max()  = '2024-06-30'\n# df6[\"show_date\"].min() = '2023-01-01'\n# df3[\"book_theater_id\"].max() = book_00829\n# df3[\"book_theater_id\"].min() = book_00001\n# so df = 829 booking IDs × 547 dates = 453,463 rows total.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### INITIAL DATA EXPLORATION","metadata":{}},{"cell_type":"code","source":"booknow_booking = pd.read_csv(csv_path['booknow_booking.csv'])\nbooknow_theaters = pd.read_csv(csv_path['booknow_theaters.csv'])\nbooknow_visits = pd.read_csv(csv_path['booknow_visits.csv'])\ncinepos_booking = pd.read_csv(csv_path['cinePOS_booking.csv'])\ncinepos_theaters = pd.read_csv(csv_path['cinePOS_theaters.csv'])\ndate_info = pd.read_csv(csv_path['date_info.csv'])\nmovie_theater_relation = pd.read_csv(csv_path['movie_theater_id_relation.csv'])\nsample_submission = pd.read_csv(csv_path['sample_submission.csv'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"\\nDataset Shapes:\")\nprint(f\"  df1 (BookNow Booking):     {df1.shape}\")\nprint(f\"  df2 (BookNow Theaters):    {df2.shape}\")\nprint(f\"  df3 (BookNow Visits):      {df3.shape} <- TARGET HERE\")\nprint(f\"  df4 (CinePOS Booking):     {df4.shape}\")\nprint(f\"  df5 (CinePOS Theaters):    {df5.shape}\")\nprint(f\"  df6 (Date Info):           {df6.shape}\")\nprint(f\"  df7 (Theater Mapping):     {df7.shape}\")\nprint(f\"  df8 (Sample Submission):   {df8.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EDA\nprint(\"df1: BookNow Booking Data\")\nprint(df1.info())\ndf1.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df2.info())\nprint(f\"\\nUnique theater types: {df2['theater_type'].unique()}\")\nprint(f\"Number of areas: {df2['theater_area'].nunique()}\")\ndf2.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df3.info())\nprint(\"\\nTarget Variable Stats:\")\nprint(df3['audience_count'].describe())\ndf3.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"df4: \")\nprint(\"\\n POS Booking Information\")\nprint(df4.info())\n(df3.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"df6: Date Information\")\nprint(df6.info())\ndf6.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"df7: \")\nprint(\"\\n theater id and POS id Relation\")\nprint(df7.info())\n(df7.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"df8: Submission Format\")\nprint(df8.head(1))\nprint(df8.tail(1))\n# test date = 1 march to 22 april ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Target Variable Analysis","metadata":{}},{"cell_type":"code","source":"# need to check for outliers, distribution patterns, and temporal trends\n#helps in feature engineering\ndf3['show_date'] = pd.to_datetime(df3['show_date'])\nprint(\"\\nBasic Statistics:\")\nprint(df3['audience_count'].describe())\n\nprint(f\"\\nDate Range in Training Data:\")\nprint(f\"From:{df3['show_date'].min()}\")\nprint(f\"To:{df3['show_date'].max()}\")\nprint(f\"Total days:{(df3['show_date'].max() - df3['show_date'].min()).days}\")\n\nprint(f\"\\nNumber of theaters in training: {df3['book_theater_id'].nunique()}\")\nprint(f\"Total records: {len(df3)}\")\nprint(f\"Average records per theater: {len(df3) / df3['book_theater_id'].nunique():.1f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.hist(df3['audience_count'], bins=150, color='yellow', edgecolor='black')\nplt.title(\"Distribution of Audience Count\")\nplt.xlabel(\"Audience Count\")\nplt.ylabel(\"Frequency (How many times it happened)\")\nplt.axvline(df3['audience_count'].mean(), color='red', linestyle='--', label='Average Value (Mean)')\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\ndaily_avg = df3.groupby('show_date')['audience_count'].mean()\nplt.plot(daily_avg.index, daily_avg.values, color='green')\nplt.title(\"Average Audience Over Time\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Average Audience Count\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.boxplot(df3['audience_count'])\nplt.title(\"Boxplot of Audience Count\", fontsize=14, fontweight='bold')\nplt.ylabel(\"Audience Count\")\nplt.xticks([])\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# calculating outlier bond\nQ1 = df3['audience_count'].quantile(0.25)\nQ2 = df3['audience_count'].quantile(0.50)\nQ3 = df3['audience_count'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n\nprint(f\"IQR:{IQR}\")\nprint(f\"lower Bound:{lower_bound}\")\nprint(f\"upper Bound:{upper_bound}\")\n\noutliers = df3[(df3['audience_count'] < lower_bound) | (df3['audience_count'] > upper_bound)]\nprint(f\"Outliers detected: {len(outliers)} rows ({len(outliers)/len(df3)*100}%)\")\nprint(f\"Max audience count: {df3['audience_count'].max()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing value analysis\ndef check_missing(df, name):\n    print(f\"\\n{name}:\")\n    missing = df.isnull().sum()\n    if missing.sum() == 0:\n        print(\"No missing values!\")\n    else:\n        missing_pct = (missing / len(df) * 100).round(2)\n        missing_df = pd.DataFrame({\n            'Count': missing[missing > 0], \n            'Percentage': missing_pct[missing > 0]\n        })\n        print(missing_df)\n\ncheck_missing(df1,\"df1 (BookNow Booking)\")\ncheck_missing(df2,\"df2 (BookNow Theaters)\")\ncheck_missing(df3,\"df3 (BookNow Visits)\")\ncheck_missing(df6,\"df6 (Date Info)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights from basic EDA\n1. df2 has theater metadata (type, area, location)\n2. df3 has our TARGET variable (audience_count)\n3. df6 has calendar information (day of week)\n4. need to predict for 1st march to 22nd April 2024 (both inclusive)\n5. df3 has 214046 rows while df1 has only 68336 \n6. From df3 we know total 826 unique theaters are present and total day are 423\n7. final df need a complete skeleton dataframe of all theater-date combinations\n   skeleton df row = 826 * 423 = 341548\n8. audience count majorly lies between 18 - 59 with upper bound at 118\n9. there are some outlier of audience count present from 118 to 1350 which covers about 2.6% (approx) of total df3\n10. graph of average audinece count shows too much upward or downward trend which means train df needs to introduce weekends, holiday(sunday), lag, roll,\n\n\n### Stratigy\n1. for lat, long empty values will be replace by median\n2. for categorical (theater_area, theater_type) i'll use unknown_area or unknown_type\n3. i'll create ALL possible theater-date combinations (424 days × 829 theaters = 351,496 rows) so finally I'll  have a complete timeline for each theater\n4. Missing dates will help us identify when theaters were closed\n5. Then Merging will be introduced of main df with other other dataframes","metadata":{}},{"cell_type":"code","source":"# skeleton df\n# Create date range\nstart_date = dt.date(2023, 1, 1)\nend_date = dt.date(2024, 2, 28)\ndates = pd.date_range(start=start_date, end=end_date)\n\n# Create theater IDs\nbooking_ids = [f\"book_{i:05d}\" for i in range(1, 830)]\nprint(f\"dates: {len(dates)} days\")\nprint(f\"theaters: {len(booking_ids)}\")\nprint(f\"total combinations: {len(dates) * len(booking_ids)}\")\n\n# Create base dataframe\nf_t_df = pd.DataFrame(\n    [(b, d) for d in dates for b in booking_ids],\n    columns=[\"book_theater_id\", \"show_date\"]\n)\n\nf_t_df[\"show_date\"] = pd.to_datetime(f_t_df[\"show_date\"])\nf_t_df[\"ID\"] = f_t_df[\"book_theater_id\"] + \"_\" + f_t_df[\"show_date\"].dt.strftime(\"%Y-%m-%d\")\n\nprint(f\"base df created: {f_t_df.shape}\")\nprint(f\"columns: {f_t_df.columns.tolist()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge df6 into main df \ndf6['show_date'] = pd.to_datetime(df6['show_date'])\nf_t_df = f_t_df.merge(df6, on='show_date', how='left')\nf_t_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge df3 into main df \nf_t_df = f_t_df.merge(df3, on=['book_theater_id', 'show_date'], how='left')\nprint(f\"rows with audience:{f_t_df['audience_count'].notna().sum()}\")\nprint(f\"rows without audience:{f_t_df['audience_count'].isna().sum()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge df2 into main df \nf_t_df = f_t_df.merge(df2, on='book_theater_id', how='left')\nf_t_df.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f_t_df['latitude'] = f_t_df['latitude'].fillna(f_t_df['latitude'].median())\nf_t_df['longitude'] = f_t_df['longitude'].fillna(f_t_df['longitude'].median())\nf_t_df['theater_area'] = f_t_df['theater_area'].fillna('unknown_area')\nf_t_df['theater_type'] = f_t_df['theater_type'].fillna('unknown_type')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f_t_df.isnull().sum()[f_t_df.isnull().sum() > 0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Date Related","metadata":{}},{"cell_type":"code","source":"# Extract basic date features\nf_t_df['show_day'] = f_t_df['show_date'].dt.day\nf_t_df['show_month'] = f_t_df['show_date'].dt.month\nf_t_df['show_year'] = f_t_df['show_date'].dt.year\nf_t_df['encoded_day_of_week'] = f_t_df['show_date'].dt.dayofweek\nf_t_df['day_of_year'] = f_t_df['show_date'].dt.dayofyear\nf_t_df['week_of_year'] = f_t_df['show_date'].dt.isocalendar().week\nf_t_df['quarter'] = f_t_df['show_date'].dt.quarter\n\n# Create weekend indicators\nf_t_df['is_weekend'] = (f_t_df['encoded_day_of_week'] >= 5).astype(int)\nf_t_df['is_sunday'] = (f_t_df['encoded_day_of_week'] == 6).astype(int)\nf_t_df['is_friday'] = (f_t_df['encoded_day_of_week'] == 4).astype(int)\n\n# Cyclical encodings\nf_t_df['dow_sin'] = np.sin(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)\nf_t_df['dow_cos'] = np.cos(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)\nf_t_df['month_sin'] = np.sin(2 * np.pi * f_t_df['show_month'] / 12)\nf_t_df['month_cos'] = np.cos(2 * np.pi * f_t_df['show_month'] / 12)\nf_t_df['doy_sin'] = np.sin(2 * np.pi * f_t_df['day_of_year'] / 365)\nf_t_df['doy_cos'] = np.cos(2 * np.pi * f_t_df['day_of_year'] / 365)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### categorical ","metadata":{}},{"cell_type":"code","source":"# Get unique values\nunique_theater_type = f_t_df['theater_type'].unique()\nunique_theater_area = f_t_df['theater_area'].unique()\n\n# Create mapping dictionaries\ntheater_type_dict = {}\ntheater_area_dict = {}\n\nfor i in range(len(unique_theater_type)):\n    theater_type_dict[unique_theater_type[i]] = i\n\nfor i in range(len(unique_theater_area)):\n    theater_area_dict[unique_theater_area[i]] = i\n\n# Apply encoding\nf_t_df['encoded_theater_type'] = f_t_df['theater_type'].map(theater_type_dict)\nf_t_df['encoded_theater_area'] = f_t_df['theater_area'].map(theater_area_dict)\n\nprint(f\"Encoded {len(theater_type_dict)} theater types\")\nprint(f\"Encoded {len(theater_area_dict)} theater areas\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Time series features (lag and roll)\n","metadata":{}},{"cell_type":"code","source":"# sort the df with dates\nf_t_df = f_t_df.sort_values(['book_theater_id', 'show_date']).reset_index(drop=True)\nprint(f\"start date: {f_t_df['show_date'].iloc[0]}\")\nprint(f\"End date: {f_t_df['show_date'].iloc[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create lag features\nfor lag in [1, 7, 14, 21, 28, 30]:\n    f_t_df[f'audience_lag_{lag}'] = f_t_df.groupby('book_theater_id')['audience_count'].shift(lag)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Createing rolling features\ndef roll_mean(window, x):\n    return x.shift(1).rolling(window, min_periods=1).mean()\n\ndef roll_std(window, x):\n    return x.shift(1).rolling(window, min_periods=1).std()\n\nfor window in [7, 14, 30]:\n    f_t_df[f'audience_roll_mean_{window}'] = (\n        f_t_df.groupby('book_theater_id')['audience_count']\n               .transform(lambda x: roll_mean(window, x))\n    )\n    f_t_df[f'audience_roll_std_{window}'] = (\n        f_t_df.groupby('book_theater_id')['audience_count']\n               .transform(lambda x: roll_std(window, x))\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f_t_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f_t_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Observation on Feature engg. (part 1)\n1. Basic dates probably directly wont influence but its components (day, month, year) and there extracting pattern from it can effect model performance\n   Ex: capture audience_count on weekends, sundays, or fridays\n\n2. Cyclical encoding (sin/cos) to making sure sure model understans the cyclical pattern\n\n3. creating weekend indicators assuming friday(when generaly movie realses) , sundays (since it holiday)\n\n4. since min audience_count is 2 that means there is no error present in audeince count which concluded that if audience_count is more than upper bound that means that shows is hit/popular show  this will help model \n\n5. lag feature help model to understand the trends of weekly pattern with yesterday pattern. \n\n6. rolling features like mean and std were used to capture avg audience over last N days and std used to capture how volatile the audience count is\n","metadata":{}},{"cell_type":"code","source":"# Remove rows with missing or zero audience (future dates/closed days)\nbefore = len(f_t_df)\nf_t_df['audience_count'] = f_t_df['audience_count'].fillna(0)\nf_t_df = f_t_df[f_t_df['audience_count'] > 0].reset_index(drop=True)\nafter = len(f_t_df)\n\nprint(f\"Removed: {before - after} rows\")\nprint(f\"Final training size: {f_t_df.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"duplicates = df3[df3.duplicated(subset=['book_theater_id', 'show_date', 'audience_count'], keep=False)]\nprint(len(duplicates))\nduplicates.head()\ndf3.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f_t_df.select_dtypes(include=['number']).columns)\nprint(f_t_df.select_dtypes(exclude=['number']).columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f_t_df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_features = f_t_df.select_dtypes(include=['number']).columns.to_list()\n  \n# Correlation heatmap\nplt.figure(figsize=(21, 16))\nsns.heatmap(f_t_df[numeric_features].corr(), cmap='coolwarm', annot=False)\nplt.title(\"Correlation Matrix of All Features\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bar plot of correlations with target\ntarget = \"audience_count\"\ncorr_with_target = f_t_df[numeric_features].corr()[target].drop(target)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x=corr_with_target.index, y=corr_with_target.values)\nplt.xticks(rotation=90)\nplt.ylabel(\"Correlation with audience_count\")\nplt.title(\"Feature Importance by Correlation\")\nplt.tight_layout()\nplt.show()\n\n# Print top correlations\nprint(\"ranking correlated features highest to lowest\")\nprint(corr_with_target.sort_values(ascending=False).head(53))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre Model Setting\n","metadata":{}},{"cell_type":"code","source":"# sicne this is forcasting based reg prob so it req time series split\n\nf_t_df = f_t_df.sort_values('show_date').reset_index(drop=True)\n\n# split in 80 - 20 \nsplit_idx = int(len(f_t_df) * 0.8)\n\ntrain_df = f_t_df.iloc[:split_idx].copy()\ntest_df = f_t_df.iloc[split_idx:].copy()\n\n# Time Series Cross-Validation setup (on training data only)\ntscv = TimeSeriesSplit(n_splits=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_col = \"audience_count\"\n\n# Separate X and y for train\ny_train = train_df[target_col]\nX_train = train_df.drop(columns=[target_col,\n                                 'ID',\n                                 'show_date',] , errors=\"ignore\")\n\n\n# separate X and y for test\ny_test = test_df[target_col]\nX_test = test_df.drop(columns=[target_col,\n                                 'ID',\n                                 'show_date'] , errors=\"ignore\")\n\n\nprint(f\"  X_train: {X_train.shape}\")\nprint(f\"  y_train: {y_train.shape}\")\nprint(f\"  X_test: {X_test.shape}\")\nprint(f\"  y_test: {y_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.isna().sum())\nprint(X_test.isna().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"incomplete_col = [\n    \"audience_lag_1\",\n    \"audience_lag_7\",\n    \"audience_lag_14\",\n    \"audience_lag_21\",\n    \"audience_lag_28\",\n    \"audience_lag_30\",\n    \"audience_roll_mean_7\",\n    \"audience_roll_std_7\",\n    \"audience_roll_mean_14\",\n    \"audience_roll_std_14\",\n    \"audience_roll_mean_30\",\n    \"audience_roll_std_30\",\n]\nfor col in incomplete_col:\n    X_train[col].fillna(0, inplace=True)\n    X_test[col].fillna(0, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating Pre Processor\n\n# creating numberical and categorical cols\nnumerical_cols = X_train.select_dtypes(include=['number']).columns.tolist()\ncategorical_cols = X_train.select_dtypes(exclude=['number']).columns.tolist()\n\n\n# Create preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', MinMaxScaler(), numerical_cols),\n        ('cat', OneHotEncoder( handle_unknown='ignore', sparse_output=False), categorical_cols)\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### model Strategy:\n1. I will use 3 models that are Dummy Reg, Linear Reg, Decision Tree\n2. Dummy Reg will help me to set a floor of worst acceptable performanace\n3. linear reg i use to test if linear relation exist or not\n4. decision tree i use to test if non linear/tree model are better or not","metadata":{}},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating a common function for comparing base model\ndef base_model_comparison(model, model_name):\n\n    # Create pipeline (preprocessing + model)\n    pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('model', model)\n    ])\n\n    # Train model\n    pipeline.fit(X_train, y_train)\n\n    # Predict\n    train_pred = pipeline.predict(X_train)\n    test_pred  = pipeline.predict(X_test)\n\n    # Scores\n    train_r2  = r2_score(y_train, train_pred)\n    test_r2   = r2_score(y_test, test_pred)\n    train_mae = mean_absolute_error(y_train, train_pred)\n    test_mae  = mean_absolute_error(y_test, test_pred)\n\n    \n    print(\"\\nTrain Performance:\")\n    print(\"R²:\", round(train_r2, 4))\n    print(\"MAE:\", round(train_mae, 2))\n\n    print(\"\\nTest Performance:\")\n    print(\"R²:\", round(test_r2, 4))\n    print(\"MAE:\", round(test_mae, 2))\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.dummy import DummyRegressor\n# base_model_comparison(DummyRegressor(strategy='mean'), \"Dummy Regressor\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# base_model_comparison(LinearRegression(), \"Linear Regression\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# base_model_comparison(DecisionTreeRegressor(random_state=42), \"Decision Tree Regressor\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation on Model Output\n1. since this were the outputs\n---\n\nfor dummy reg\n\nTrain Performance:\n  R² : 0.0\n  MAE: 24.76\n\nTest Performance:\n  R² : -0.0011\n  MAE: 24.54\n\n\n\n---\n\nfor linear reg \n\nTrain Performance:\n  R² : 0.5189\n  MAE: 15.12\n\nTest Performance:\n  R² : -7.608488841784314e+22\n  MAE: 8442996548036.37\n\n\n\n\n  \n---\n\nfor decision tree\n\nTrain Performance:\n  R² : 1.0\n  MAE: 0.0\n  \n\n\nTest Performance:\n  R² : -0.0832\n  MAE: 21.97\n\n\n  \n---\n\nfrom this it is clearlly confirms and also observed that tree based model are perfect for this df.","metadata":{}},{"cell_type":"markdown","source":"From above observation it is completly understandable that we need tree based model and with hpt only they might able to give me best r2 score. \n###### Stratigy: \nso finally i will test some tree based model and check which models are perfect and give me best r2 score after that we will calculate the best weights then we will simple train the complete dataset on that so that in last we get best r2 score.","metadata":{}},{"cell_type":"code","source":"# ## Models\n# model 1 : Random Forest\n\n# model_rf = RandomForestRegressor(\n#     n_estimators=200,\n#     max_depth = 16,    #\n#     random_state=42,\n#     n_jobs=-1)\n\n# rf_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('random_forest', model_rf)\n# ])\n\n# rf_pipeline.fit(X_train, y_train)\n\n# # Train scores\n# rf_train_pred = rf_pipeline.predict(X_train)\n# rf_train_r2  = r2_score(y_train, rf_train_pred)\n# rf_train_mae = mean_absolute_error(y_train, rf_train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(rf_train_r2, 4))\n# print(\"MAE:\", round(rf_train_mae, 2))\n\n# # Test scores\n# rf_test_pred = rf_pipeline.predict(X_test)\n# rf_test_r2 = r2_score(y_test, rf_test_pred)\n# rf_test_mae = mean_absolute_error(y_test, rf_test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R² :\", round(rf_test_r2, 4))\n# print(\"MAE:\", round(rf_test_mae, 2))\n\n\n\n\n\n######################## OUTPUT #######################\n# output = (without max depth parameter)\n\n# Train Performance:\n# R²: 0.9342\n# MAE: 5.47\n\n# Test Performance:\n# R²: 0.5197\n# MAE: 15.17\n\n\n# output: (with max depth parameter)\n\n# Train Performance:\n# R²: 0.7452\n# MAE: 12.38\n\n# Test Performance:\n# R²: 0.5241\n# MAE: 15.05\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here i use random forest with max depth to 16 and without max depth parameter.\nthe reason to use only 16 is to minimize the overfitting \nsince without max depth generalisation gap is \n0.9342 - 0.5197 = 0.4145\nsince this gap in r2 socre shows the gap is very high which show model is overfitting\nand with max depth introduction the gap becomes \n0.745 - 0.524 = 0.21\nthis show the we have reduce the overfitting and we can now do hpt on rnadom forest","metadata":{}},{"cell_type":"code","source":"# # model 1 : Random Forest (HPT)\n\n# param_dist_rf = {\n#     'random_forest__n_estimators': [100, 200 ],\n#     'random_forest__max_depth': [5, 10, 15, 20, None],\n#     'random_forest__min_samples_split': [10, 20],\n# }\n\n# model_rf = RandomForestRegressor(\n#     random_state=42, \n#     n_jobs=-1)\n\n# rf_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('random_forest', model_rf)\n# ])\n\n# model_rf_cv = RandomizedSearchCV(\n#     rf_pipeline,\n#     param_distributions=param_dist_rf,\n#     n_iter=10,\n#     cv=tscv,\n#     scoring='r2',  \n#     n_jobs=-1,\n#     random_state=42,\n# )\n\n# model_rf_cv.fit(X_train, y_train)\n\n# # Best params\n# print(\"best parameters:\", model_rf_cv.best_params_)\n\n# # Best R2\n# print(\"best CV R2:\", model_rf_cv.best_score_)\n\n# # Build final model with best parameters\n# best_params = model_rf_cv.best_params_\n\n# best_model_rf = RandomForestRegressor(\n#     n_estimators = best_params['random_forest__n_estimators'],\n#     max_depth = best_params['random_forest__max_depth'],\n#     min_samples_split = best_params['random_forest__min_samples_split'],\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# rf_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('random_forest', best_model_rf)\n# ])\n\n# rf_tuned_pipeline.fit(X_train, y_train)\n\n# # Train scores\n# train_pred = rf_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n# # Test scores\n# test_pred = rf_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n'''\nFitting 2 folds for each of 10 candidates, totalling 20 fits\n\nbest parameters: {'random_forest__n_estimators': 100, 'random_forest__min_samples_split': 5, 'random_forest__max_depth': 20}\nbest CV R2: 0.5185869086520944\n\nTrain Performance:\nR²: 0.7709\nMAE: 11.11\n\nTest Performance:\nR²: 0.5248\nMAE: 15.08\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"before  hpt random forest test r2 was about 0.524 and now its again 0.524 but train data now will fits better in hpt\nand mae intial was 15.05 and now it is 15.08 almost no change ok.","metadata":{}},{"cell_type":"code","source":"# # model 2 : Extra Trees\n# model_et = ExtraTreesRegressor(\n#     n_estimators=200,\n#     max_depth=16,\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# et_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('extra_trees', model_et)\n# ])\n\n# et_pipeline.fit(X_train, y_train)\n\n# # Train scores\n# et_train_pred = et_pipeline.predict(X_train)\n# et_train_r2  = r2_score(y_train, et_train_pred)\n# et_train_mae = mean_absolute_error(y_train, et_train_pred)\n\n# print(\"\\nTrain Performance (Extra Trees):\")\n# print(\"R²:\", round(et_train_r2, 4))\n# print(\"MAE:\", round(et_train_mae, 2))\n\n# # Test scores\n# et_test_pred = et_pipeline.predict(X_test)\n# et_test_r2 = r2_score(y_test, et_test_pred)\n# et_test_mae = mean_absolute_error(y_test, et_test_pred)\n\n# print(\"\\nTest Performance (Extra Trees):\")\n# print(\"R²:\", round(et_test_r2, 4))\n# print(\"MAE:\", round(et_test_mae, 2))\n\n'''\nTrain Performance (Extra Trees):\nR²: 0.6786\nMAE: 13.63\n\nTest Performance (Extra Trees):\nR²: 0.5425\nMAE: 14.82\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # model 2 : Extra Trees (HPT)\n\n# param_dist_et = {\n#     'extra_trees__n_estimators': [100, 200 ],\n#     'extra_trees__max_depth': [5, 10, 15, 20, None],\n#     'extra_trees__min_samples_split': [2, 5, 10, 20],\n# }\n\n# model_et = ExtraTreesRegressor(\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# et_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('extra_trees', model_et)\n# ])\n\n\n# model_et_cv  = RandomizedSearchCV(\n#     et_pipeline,\n#     param_distributions=param_dist_et,\n#     n_iter=10,\n#     cv=tscv,\n#     scoring='r2',  \n#     n_jobs=-1,\n#     random_state=42,\n#     verbose=1\n# )\n\n# model_et_cv.fit(X_train, y_train)\n\n# # Best params\n# print(\"\\nbest parameters:\", model_et_cv.best_params_)\n\n# # Best r2\n# print(\"Best CV r2:\", model_et_cv.best_score_)\n\n# # Build final model with best parameters\n# best_params = model_et_cv.best_params_\n\n# best_model_et = ExtraTreesRegressor(\n#     n_estimators = best_params['extra_trees__n_estimators'],\n#     max_depth = best_params['extra_trees__max_depth'],    \n#     min_samples_split = best_params['extra_trees__min_samples_split'],\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# et_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('extra_trees', best_model_et)\n# ])\n\n# et_tuned_pipeline.fit(X_train, y_train)\n\n# # Train scores\n# train_pred = et_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n# # Test scores\n# test_pred = et_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n\n'''\nFitting 2 folds for each of 10 candidates, totalling 20 fits\n\nbest parameters: {'extra_trees__n_estimators': 100, 'extra_trees__min_samples_split': 5, 'extra_trees__max_depth': 20}\nBest CV r2: 0.3930664296971299\n\nTrain Performance:\nR²: 0.4142\nMAE: 18.46\n\nTest Performance:\nR²: 0.3833\nMAE: 18.81\n  \n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Extra tree without hpt\n \n ---\n \n Train Performance (Extra Trees):\n  R² : 0.6786\n  MAE: 13.63\n\nTest Performance (Extra Trees):\n  R² : 0.5425\n  MAE: 14.82\n\n---\n \nExtra tree without hpt\n\n---\nTrain Performance:\nR²: 0.4142\nMAE: 18.46\n\nTest Performance:\nR²: 0.3833\nMAE: 18.81\n \nbefore hpt the training performance is fairly high and test performance was little lower and has acceptable Generalization and after hpt there is big drop in train and test performance and now after hpt the model is underfitting the data and hpt made the model worse. so hpt underfit the mdoel and reduce its learning capcity.","metadata":{}},{"cell_type":"code","source":"# # # model 3 : Decison Trees\n\n# model_dt = DecisionTreeRegressor(\n#     random_state=42\n# )\n\n# dt_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('decision_tree', model_dt)\n# ])\n\n# # Fit model\n# dt_pipeline.fit(X_train, y_train)\n\n# # Train scores\n# dt_train_pred = dt_pipeline.predict(X_train)\n# dt_train_r2  = r2_score(y_train, dt_train_pred)\n# dt_train_mae = mean_absolute_error(y_train, dt_train_pred)\n\n# print(\"\\nTrain Performance (Decision Tree):\")\n# print(\"R²:\", round(dt_train_r2, 4))\n# print(\"MAE:\", round(dt_train_mae, 2))\n\n# #  Test scores \n# dt_test_pred = dt_pipeline.predict(X_test)\n# dt_test_r2 = r2_score(y_test, dt_test_pred)\n# dt_test_mae = mean_absolute_error(y_test, dt_test_pred)\n\n# print(\"\\nTest Performance (Decision Tree):\")\n# print(\"R²:\", round(dt_test_r2, 4))\n# print(\"MAE:\", round(dt_test_mae, 2))\n\n'''\n\nTrain Performance:\nR²: 1.0\nMAE: 0.0\n  \nTest Performance:\nR²: -0.0832\nMAE: 21.97\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # model 3 : Decison Trees (HPT)\n\n\n# param_dist_dt = {\n#     'decision_tree__max_depth': [5, 10, 15, 20, None],\n#     'decision_tree__min_samples_split': [2, 5, 10, 20],\n# }\n\n# model_dt = DecisionTreeRegressor(\n#     random_state=42\n# )\n\n# dt_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('decision_tree', model_dt)\n# ])\n\n# model_dt_cv = RandomizedSearchCV(\n#     dt_pipeline,\n#     param_distributions=param_dist_dt,\n#     n_iter=10,\n#     cv=tscv,\n#     scoring='r2',\n#     n_jobs=-1,\n#     random_state=42,\n#     verbose=1\n# )\n\n# model_dt_cv.fit(X_train, y_train)\n\n# print(\"\\nbest parameters:\", model_dt_cv.best_params_)\n# print(\"best CV r2:\", model_dt_cv.best_score_)\n\n\n# best_params = model_dt_cv.best_params_\n\n# best_model_dt = DecisionTreeRegressor(\n#     max_depth = best_params['decision_tree__max_depth'],\n#     min_samples_split = best_params['decision_tree__min_samples_split'],\n#     random_state=42\n# )\n\n# dt_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('decision_tree', best_model_dt)\n# ])\n\n# dt_tuned_pipeline.fit(X_train, y_train)\n\n# train_pred = dt_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n\n# test_pred = dt_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n'''\nFitting 2 folds for each of 10 candidates, totalling 20 fits\n\nbest parameters: {'decision_tree__min_samples_split': 2, 'decision_tree__max_depth': 5}\nbest CV r2: 0.4621231525749027\n\nTrain Performance:\nR²: 0.4838\nMAE: 15.88\n\nTest Performance:\nR²: 0.4535\nMAE: 15.95\n  \n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Decision Tree without hpt\n \n ---\n \nTrain Performance: \n\nR² : 1.0 \nMAE: 0.0 \n\nTest Performance: \n\nR² : -0.0832 \nMAE: 21.97\n\n---\n \nDecision Tree without hpt\n\n---\n\nTrain Performance:\nR²: 0.4838\nMAE: 15.88\n\nTest Performance:\nR²: 0.4535\nMAE: 15.95\n  \n\nbefore hpt the decision tree showed extreme overfitting and training performace was perfect but test was extremly poor which shows a very large genralization gap \n1 - (-0.0832) = 1.0832 shows extreme overfitting\nafter hpt the overfitting overfitting get fixed and test score improved better\nafter hpt the generalization gap \n0.4838 - 0.4535 = 0.03 small and acceptable\nthis shows now after hpt the decision tree model is performing much better way.","metadata":{}},{"cell_type":"code","source":"# # # # model 4 : LightGBM \n\n# model_lgb = LGBMRegressor(\n#     random_state=42\n# )\n\n# lgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('lightgbm', model_lgb)\n# ])\n\n# # Fit model\n# lgb_pipeline.fit(X_train, y_train)\n\n# #  Train scores \n# lgb_train_pred = lgb_pipeline.predict(X_train)\n# lgb_train_r2  = r2_score(y_train, lgb_train_pred)\n# lgb_train_mae = mean_absolute_error(y_train, lgb_train_pred)\n\n# print(\"\\nTrain Performance (LightGBM):\")\n# print(\"R²:\", round(lgb_train_r2, 4))\n# print(\"MAE:\", round(lgb_train_mae, 2))\n\n# # Test scores \n# lgb_test_pred = lgb_pipeline.predict(X_test)\n# lgb_test_r2 = r2_score(y_test, lgb_test_pred)\n# lgb_test_mae = mean_absolute_error(y_test, lgb_test_pred)\n\n# print(\"\\nTest Performance (LightGBM):\")\n# print(\"R²:\", round(lgb_test_r2, 4))\n# print(\"MAE:\", round(lgb_test_mae, 2))\n\n'''\nTrain Performance (LightGBM):\nR²: 0.5814\nMAE: 14.3\n\nTest Performance (LightGBM):\nR²: 0.5361\nMAE: 14.99\n  \n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # model 4 : LightGBM (HPT)\n\n# param_dist_lgb = {\n#     'lgbm__max_depth': [8, 10, 12, 15],\n#     'lgbm__learning_rate': [0.01, 0.05, 0.1],\n#     'lgbm__n_estimators': [500, 1000, 1500],\n#     'lgbm__reg_alpha': [0.01, 0.1, 0.5],\n#     'lgbm__reg_lambda': [0.01, 0.1, 0.5]\n# }\n\n# model_lgb = LGBMRegressor(\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# lgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('lgbm', model_lgb)\n# ])\n\n# model_lgb_cv = RandomizedSearchCV(\n#     lgb_pipeline,\n#     param_distributions=param_dist_lgb,\n#     n_iter=20,\n#     cv=tscv,\n#     scoring='r2',\n#     n_jobs=-1,\n#     random_state=42,\n#     verbose=1\n# )\n\n# model_lgb_cv.fit(X_train, y_train)\n\n# print(\"\\nbest parameters:\", model_lgb_cv.best_params_)\n# print(\"best CV r2:\", model_lgb_cv.best_score_)\n\n\n# best_params = model_lgb_cv.best_params_\n\n# best_model_lgb = LGBMRegressor(\n#     max_depth = best_params['lgbm__max_depth'],\n#     learning_rate = best_params['lgbm__learning_rate'],\n#     n_estimators = best_params['lgbm__n_estimators'],\n#     reg_alpha =  best_params['lgbm__reg_alpha'],\n#     reg_lambda =  best_params['lgbm__reg_lambda'],\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# lgb_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('lgbm', best_model_lgb)\n# ])\n\n# lgb_tuned_pipeline.fit(X_train, y_train)\n\n\n# train_pred = lgb_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n\n# test_pred = lgb_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n'''\n\nbest parameters: {'lgbm__reg_lambda': 0.5, 'lgbm__reg_alpha': 0.1, 'lgbm__n_estimators': 500, 'lgbm__max_depth': 8, 'lgbm__learning_rate': 0.01}\nbest CV r2: 0.534407553598029\n\nTrain Performance:\nR²: 0.5585\nMAE: 14.58\n\nTest Performance:\nR²: 0.5395\nMAE: 14.97\n '''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" LightGBM without hpt\n \n ---\n \nTrain Performance (LightGBM):\n  R² : 0.5814\n  MAE: 14.3\n\nTest Performance (LightGBM):\n  R² : 0.5361\n  MAE: 14.99\n  \n\n---\n \nLightGBM with hpt\n\n---\n\nTrain Performance:\nR²: 0.5585\nMAE: 14.58\n\nTest Performance:\nR²: 0.5395\nMAE: 14.97\n\n---\n\nthis model is actually performing very well and its genralisation gap is very low 0.5585 - 0.5395 = 0.019 approx \nand train and test performance almost identical and test performace is improove little \nthis means is the genralisation is good and very minimal overfitting. \n  ","metadata":{}},{"cell_type":"code","source":"# # # # # model 5 : Hist Gradient Boosting \n# model_hgb = HistGradientBoostingRegressor(\n#     random_state=42\n# )\n\n# hgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('hist_gbr', model_hgb)\n# ])\n\n# # Fit model\n# hgb_pipeline.fit(X_train, y_train)\n\n# # ---- Train scores ----\n# hgb_train_pred = hgb_pipeline.predict(X_train)\n# hgb_train_r2  = r2_score(y_train, hgb_train_pred)\n# hgb_train_mae = mean_absolute_error(y_train, hgb_train_pred)\n\n# print(\"\\nTrain Performance (HistGradientBoosting):\")\n# print(\"R²:\", round(hgb_train_r2, 4))\n# print(\"MAE:\", round(hgb_train_mae, 2))\n\n# # ---- Test scores ----\n# hgb_test_pred = hgb_pipeline.predict(X_test)\n# hgb_test_r2 = r2_score(y_test, hgb_test_pred)\n# hgb_test_mae = mean_absolute_error(y_test, hgb_test_pred)\n\n# print(\"\\nTest Performance (HistGradientBoosting):\")\n# print(\"R²:\", round(hgb_test_r2, 4))\n# print(\"MAE:\", round(hgb_test_mae, 2))\n\n'''\nTrain Performance (HistGradientBoosting):\nR²: 0.5794\nMAE: 14.31\n\nTest Performance (HistGradientBoosting):\nR²: 0.5382\nMAE: 14.95\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # model 5 : Hist Gradient Boosting (HPT)\n\n\n# param_dist_hgb = {\n#     'hgb__max_depth': [6, 8, 10, 12, None],\n#     'hgb__learning_rate': [0.01, 0.05, 0.1],\n#     'hgb__max_iter': [500, 1000, 1500],\n# }\n\n# model_hgb = HistGradientBoostingRegressor(\n#     random_state=42\n# )\n\n# hgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('hgb', model_hgb)\n# ])\n\n\n\n# model_hgb_cv = RandomizedSearchCV(\n#     hgb_pipeline,\n#     param_distributions=param_dist_hgb,\n#     n_iter=20,\n#     cv=tscv,\n#     scoring='r2',\n#     n_jobs=-1,\n#     random_state=42,\n#     verbose=1\n# )\n\n# model_hgb_cv.fit(X_train, y_train)\n\n# print(\"\\nbest parameters:\", model_hgb_cv.best_params_)\n# print(\"best CV r2:\", model_hgb_cv.best_score_)\n\n# best_params = model_hgb_cv.best_params_\n\n# best_model_hgb = HistGradientBoostingRegressor(\n#     max_depth = best_params['hgb__max_depth'],\n#     learning_rate = best_params['hgb__learning_rate'],\n#     max_iter = best_params['hgb__max_iter'],\n#     random_state = 42\n# )\n\n# hgb_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('hgb', best_model_hgb)\n# ])\n\n# hgb_tuned_pipeline.fit(X_train, y_train)\n\n\n# #Train Performance\n\n# train_pred = hgb_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n\n# # Test Performance\n# test_pred = hgb_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n'''\nFitting 2 folds for each of 20 candidates, totalling 40 fits\n\nbest parameters: {'hgb__max_iter': 500, 'hgb__max_depth': 8, 'hgb__learning_rate': 0.01}\nbest CV r2: 0.5266836220518083\n\nTrain Performance:\nR²: 0.5575\nMAE: 14.59\n\nTest Performance:\nR²: 0.5396\nMAE: 14.95\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" HistGradientBoosting without hpt\n \n ---\n \nTrain Performance (HistGradientBoosting):\n  R² : 0.5794\n  MAE: 14.31\n\nTest Performance (HistGradientBoosting):\n  R² : 0.5382\n  MAE: 14.95\n  \n\n---\n \nHistGradientBoosting without hpt\n\n---\n\nTrain Performance:\nR²: 0.5575\nMAE: 14.59\n\nTest Performance:\nR²: 0.5396\nMAE: 14.95\n\n---\n\nthe hgb model strong and stable performance with minimal overfitting and test and train score were very close.\nR² gap = 0.5575 − 0.5396 = 0.0179\nand defalt hgb model is performing very well after hpt the model performance imporved slightly better ","metadata":{}},{"cell_type":"code","source":"# # # # # # model 6 : XG Boosting \n\n# model_xgb = XGBRegressor(\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# xgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('xgboost', model_xgb)\n# ])\n\n# # Fit model\n# xgb_pipeline.fit(X_train, y_train)\n\n# # ---- Train scores ----\n# xgb_train_pred = xgb_pipeline.predict(X_train)\n# xgb_train_r2  = r2_score(y_train, xgb_train_pred)\n# xgb_train_mae = mean_absolute_error(y_train, xgb_train_pred)\n\n# print(\"\\nTrain Performance (XGBoost):\")\n# print(\"R²:\", round(xgb_train_r2, 4))\n# print(\"MAE:\", round(xgb_train_mae, 2))\n\n# # ---- Test scores ----\n# xgb_test_pred = xgb_pipeline.predict(X_test)\n# xgb_test_r2 = r2_score(y_test, xgb_test_pred)\n# xgb_test_mae = mean_absolute_error(y_test, xgb_test_pred)\n\n# print(\"\\nTest Performance (XGBoost):\")\n# print(\"R²:\", round(xgb_test_r2, 4))\n# print(\"MAE:\", round(xgb_test_mae, 2))\n\n'''\nTrain Performance (XGBoost):\nR²: 0.6536\nMAE: 13.71\n\nTest Performance (XGBoost):\nR²: 0.4947\nMAE: 15.37\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # # # model 6 : XG Boosting (HPT)\n\n# param_dist_xgb = {\n#     'xgb__n_estimators': [ 500, 1000, 1500],\n#     'xgb__max_depth': [8, 10, 12, 15],\n#     'xgb__learning_rate': [0.01, 0.05, 0.1],\n#     'xgb__reg_alpha': [ 0.01, 0.1, 0.5],\n#     'xgb__reg_lambda': [0.01, 0.1, 0.5]\n# }\n\n# model_xgb = XGBRegressor(\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# xgb_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('xgb', model_xgb)\n# ])\n\n# model_xgb_cv = RandomizedSearchCV(\n#     xgb_pipeline,\n#     param_distributions=param_dist_xgb,\n#     n_iter=20,\n#     cv=tscv,\n#     scoring='r2',\n#     n_jobs=-1,\n#     random_state=42,\n#     verbose=1\n# )\n\n# model_xgb_cv.fit(X_train, y_train)\n\n# print(\"\\nbest parameters:\", model_xgb_cv.best_params_)\n# print(\"best CV r2:\", model_xgb_cv.best_score_)\n\n# best_params = model_xgb_cv.best_params_\n\n# best_model_xgb = XGBRegressor(\n#     n_estimators = best_params['xgb__n_estimators'],\n#     max_depth = best_params['xgb__max_depth'],\n#     learning_rate = best_params['xgb__learning_rate'],\n#     reg_alpha = best_params['xgb__reg_alpha'],\n#     reg_lambda = best_params['xgb__reg_lambda'],\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# xgb_tuned_pipeline = Pipeline([\n#     ('preprocess', preprocessor),\n#     ('xgb', best_model_xgb)\n# ])\n\n# xgb_tuned_pipeline.fit(X_train, y_train)\n\n# train_pred = xgb_tuned_pipeline.predict(X_train)\n# train_r2  = r2_score(y_train, train_pred)\n# train_mae = mean_absolute_error(y_train, train_pred)\n\n# print(\"\\nTrain Performance:\")\n# print(\"R²:\", round(train_r2, 4))\n# print(\"MAE:\", round(train_mae, 2))\n\n# test_pred = xgb_tuned_pipeline.predict(X_test)\n# test_r2 = r2_score(y_test, test_pred)\n# test_mae = mean_absolute_error(y_test, test_pred)\n\n# print(\"\\nTest Performance:\")\n# print(\"R²:\", round(test_r2, 4))\n# print(\"MAE:\", round(test_mae, 2))\n\n'''\nFitting 2 folds for each of 20 candidates, totalling 40 fits\n\nbest parameters: {'xgb__reg_lambda': 0.5, 'xgb__reg_alpha': 0.1, 'xgb__n_estimators': 500, 'xgb__max_depth': 8, 'xgb__learning_rate': 0.01}\nbest CV r2: 0.5223817320329294\n\nTrain Performance:\nR²: 0.6535\nMAE: 13.85\n\nTest Performance:\nR²: 0.5347\nMAE: 15.0\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" XGB without hpt\n \n ---\n \nTrain Performance (XGBoost):\n  R² : 0.6536\n  MAE: 13.71\n\nTest Performance (XGBoost):\n  R² : 0.4947\n  MAE: 15.37\n  \n\n---\n \nXGB without hpt\n\n---\n\nTrain Performance:\n  R² : 0.6535\n  MAE: 13.85\n\nTest Performance:\n  R² : 0.5347\n  MAE: 15.0\n\n---\n\nintially the xgboost model show moderate overfitting with noticeably better performance on the training than on the test. \ngenralisation gap is 0.6536 – 0.4947 = 0.1589\nafter hpt the models training performance is remains same and tunned model is less overfitted on data than default model.","metadata":{}},{"cell_type":"code","source":"# graph of comparing test mae and test r2 score for finding best models for ensemble \nmodels = [\"RandomForest\",\"ExtraTrees\",\"DecisionTree (HPT)\",\"LightGBM(HPT)\",\"HistGB(HPT)\",\"XGBoost(HPT)\"]\n\ntest_r2 = [0.5241, 0.5425, 0.4535, 0.5395, 0.5396, 0.5347]\nmae = [15.07, 14.82, 15.95, 14.97,  14.95, 15.0]\n\n\nplt.figure(figsize=(8,5))\nplt.bar(models, test_r2)\nplt.title(\"Test R² Comparison (All Models)\")\nplt.ylabel(\"Test R²\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(8,5))\nplt.plot(models, mae)\nplt.title(\"Test MAE Comparison (All Models)\")\nplt.ylabel(\"Test MAE\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### OBservation on models\nfrom both graphs it clearly observed dt model is performing extremly worse, then followed by random forest\nfrom these graphs its clearly decision tree and Random forest we need to drop all tree of these model and we have to use enseble of lightgbm, extra trees, histgb and xgb\ni wont choose rndom forest because extra trees model is already providing me strong and randomized bagging so. and Et is more randomized and it inc diversity more and rf's r2 score is slightly lower than extra tree \nsince enseble beinifits from combining diverse model so i use 1 bagging (Extra tree) and 3 boosting models (LightGBM, HistGBM, XGBoost) ","metadata":{}},{"cell_type":"markdown","source":"so first what we do is we try first each on individual select model then in last we create an enseblem of all 4 models","metadata":{}},{"cell_type":"code","source":"f_t_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# i will use OneHotEncoder on these\ncategorical_columns = [\n    'book_theater_id', \n    'day_of_week', \n    'theater_type', \n    'theater_area'\n]\n\n\n# i will use MinMaxScaler on these\nstatic_numerical_columns = [\n    'latitude', \n    'longitude', \n    'show_day', \n    'show_month', \n    'show_year',\n    'encoded_day_of_week', \n    'day_of_year', \n    'week_of_year', \n    'quarter',\n    'is_weekend', \n    'is_sunday', \n    'is_friday',\n    'dow_sin', \n    'dow_cos', \n    'month_sin', \n    'month_cos', \n    'doy_sin', \n    'doy_cos',\n    'encoded_theater_type', \n    'encoded_theater_area'\n]\n\n# i will use MinMaxScaler on these\ndynamic_columns = [\n    'audience_lag_1', \n    'audience_lag_7', \n    'audience_lag_14',\n    'audience_lag_21', \n    'audience_lag_28', \n    'audience_lag_30',\n    'audience_roll_mean_7', \n    'audience_roll_std_7',\n    'audience_roll_mean_14', \n    'audience_roll_std_14',\n    'audience_roll_mean_30', \n    'audience_roll_std_30'\n]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN in dynamic columns with 0\nfor column_name in dynamic_columns:\n    if column_name in f_t_df.columns:\n        f_t_df[column_name] = f_t_df[column_name].fillna(0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = f_t_df['audience_count'].values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training samples:\", len(y_train))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# OHE for categorical columns\n# MinMaxScaler for static and dynamic numerical columns\nohe_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nstatic_scaler = MinMaxScaler()\ndynamic_scaler = MinMaxScaler()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### fitting\nprint(\"fitting start\")\nohe_encoder.fit(f_t_df[categorical_columns])\nprint(\"cat fitted\")\nstatic_scaler.fit(f_t_df[static_numerical_columns])\nprint(\"static fitted\")\ndynamic_scaler.fit(f_t_df[dynamic_columns])\nprint(\"dynamic fitted\")\nprint(\"completepreprocessor fitted\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### transformation of train cols\n\nX_train_categorical = ohe_encoder.transform(f_t_df[categorical_columns])\nprint(\"cat done\")\nX_train_static = static_scaler.transform(f_t_df[static_numerical_columns])\nprint(\"static done\")\nX_train_dynamic = dynamic_scaler.transform(f_t_df[dynamic_columns])\nprint(\"dynamic done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combining all features \nX_train_combined = np.hstack([X_train_categorical, X_train_static, X_train_dynamic])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating historical data used for lag features since history dic stores audince count for each theater\nhistory_dict = {}\n\nall_theaters = f_t_df['book_theater_id'].unique()\n\nfor theater_id in all_theaters:\n    theater_data = f_t_df[f_t_df['book_theater_id'] == theater_id]\n    theater_data = theater_data.sort_values('show_date')\n    audience_list = theater_data['audience_count'].tolist()\n    history_dict[theater_id] = audience_list\n\nprint(\"total number of therathers are\", len(history_dict))\nprint(\"  Example: book_00001 has\", len(history_dict.get('book_00001', [])), \"days of history\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_date = dt.date(2024, 3, 1)\nend_date = dt.date(2024, 4, 22)\nall_dates = pd.date_range(start=start_date, end=end_date)\n\nall_booking_ids = []\nfor i in range(1, 830):\n    booking_id = f\"book_{i:05d}\"\n    all_booking_ids.append(booking_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating combinations ok\n\ncombinations = []\nfor date in all_dates:\n    for booking_id in all_booking_ids:\n        combinations.append((booking_id, date))\n\n\nt_df = pd.DataFrame(combinations, columns=[\"book_theater_id\", \"show_date\"])\nt_df[\"show_date\"] = pd.to_datetime(t_df[\"show_date\"])\nt_df[\"ID\"] = t_df[\"book_theater_id\"] + \"_\" + t_df[\"show_date\"].dt.strftime(\"%Y-%m-%d\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### merging of meta data for test df\ndf6['show_date'] = pd.to_datetime(df6['show_date'])\nt_df = t_df.merge(df6, on='show_date', how='left')\nt_df = t_df.merge(df2, on='book_theater_id', how='left')\n\n# filling nan\nt_df['latitude'] = t_df['latitude'].fillna(f_t_df['latitude'].median()) # Use median from train\nt_df['longitude'] = t_df['longitude'].fillna(f_t_df['longitude'].median())\nt_df['theater_area'] = t_df['theater_area'].fillna('unknown_area')\nt_df['theater_type'] = t_df['theater_type'].fillna('unknown_type')\n\n# feature engg\nt_df['show_day'] = t_df['show_date'].dt.day\nt_df['show_month'] = t_df['show_date'].dt.month\nt_df['show_year'] = t_df['show_date'].dt.year\nt_df['encoded_day_of_week'] = t_df['show_date'].dt.dayofweek\nt_df['day_of_year'] = t_df['show_date'].dt.dayofyear\nt_df['week_of_year'] = t_df['show_date'].dt.isocalendar().week\nt_df['quarter'] = t_df['show_date'].dt.quarter\n\nt_df['is_weekend'] = (t_df['encoded_day_of_week'] >= 5).astype(int)\nt_df['is_sunday'] = (t_df['encoded_day_of_week'] == 6).astype(int)\nt_df['is_friday'] = (t_df['encoded_day_of_week'] == 4).astype(int)\n\n# cyclical encodings\nt_df['dow_sin'] = np.sin(2 * np.pi * t_df['encoded_day_of_week'] / 7)\nt_df['dow_cos'] = np.cos(2 * np.pi * t_df['encoded_day_of_week'] / 7)\nt_df['month_sin'] = np.sin(2 * np.pi * t_df['show_month'] / 12)\nt_df['month_cos'] = np.cos(2 * np.pi * t_df['show_month'] / 12)\nt_df['doy_sin'] = np.sin(2 * np.pi * t_df['day_of_year'] / 365)\nt_df['doy_cos'] = np.cos(2 * np.pi * t_df['day_of_year'] / 365)\n\n# 4. feature Engineering: Catorgrical Encoding\nt_df['encoded_theater_type'] = t_df['theater_type'].map(theater_type_dict).fillna(0)\nt_df['encoded_theater_area'] = t_df['theater_area'].map(theater_area_dict).fillna(0)\n\n# # initialize audience_count as NaN (we will predict this)\n# t_df['audience_count'] = np.nan\n\n# Initialize lag columns with zeros\nfor column_name in dynamic_columns:\n    t_df[column_name] = 0.0\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### transforming test feaatures ok\nprint(\"transformation start\")\nX_test_categorical = ohe_encoder.transform(t_df[categorical_columns])\nprint(\"cat done\")\nX_test_static = static_scaler.transform(t_df[static_numerical_columns])\nprint(\"transformation complete\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ### model config\n# # Model 1: ExtraTrees\n# print(\"training model 1: extraTreesRegressor\")\n# model_1_extratrees = ExtraTreesRegressor(\n#     n_estimators=200,\n#     max_depth=16,\n#     random_state=42,\n#     n_jobs=-1\n# )\n# model_1_extratrees.fit(X_train_combined, y_train)\n# print(\"model 1 trained\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Model 2: LightGBM\nprint(\"training model 2: LGBMRegressor\")\nmodel_2_lightgbm = LGBMRegressor(\n    n_estimators=500,\n    max_depth=8,\n    learning_rate=0.01,\n    reg_alpha=0.1,\n    reg_lambda=0.5,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1\n)\nmodel_2_lightgbm.fit(X_train_combined, y_train)\nprint(\"model 2 trained\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Model 3: HistGradientBoosting\nprint(\"training model 3: HistGradientBoostingRegressor\")\nmodel_3_histgb = HistGradientBoostingRegressor(\n    max_iter=500,\n    max_depth=8,\n    learning_rate=0.01,\n    random_state=42\n)\nmodel_3_histgb.fit(X_train_combined, y_train)\nprint(\"model 3 trained\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Model 4: XGBoost\nprint(\"training Model 4: XGBRegressor\")\nmodel_4_xgboost = XGBRegressor(\n    n_estimators=500,\n    max_depth=8,\n    learning_rate=0.01,\n    reg_alpha=0.1,\n    reg_lambda=0.5,\n    random_state=42,\n    n_jobs=-1\n)\nmodel_4_xgboost.fit(X_train_combined, y_train)\nprint(\"model 4 trained\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### predicting function \ndef making_predictions(model, model_name, test_dataframe, history_dictionary, X_test_cat, X_test_stat, dyn_scaler, dyn_cols):\n    # make a copy of history so original is untouched\n    current_history = {}\n    for theater_id in history_dictionary:\n        current_history[theater_id] = history_dictionary[theater_id].copy()\n\n    # sort test data by date\n    test_dataframe = test_dataframe.sort_values(['show_date', 'book_theater_id'])\n    test_dataframe = test_dataframe.reset_index(drop=True)\n    \n    # get all unique dates\n    all_unique_dates = sorted(test_dataframe['show_date'].unique())\n    total_days = len(all_unique_dates)\n    \n    # create empty array to store predictions\n    all_predictions = np.zeros(len(test_dataframe))\n    \n    \n    # loop through each day\n    day_number = 0\n    \n    while day_number < total_days:\n        \n        # get current date\n        current_date = all_unique_dates[day_number]\n        \n        \n        \n        # get row indices for current date\n        date_mask = test_dataframe['show_date'] == current_date\n        row_indices = np.where(date_mask)[0]\n        \n        # get theaters for current date\n        theaters_today = test_dataframe.loc[row_indices, 'book_theater_id'].values\n        num_theaters = len(theaters_today)\n\n\n        \n        # CALCULATE LAG FEATURES FOR EACH THEATER\n        # Create empty arrays for lag features\n        lag_1_array = np.zeros(num_theaters)\n        lag_7_array = np.zeros(num_theaters)\n        lag_14_array = np.zeros(num_theaters)\n        lag_21_array = np.zeros(num_theaters)\n        lag_28_array = np.zeros(num_theaters)\n        lag_30_array = np.zeros(num_theaters)\n        roll_mean_7_array = np.zeros(num_theaters)\n        roll_std_7_array = np.zeros(num_theaters)\n        roll_mean_14_array = np.zeros(num_theaters)\n        roll_std_14_array = np.zeros(num_theaters)\n        roll_mean_30_array = np.zeros(num_theaters)\n        roll_std_30_array = np.zeros(num_theaters)\n        \n        # loop through each theater\n        theater_index = 0\n        \n        while theater_index < num_theaters:\n            \n            # get theater ID\n            theater_id = theaters_today[theater_index]\n            \n            # get history for this theater\n            if theater_id in current_history:\n                theater_history = current_history[theater_id]\n            else:\n                theater_history = []\n            \n            # get length of history\n            history_length = len(theater_history)\n            \n            # calculate lag features\n            \n            # lag 1 (yesterday)\n            if history_length >= 1:\n                lag_1_array[theater_index] = theater_history[-1]\n            else:\n                lag_1_array[theater_index] = 0\n            \n            # lag 7 (7 days ago)\n            if history_length >= 7:\n                lag_7_array[theater_index] = theater_history[-7]\n            else:\n                if history_length >= 1:\n                    lag_7_array[theater_index] = theater_history[-1]\n                else:\n                    lag_7_array[theater_index] = 0\n            \n            # lag 14 (14 days ago)\n            if history_length >= 14:\n                lag_14_array[theater_index] = theater_history[-14]\n            else:\n                if history_length >= 1:\n                    lag_14_array[theater_index] = theater_history[-1]\n                else:\n                    lag_14_array[theater_index] = 0\n            \n            # lag 21 (21 days ago)\n            if history_length >= 21:\n                lag_21_array[theater_index] = theater_history[-21]\n            else:\n                if history_length >= 1:\n                    lag_21_array[theater_index] = theater_history[-1]\n                else:\n                    lag_21_array[theater_index] = 0\n            \n            # lag 28 (28 days ago)\n            if history_length >= 28:\n                lag_28_array[theater_index] = theater_history[-28]\n            else:\n                if history_length >= 1:\n                    lag_28_array[theater_index] = theater_history[-1]\n                else:\n                    lag_28_array[theater_index] = 0\n            \n            # lag 30 (30 days ago)\n            if history_length >= 30:\n                lag_30_array[theater_index] = theater_history[-30]\n            else:\n                if history_length >= 1:\n                    lag_30_array[theater_index] = theater_history[-1]\n                else:\n                    lag_30_array[theater_index] = 0\n            \n            # rolling features\n            if history_length >= 1:\n                # last 7 days\n                if history_length >= 7:\n                    last_7_days = theater_history[-7:]\n                else:\n                    last_7_days = theater_history\n                \n                # last 14 days\n                if history_length >= 14:\n                    last_14_days = theater_history[-14:]\n                else:\n                    last_14_days = theater_history\n                \n                # last 30 days\n                if history_length >= 30:\n                    last_30_days = theater_history[-30:]\n                else:\n                    last_30_days = theater_history\n                \n                # calculate mean and std\n                roll_mean_7_array[theater_index] = np.mean(last_7_days)\n                roll_mean_14_array[theater_index] = np.mean(last_14_days)\n                roll_mean_30_array[theater_index] = np.mean(last_30_days)\n                \n                if len(last_7_days) > 1:\n                    roll_std_7_array[theater_index] = np.std(last_7_days)\n                else:\n                    roll_std_7_array[theater_index] = 0\n                \n                if len(last_14_days) > 1:\n                    roll_std_14_array[theater_index] = np.std(last_14_days)\n                else:\n                    roll_std_14_array[theater_index] = 0\n                \n                if len(last_30_days) > 1:\n                    roll_std_30_array[theater_index] = np.std(last_30_days)\n                else:\n                    roll_std_30_array[theater_index] = 0\n            \n            # move to next theater\n            theater_index = theater_index + 1\n        \n        # COMBINE LAG FEATURES INTO MATRIX\n        \n        # create matrix with all lag features\n        # shape: (num_theaters, 12) because we have 12 lag/rolling features\n        lag_feature_matrix = np.zeros((num_theaters, 12))\n        \n        lag_feature_matrix[:, 0] = lag_1_array\n        lag_feature_matrix[:, 1] = lag_7_array\n        lag_feature_matrix[:, 2] = lag_14_array\n        lag_feature_matrix[:, 3] = lag_21_array\n        lag_feature_matrix[:, 4] = lag_28_array\n        lag_feature_matrix[:, 5] = lag_30_array\n        lag_feature_matrix[:, 6] = roll_mean_7_array\n        lag_feature_matrix[:, 7] = roll_std_7_array\n        lag_feature_matrix[:, 8] = roll_mean_14_array\n        lag_feature_matrix[:, 9] = roll_std_14_array\n        lag_feature_matrix[:, 10] = roll_mean_30_array\n        lag_feature_matrix[:, 11] = roll_std_30_array\n        \n        # SCALE LAG FEATURES (only thing we scale in loop!)        \n        X_dynamic_scaled = dyn_scaler.transform(lag_feature_matrix)\n        \n        # COMBINE ALL FEATURES       \n        # get pre-transformed categorical and static features for these rows\n        X_cat_today = X_test_cat[row_indices]\n        X_stat_today = X_test_stat[row_indices]\n        \n        # combining categorical + static + dynamic\n        X_today_combined = np.hstack([X_cat_today, X_stat_today, X_dynamic_scaled])\n        \n        # MAKE PREDICTIONS        \n        predictions_today = model.predict(X_today_combined)\n        \n        # make sure predictions are not negative\n        predictions_today = np.maximum(predictions_today, 0)\n        \n        # Store predictions\n        all_predictions[row_indices] = predictions_today\n        \n        # UPDATE HISTORY FOR NEXT DAY        \n        theater_index = 0\n        while theater_index < num_theaters:\n            \n            theater_id = theaters_today[theater_index]\n            predicted_value = predictions_today[theater_index]\n            \n            # add prediction to history\n            if theater_id not in current_history:\n                current_history[theater_id] = []\n            \n            current_history[theater_id].append(predicted_value)\n            \n            theater_index = theater_index + 1\n        \n        # move to next day\n        day_number = day_number + 1\n    \n    print(\"DONE\")\n    \n    return all_predictions\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission(df_template, df_predictions, id_col='ID', target_col='audience_count', out_path='submission.csv'):\n\n\n    # ENsure df_predictions only has ID + audience_count\n    df_predictions = df_predictions[[id_col, target_col]]\n\n    # drop old audience_count from orginal  \n    if target_col in df_template.columns:\n        df_template = df_template.drop(columns=[target_col])\n\n    # merge template with predictions on ID\n    final_submission = df_template.merge(\n        df_predictions,\n        on=id_col,\n        how='left'\n    )\n\n    # fill missing prediction values with 0 or any fallback\n    final_submission[target_col] = final_submission[target_col].fillna(0).astype(int)\n\n    # save\n    final_submission.to_csv(out_path, index=False)\n\n    print(f\"submission saved {out_path}\")\n    print(f\"total rows: {len(final_submission)} (should be {len(df_template)})\")\n\n    return final_submission\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # run Model 1: ExtraTrees\n# print(\"MODEL 1: ExtraTrees\")\n# predictions_model_1 = making_predictions(\n#     model=model_1_extratrees,\n#     model_name=\"ExtraTrees\",\n#     test_dataframe=t_df.copy(),\n#     history_dictionary=history_dict,\n#     X_test_cat=X_test_categorical,\n#     X_test_stat=X_test_static,\n#     dyn_scaler=dynamic_scaler,\n#     dyn_cols=dynamic_columns\n# )\n\n# # save submission for Model 1\n# submission_1 = pd.DataFrame()\n# submission_1['ID'] = t_df['ID']\n# submission_1['audience_count'] = np.round(predictions_model_1).astype(int)\n\n# # create final aligned submission\n# final_sub = create_submission(\n#     df_template=df8,\n#     df_predictions=submission_1,\n#     id_col='ID',\n#     target_col='audience_count',\n#     out_path='submission.csv'\n# )\n# print(\"done\")\n\n### final r2 score = 0.39228\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # run Model 2: LightGBM\nprint(\"MODEL 2: LightGBM\")\npredictions_model_2 = making_predictions(\n    model=model_2_lightgbm,\n    model_name=\"LightGBM\",\n    test_dataframe=t_df.copy(),\n    history_dictionary=history_dict,\n    X_test_cat=X_test_categorical,\n    X_test_stat=X_test_static,\n    dyn_scaler=dynamic_scaler,\n    dyn_cols=dynamic_columns\n)\n\n# save submission for Model 2\nsubmission_2 = pd.DataFrame()\nsubmission_2['ID'] = t_df['ID']\nsubmission_2['audience_count'] = np.round(predictions_model_2).astype(int)\n\n# create final aligned submission\nfinal_sub = create_submission(\n    df_template=df8,\n    df_predictions=submission_2,\n    id_col='ID',\n    target_col='audience_count',\n    out_path='submission.csv'\n)\nprint(\"done\")\n\n\n# final r2 score = 0.41477","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # run Model 3: HistGradientBoosting\n# print(\"MODEL 3: HistGradientBoosting \")\n# predictions_model_3 = making_predictions(\n#     model=model_3_histgb,\n#     model_name=\"HistGradientBoosting\",\n#     test_dataframe=t_df.copy(),\n#     history_dictionary=history_dict,\n#     X_test_cat=X_test_categorical,\n#     X_test_stat=X_test_static,\n#     dyn_scaler=dynamic_scaler,\n#     dyn_cols=dynamic_columns\n# )\n\n\n# # save submission for Model 2\n# submission_3 = pd.DataFrame()\n# submission_3['ID'] = t_df['ID']\n# submission_3['audience_count'] = np.round(predictions_model_3).astype(int)\n\n# # create final aligned submission\n# final_sub = create_submission(\n#     df_template=df8,\n#     df_predictions=submission_3,\n#     id_col='ID',\n#     target_col='audience_count',\n#     out_path='submission.csv'\n# )\n# print(\"done\")\n\n\n# final r2 score = 0.41323","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # run Model 4: XGBoost\n# print(\"MODEL 4: XGBoost \")\n# predictions_model_4 = making_predictions(\n#     model=model_4_xgboost,\n#     model_name=\"XGBoost\",\n#     test_dataframe=t_df.copy(),\n#     history_dictionary=history_dict,\n#     X_test_cat=X_test_categorical,\n#     X_test_stat=X_test_static,\n#     dyn_scaler=dynamic_scaler,\n#     dyn_cols=dynamic_columns\n# )\n\n\n# # save submission for Model 2\n# submission_4 = pd.DataFrame()\n# submission_4['ID'] = t_df['ID']\n# submission_4['audience_count'] = np.round(predictions_model_4).astype(int)\n\n# # create final aligned submission\n# final_sub = create_submission(\n#     df_template=df8,\n#     df_predictions=submission_4,\n#     id_col='ID',\n#     target_col='audience_count',\n#     out_path='submission.csv'\n# )\n# print(\"done\")\n\n\n# final r2 score = 0.40651","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Ensemble strategy\n### skipping Extra trees it is also taking too much time and slightly lower score than lightgbm, xgboost and hgb\n## total weight = 1.23451\nweight_of_lgbm = 0.41477/1.23451\nprint(weight_of_lgbm)\nweight_of_hgb = 0.41323/1.23451\nprint(weight_of_hgb)\nweight_of_xgb = 0.40651/1.23451\nprint(weight_of_xgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ensemble_predictions = ( (weight_of_lgbm * predictions_model_2) + (weight_of_hgb*predictions_model_3) + (weight_of_xgb * predictions_model_4)) \n\n# # Save ensemble submission\n# submission_ensemble = pd.DataFrame()\n# submission_ensemble['ID'] = t_df['ID']\n# submission_ensemble['audience_count'] = np.round(ensemble_predictions).astype(int)\n\n# # create final aligned submission\n# final_sub = create_submission(\n#     df_template=df8,\n#     df_predictions=submission_ensemble,\n#     id_col='ID',\n#     target_col='audience_count',\n#     out_path='submission.csv'\n# )\n# print(\"done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### finally individual score of lightgbm scores best of 0.41477","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
