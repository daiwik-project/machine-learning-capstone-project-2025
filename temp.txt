# machine-learning-capstone-project-2025
---
# Tutorial: machine-learning-capstone-project-2025

The `machine-learning-capstone-project-2025` project focuses on **forecasting cinema audience counts**. It systematically *processes raw data* through a defined pipeline, *engineers crucial time-series features* like lags and rolling averages, and then uses an *iterative prediction mechanism* that continuously updates historical data with new forecasts. Multiple machine learning models are *evaluated and fine-tuned*, with the best-performing ones ultimately *combined into an ensemble* for more robust and accurate predictions.


## Visual Overview

```mermaid
flowchart TD
    A0["Data Preprocessing Pipeline
"]
    A1["Time-Series Feature Engineering
"]
    A2["Iterative Time-Series Prediction
"]
    A3["Historical Data Management for Prediction
"]
    A4["Model Evaluation and Selection Strategy
"]
    A5["Ensemble Modeling Strategy
"]
    A0 -- "Prepares data for" --> A1
    A2 -- "Calculates features via" --> A1
    A2 -- "Updates and uses history from" --> A3
    A4 -- "Utilizes preprocessed data ..." --> A0
    A4 -- "Informs model selection for" --> A5
    A5 -- "Combines predictions from" --> A2
```

## Chapters

1. [Data Preprocessing Pipeline
](01_data_preprocessing_pipeline_.md)
2. [Historical Data Management for Prediction
](02_historical_data_management_for_prediction_.md)
3. [Time-Series Feature Engineering
](03_time_series_feature_engineering_.md)
4. [Model Evaluation and Selection Strategy
](04_model_evaluation_and_selection_strategy_.md)
5. [Iterative Time-Series Prediction
](05_iterative_time_series_prediction_.md)
6. [Ensemble Modeling Strategy
](06_ensemble_modeling_strategy_.md)




---
<div style="page-break-before: always;"></div>


<a name="chapter1"></a>
# Chapter 1: Data Preprocessing Pipeline

Welcome to the first chapter of our machine learning capstone project! Here, we'll dive into "Data Preprocessing Pipeline," a fundamental concept in making sure our machine learning models get the best possible data to learn from.

### Why is Data Preprocessing So Important?

Imagine you're a chef preparing a delicious meal. You wouldn't just throw raw, unwashed ingredients into a pot, right? You'd clean the vegetables, chop them, measure out spices, and perhaps marinate the meat. Data preprocessing is exactly like that for a machine learning model.

Our goal in this project is to predict the number of people who will attend a cinema show. We have a lot of raw data about movie theaters, dates, and past audience counts. This raw data, straight out of the database, is often messy. It might have:
*   **Missing information**: What if we don't know the exact location of a theater?
*   **Numbers with very different scales**: Is a theater's latitude (e.g., `22.6`) as important as the number of tickets sold (`1350`)?
*   **Text descriptions**: How does a computer understand "Drama" or "Action" as a theater type?

Our "Data Preprocessing Pipeline" is an automated assembly line that takes these "raw ingredients" (our messy data) and transforms them into a "standardized, usable format" that our machine learning model can easily understand and learn from.

Let's break down the key steps in this "assembly line."

### Key Concepts in Data Preprocessing

To make our raw data clean and understandable for the model, we perform several crucial steps:

#### 1. Handling Missing Values

Sometimes, our data might have gaps. For example, a theater's exact `latitude` or `longitude` might be missing, or the `theater_area` might not be recorded. Machine learning models usually can't work with missing data.

*   **The Problem**: Missing values can cause errors or make our model perform poorly.
*   **The Solution**: We need to "fill in the blanks."
    *   For **numerical features** (like `latitude` and `longitude`), we can often replace missing values with the `median` (the middle value) from all the available data. This is a safe choice because it's not easily affected by extreme values.
    *   For **categorical features** (like `theater_area` or `theater_type`, which are text labels), we can replace missing values with a special label like `'unknown'`. This tells the model that there was missing information for that specific data point.

**Example from our project data (`df2` - BookNow Theaters):**

In `df2`, we found that `book_theater_id` had many missing values, which could imply theaters without booking data in this specific dataset. `latitude` and `longitude` are also common to have missing values if a theater's exact location isn't recorded.
And for `theater_area` or `theater_type`, if a record misses this detail, we'd mark it as `'unknown'`.

```python
# Assuming 'f_t_df' is our combined dataframe
# Replacing missing latitude/longitude with the median
f_t_df['latitude'] = f_t_df['latitude'].fillna(f_t_df['latitude'].median())
f_t_df['longitude'] = f_t_df['longitude'].fillna(f_t_df['longitude'].median())

# Replacing missing categorical areas/types with 'unknown'
f_t_df['theater_area'] = f_t_df['theater_area'].fillna('unknown_area')
f_t_df['theater_type'] = f_t_df['theater_type'].fillna('unknown_type')

# Output: After running this, the 'latitude', 'longitude', 'theater_area',
# and 'theater_type' columns in f_t_df will no longer have missing values.
# Numerical missing values are replaced by the median from the existing data,
# and categorical missing values are replaced by the string 'unknown_area' or 'unknown_type'.
```

#### 2. Scaling Numerical Features

Machine learning models, especially those that calculate distances or rely on the magnitude of numbers (like some linear models), can get confused if one feature has very large values and another has very small values.

*   **The Problem**: Features with larger values might unfairly dominate the model's learning process. For example, `audience_count` could be 1350, while `latitude` is 22.
*   **The Solution**: We scale numerical features to bring them into a similar range. A common technique is `MinMaxScaler`, which transforms all values to a range between 0 and 1.

Imagine measuring ingredients for a recipe. Some might be in cups, others in teaspoons. Scaling is like converting all measurements to a common unit so you can compare them fairly.

```python
from sklearn.preprocessing import MinMaxScaler

# Example of scaling a numerical column
# We apply MinMaxScaler to columns like 'latitude', 'longitude', 'show_day', etc.
# This makes their values fall between 0 and 1.
scaler = MinMaxScaler()
f_t_df['latitude_scaled'] = scaler.fit_transform(f_t_df[['latitude']])

# Output: The 'latitude_scaled' column will now contain values between 0 and 1.
# This makes sure that latitude doesn't disproportionately influence the model
# just because its raw values might be larger than, say, 'is_weekend' (which is 0 or 1).
```

#### 3. Encoding Categorical Features

Machine learning models understand numbers, not text. So, how do we tell a model that a `theater_type` can be "Drama," "Action," or "Comedy"?

*   **The Problem**: Text labels like "Drama" don't have inherent numerical meaning for a model.
*   **The Solution**: We convert these text categories into numerical representations. `OneHotEncoder` is a popular method. It creates new binary (0 or 1) columns for each unique category.

Think of it like assigning a unique "on/off" switch for each type. If a theater is a "Drama" type, the "Drama" switch is "on" (1), and all other type switches are "off" (0).

**Example from our project data (`df2` - BookNow Theaters):**

The `theater_type` column could have values like 'Drama', 'Other', 'Action', 'Comedy'.
`OneHotEncoder` would transform this into new columns:

| theater_type | theater_type_Drama | theater_type_Other | theater_type_Action | theater_type_Comedy |
| :----------- | :----------------- | :----------------- | :------------------ | :------------------ |
| Drama        | 1                  | 0                  | 0                   | 0                   |
| Action       | 0                  | 0                  | 1                   | 0                   |
| Comedy       | 0                  | 0                  | 0                   | 1                   |
| Other        | 0                  | 1                  | 0                   | 0                   |

```python
from sklearn.preprocessing import OneHotEncoder

# Example of one-hot encoding a categorical column
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = encoder.fit_transform(f_t_df[['theater_type']])

# The output `encoded_features` is a NumPy array.
# For example, if a theater type was 'Drama', and 'Drama' is the first category it learned,
# a row in this array might look like [1.0, 0.0, 0.0, 0.0].
# This converts the text into a format the model can use.
```

### The Data Preprocessing Pipeline in Action

The "Data Preprocessing Pipeline" ties all these steps together. It's a sequence of operations that automatically cleans and transforms our data in the correct order. This ensures that every time we feed new raw data into our model (e.g., when making predictions for future dates), it goes through the exact same preparation steps.

#### How it works (Step-by-Step Walkthrough):

1.  **Raw Data In**: Our initial messy data (like `booknow_theaters.csv`, `date_info.csv`, `booknow_visits.csv`) is loaded.
2.  **Combine Data**: We combine relevant data from various files into one large table.
3.  **Handle Missing Values**: The pipeline first identifies and fills any missing numerical values (e.g., `latitude`, `longitude`) with their medians. Then, it handles missing categorical values (e.g., `theater_area`, `theater_type`) by marking them as 'unknown'.
4.  **Scale Numerical Features**: Next, all numerical columns (like our geographical coordinates, day, month, year, etc.) are scaled to a consistent range (0 to 1) using `MinMaxScaler`.
5.  **Encode Categorical Features**: Finally, categorical text columns (`day_of_week`, `theater_type`, `theater_area`) are converted into multiple binary numerical columns using `OneHotEncoder`.

This sequence ensures that by the end, our data is clean, consistent, and ready for our machine learning models.

Here's a simple diagram to visualize this process:

```mermaid
sequenceDiagram
    Actor RawData
    Participant MissingHandler
    Participant Scaler
    Participant Encoder
    Actor ProcessedData

    RawData->MissingHandler: Unprepared data (with missing entries)
    Note over MissingHandler: Fills missing numbers (e.g., with median) <br> and categories (e.g., with 'unknown')
    MissingHandler->Scaler: Cleaned data
    Note over Scaler: Adjusts numerical ranges (e.g., 0-1)
    Scaler->Encoder: Scaled numerical data
    Note over Encoder: Converts text labels to binary columns
    Encoder->ProcessedData: Data ready for ML!
```

#### Code Implementation

In our project, we use `sklearn.compose.ColumnTransformer` and `sklearn.pipeline.Pipeline` to build this preprocessing flow. `ColumnTransformer` is especially useful because it lets us apply different transformations to different columns (e.g., `MinMaxScaler` for numbers, `OneHotEncoder` for categories) all at once.

First, we separate our columns into different types:

```python
# From notebook cell [71]
categorical_columns = ['book_theater_id', 'day_of_week', 'theater_type', 'theater_area']
static_numerical_columns = ['latitude', 'longitude', 'show_day', 'show_month', 'show_year',
                            'encoded_day_of_week', 'day_of_year', 'week_of_year', 'quarter',
                            'is_weekend', 'is_sunday', 'is_friday', 'dow_sin', 'dow_cos',
                            'month_sin', 'month_cos', 'doy_sin', 'doy_cos',
                            'encoded_theater_type', 'encoded_theater_area']
dynamic_columns = ['audience_lag_1', 'audience_lag_7', 'audience_lag_14',
                   'audience_lag_21', 'audience_lag_28', 'audience_lag_30',
                   'audience_roll_mean_7', 'audience_roll_std_7',
                   'audience_roll_mean_14', 'audience_roll_std_14',
                   'audience_roll_mean_30', 'audience_roll_std_30']

# Output: These lists define which columns are handled by which preprocessor.
```

Next, we define our individual transformers (like the `MinMaxScaler` and `OneHotEncoder`) and combine them using `ColumnTransformer` to create our full preprocessor.

```python
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# From notebook cell [75]
ohe_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
static_scaler = MinMaxScaler()
dynamic_scaler = MinMaxScaler()

# From notebook cell [52] (simplified for tutorial)
preprocessor = ColumnTransformer(
    transformers=[
        ('num_static', static_scaler, static_numerical_columns),
        ('num_dynamic', dynamic_scaler, dynamic_columns),
        ('cat', ohe_encoder, categorical_columns)
    ],
    remainder='passthrough' # Keep other columns (like 'ID', 'show_date') as is
)

# Output: 'preprocessor' is now an object that knows exactly how to apply
# scaling to numerical features and one-hot encoding to categorical features.
# It's like a blueprint for our data preparation.
```

This `preprocessor` object then becomes the first step in our full machine learning pipeline. When we call `preprocessor.fit_transform(our_data)`, it automatically performs all these steps on the specified columns, returning a clean, numerical dataset ready for model training.

### Conclusion

In this chapter, we learned that a "Data Preprocessing Pipeline" is essential for preparing raw data for machine learning. We explored its core components: handling missing values, scaling numerical features, and encoding categorical features. Understanding these steps ensures that our models receive high-quality data, leading to better predictions.

Next, we'll dive into how we manage and use historical data for making predictions, which is crucial for time-series forecasting like our cinema audience project.

[Historical Data Management for Prediction](#chapter2)


---
<div style="page-break-before: always;"></div>








<a name="chapter2"></a>
# Chapter 2: Historical Data Management for Prediction

Welcome back! In [Chapter 1: Data Preprocessing Pipeline](#chapter1), we learned how to clean and prepare our raw data, transforming it into a neat, standardized format ready for our machine learning models. Now, we're going to build on that foundation by exploring "Historical Data Management for Prediction."

### Why is Historical Data Management So Important?

Imagine you're trying to predict how many people will attend a specific movie show at a particular theater next Tuesday. You wouldn't just pull a number out of thin air, right? You'd think: "How many people usually come to this theater on a Tuesday? What were the audience numbers for the past few Tuesdays, or even past few days?"

This is exactly what "Historical Data Management" is all about in our project. Our goal is to predict future `audience_count` values for each `book_theater_id`. To do this accurately, our models need to "remember" past `audience_count` values for *each individual theater*. It's like keeping a detailed logbook or diary for every single theater's daily audience performance.

The big challenge is not just storing this past data, but also being able to efficiently use it to make new predictions, and then **update** that logbook with the latest information (even if it's a prediction itself!). This continuous update is key for making predictions far into the future, day by day.

### Key Concepts

This abstraction helps us keep track of all the past `audience_count` values for every theater. Let's break it down:

#### 1. The `history_dict` – Our Theater Logbook

At the heart of our historical data management is a special Python dictionary called `history_dict`.

*   **What it is**: Think of `history_dict` as a big collection of personal diaries, one for each `book_theater_id` (each unique theater).
*   **How it's structured**:
    *   **Keys**: Each key in this dictionary is a `book_theater_id` (e.g., `'book_00001'`, `'book_00244'`). This is how we find a specific theater's diary.
    *   **Values**: The value for each key is a `list` of numbers. This list contains all the historical `audience_count` values for that specific theater, ordered from the oldest to the newest day.

#### 2. Storing Initial Past Data

Before we even start predicting, we need to fill our `history_dict` with all the *actual* past `audience_count` data we already have. This gives our logbooks a starting point. We collect all known historical audience numbers for each theater and put them into their respective lists.

#### 3. Dynamic Updating: Learning from the Future (Even if it's a Guess!)

This is the most crucial part for time-series forecasting. When we predict the `audience_count` for a theater for "Day X", that prediction itself becomes the "most recent past data" for that theater when we predict for "Day X+1".

*   **The Process**:
    1.  We predict `audience_count` for a specific theater for a given date.
    2.  As soon as we have this prediction, we add it to the end of that theater's list in our `history_dict`.
    3.  When we move to the next date, the model can now use this *newly predicted* value (along with older actual values) to calculate new features (like "what was the audience yesterday?") and make its next prediction.

This dynamic updating ensures that our predictions always leverage the most current available (or predicted) information, which is essential for accurate time-series forecasting.

### How to Use the `history_dict` for Prediction

Let's walk through how `history_dict` is used in a typical prediction scenario, where we need to predict `audience_count` day by day for all theaters for a future period.

#### Use Case: Predicting Audience for the Next 7 Days

Imagine we need to predict the `audience_count` for each theater for every day in the upcoming week.

**Step-by-Step Walkthrough:**

1.  **Start with a Prediction Date**: We pick the first day we want to predict (e.g., Monday).
2.  **Gather Theaters for Today**: Identify all the `book_theater_id`s we need to predict for on this Monday.
3.  **For Each Theater, Access its Logbook**:
    *   We go to our `history_dict` and find the list of past `audience_count`s for that specific theater.
    *   We use this list to calculate important features for our model, such as:
        *   **Lag features**: What was the audience 1 day ago? 7 days ago? (We'll learn more about these in [Chapter 3: Time-Series Feature Engineering](03_time_series_feature_engineering_.md)).
        *   **Rolling features**: What was the average audience over the last 7 days? The standard deviation?
    *   Our model (from [Chapter 4: Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md)) then takes these calculated features and other preprocessed data (from [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md)) to make a prediction for this theater for Monday.
4.  **Update the Logbook**:
    *   As soon as we have the prediction for this theater for Monday, we immediately add this predicted `audience_count` to the end of that theater's list in `history_dict`.
    *   We repeat this for all theaters for Monday.
5.  **Move to the Next Day**: Now it's Tuesday. When we go to predict for Tuesday, the `history_dict` *already includes* Monday's predictions. This allows the model to calculate the "yesterday's audience" (which is Monday's predicted audience) and other features based on the most up-to-date information.
6.  **Repeat**: This process continues day by day, dynamically updating the `history_dict` with each new prediction, forming an [Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md) process.

This approach is powerful because it simulates how real-world forecasting works, where new information becomes available and influences subsequent decisions.

### Under the Hood: How it Works

Let's look at a simplified view of the `history_dict` in action.

#### 1. Initializing `history_dict`

First, we populate `history_dict` with all the known historical `audience_count` values from our training data (`f_t_df`).

```python
# Assuming f_t_df is our combined and sorted dataframe with 'book_theater_id', 'show_date', 'audience_count'

history_dict = {}

# Get all unique theater IDs from our data
all_theaters = f_t_df['book_theater_id'].unique()

# Loop through each theater to build its history
for theater_id in all_theaters:
    # Filter data for this specific theater
    theater_data = f_t_df[f_t_df['book_theater_id'] == theater_id]
    # Sort by date to ensure correct order
    theater_data = theater_data.sort_values('show_date')
    # Get the list of audience counts and store it
    audience_list = theater_data['audience_count'].tolist()
    history_dict[theater_id] = audience_list

print(f"Total number of theaters with initial history: {len(history_dict)}")
print(f"Example: 'book_00001' has {len(history_dict.get('book_00001', []))} days of history.")

# Output:
# Total number of theaters with initial history: 826
# Example: 'book_00001' has 342 days of history.
```

Now, `history_dict` is ready, with each theater's past audience counts stored as a list.

#### 2. The Daily Prediction Loop (Simplified)

This diagram shows the core steps for a single prediction day for one theater. This entire process would then be repeated for all theaters for the current date, and then for all subsequent dates in our prediction period.

```mermaid
sequenceDiagram
    participant ForecastSystem as "Forecast System"
    participant DailyLoop as "Daily Prediction Loop"
    participant TheaterHistory as "history_dict"
    participant FeatureEngineer as "Feature Engineer"
    participant MLModel as "ML Model"

    ForecastSystem->DailyLoop: "Predict for Current Date"
    DailyLoop->TheaterHistory: "Get history for Theater X"
    Note over TheaterHistory: [..., audience_day_N-1]
    TheaterHistory-->DailyLoop: [historical_counts_X]
    DailyLoop->FeatureEngineer: "Calculate lag/roll features from [historical_counts_X]"
    FeatureEngineer-->DailyLoop: [calculated_features_X]
    DailyLoop->MLModel: "Predict audience with [calculated_features_X] + other features"
    MLModel-->DailyLoop: predicted_audience_X
    DailyLoop->TheaterHistory: "Add predicted_audience_X to history for Theater X"
    Note over TheaterHistory: history_dict['TheaterX'] is now [..., audience_day_N-1, predicted_audience_X]
    DailyLoop-->ForecastSystem: "Prediction for Theater X done"
```

#### 3. Code for the Prediction Loop

Here's a simplified look at the `make_predictions_simple` function, focusing on how `history_dict` is used.

```python
# t_df will be our dataframe for the future dates we need to predict
# X_test_categorical, X_test_static are already preprocessed static/categorical features for t_df
# dyn_scaler is the MinMaxScaler fitted on dynamic features from training data

def make_predictions_simple(model, test_dataframe, history_dictionary, X_test_cat, X_test_stat, dyn_scaler, dynamic_columns):
    # Make a copy of history_dictionary so the original is untouched
    current_history = {k: v.copy() for k, v in history_dictionary.items()}

    # Sort test data by date to process day-by-day
    test_dataframe = test_dataframe.sort_values(['show_date', 'book_theater_id']).reset_index(drop=True)
    all_unique_dates = sorted(test_dataframe['show_date'].unique())
    all_predictions = np.zeros(len(test_dataframe)) # To store all predictions

    for current_date in all_unique_dates:
        date_mask = test_dataframe['show_date'] == current_date
        row_indices = np.where(date_mask)[0] # Get rows for current date
        theaters_today = test_dataframe.loc[row_indices, 'book_theater_id'].values
        num_theaters = len(theaters_today)

        # Prepare arrays for dynamic (lag/roll) features for today's predictions
        # These arrays will be filled by looking up current_history
        lag_feature_matrix = np.zeros((num_theaters, len(dynamic_columns)))

        for i, theater_id in enumerate(theaters_today):
            theater_history = current_history.get(theater_id, [])
            history_length = len(theater_history)

            # --- Accessing history to calculate dynamic features ---
            # Simplified: just using last known audience (actual or predicted)
            if history_length > 0:
                lag_feature_matrix[i, 0] = theater_history[-1] # audience_lag_1
            # In a real scenario, you'd calculate all lags and rolling features here
            # For this tutorial, we focus on the history lookup and update logic.

        # --- Scale dynamic features ---
        X_dynamic_scaled = dyn_scaler.transform(lag_feature_matrix)

        # --- Combine all features for prediction ---
        X_cat_today = X_test_cat[row_indices]
        X_stat_today = X_test_stat[row_indices]
        X_today_combined = np.hstack([X_cat_today, X_stat_today, X_dynamic_scaled])

        # --- Make predictions ---
        predictions_today = model.predict(X_today_combined)
        predictions_today = np.maximum(predictions_today, 0) # Ensure no negative predictions
        all_predictions[row_indices] = predictions_today # Store predictions

        # --- Dynamically update history for the next day's predictions ---
        for i, theater_id in enumerate(theaters_today):
            if theater_id not in current_history:
                current_history[theater_id] = []
            current_history[theater_id].append(predictions_today[i]) # Add the new prediction

    return all_predictions

# Example usage (assuming model_2_lightgbm, t_df, etc., are defined):
# predictions_model_2 = make_predictions_simple(
#     model=model_2_lightgbm,
#     test_dataframe=t_df.copy(),
#     history_dictionary=history_dict,
#     X_test_cat=X_test_categorical,
#     X_test_stat=X_test_static,
#     dyn_scaler=dynamic_scaler,
#     dynamic_columns=dynamic_columns
# )
# print("Predictions made using LightGBM and dynamic history.")

# Output:
# Predictions made using LightGBM and dynamic history.
```

This function demonstrates the core logic: for each day, it fetches the latest history (which might include previous days' predictions), calculates features, makes predictions, and immediately updates the `history_dict` with these new predictions.

### Conclusion

In this chapter, we introduced "Historical Data Management for Prediction," focusing on how we store and dynamically update past `audience_count` data using a `history_dict`. This system is vital for time-series forecasting, allowing our models to continually learn from and leverage the most current available information. Understanding this dynamic process is key to building robust predictive systems that can forecast effectively into the future.

Next, we'll dive deeper into exactly *what kinds* of features we extract from this historical data to give our models the best possible insights.

[Time-Series Feature Engineering](03_time_series_feature_engineering_.md)

---

<div style="page-break-before: always;"></div>






















<a name="chapter3"></a>
# Chapter 3: Time-Series Feature Engineering

Welcome back! In [Chapter 2: Historical Data Management for Prediction](02_historical_data_management_for_prediction_.md), we learned how to gather and store past `audience_count` values for each theater in our `history_dict`. This is like having a detailed diary for each theater. But how do we make our machine learning models actually *understand* what's important in these diaries and in the dates themselves?

### Why is Time-Series Feature Engineering So Important?

Imagine you're trying to predict next Tuesday's audience for a cinema show. Just looking at the `show_date` as "2024-03-05" and the raw `audience_count` from past days isn't enough. A computer sees "2024-03-05" as just a string, and "50" as just a number. It doesn't inherently understand that:
*   Tuesday might have different audience patterns than Saturday.
*   The first week of the month might be busier than the last.
*   What happened **yesterday** (a 'lag' feature) is super important for today's prediction.
*   The **average audience over the last 7 days** (a 'rolling' feature) tells us about recent trends.

**"Time-Series Feature Engineering"** is like adding spices and cooking techniques to bring out the flavor of *time* in our data. It's the process of creating new, smart features from our `show_date` and `audience_count` columns. These new features help our model find hidden patterns and dependencies that are crucial for making accurate forecasts.

Let's dive into the different "spices" and "techniques" we can apply!

### Key Concepts in Time-Series Feature Engineering

We'll focus on three main ways to engineer features from `show_date` and `audience_count`:

1.  **Extracting Date Components**: Turning a single date into many descriptive numbers.
2.  **Cyclical Encodings**: Helping the model understand repeating patterns like days of the week or months.
3.  **Lag and Rolling Features**: Using past `audience_count` values to predict the future.

---

#### 1. Extracting Date Components: Making Dates More Descriptive

A single `show_date` (like `2024-03-05`) contains a lot of information. We can pull out different parts of it to create new numerical features.

**Why it's useful:**
*   A model can learn if people prefer movies on the weekend (`is_weekend`).
*   It can spot trends for certain months (`show_month`) or specific days of the month (`show_day`).

**Example: Turning a date into many numbers**

| `show_date`  | `show_day` | `show_month` | `show_year` | `day_of_week` | `is_weekend` |
| :----------- | :--------- | :----------- | :---------- | :------------ | :----------- |
| 2024-03-01   | 1          | 3            | 2024        | Friday (4)    | 0            |
| 2024-03-02   | 2          | 3            | 2024        | Saturday (5)  | 1            |
| 2024-03-03   | 3          | 3            | 2024        | Sunday (6)    | 1            |

Here's how we extract these features using Python:

```python
# Assuming f_t_df is our main DataFrame with a 'show_date' column

# Make sure 'show_date' is in datetime format
f_t_df['show_date'] = pd.to_datetime(f_t_df['show_date'])

# Extract basic date components
f_t_df['show_day'] = f_t_df['show_date'].dt.day          # Day of the month (1-31)
f_t_df['show_month'] = f_t_df['show_date'].dt.month      # Month of the year (1-12)
f_t_df['show_year'] = f_t_df['show_date'].dt.year        # Year (e.g., 2024)

# Day of the week (Monday=0, Sunday=6)
f_t_df['encoded_day_of_week'] = f_t_df['show_date'].dt.dayofweek
f_t_df['day_of_year'] = f_t_df['show_date'].dt.dayofyear  # Day of the year (1-365)
f_t_df['week_of_year'] = f_t_df['show_date'].dt.isocalendar().week # Week of the year
f_t_df['quarter'] = f_t_df['show_date'].dt.quarter        # Quarter of the year (1-4)

# Create 'is_weekend' indicator (0 for weekday, 1 for weekend)
f_t_df['is_weekend'] = (f_t_df['encoded_day_of_week'] >= 5).astype(int)
f_t_df['is_sunday'] = (f_t_df['encoded_day_of_week'] == 6).astype(int)
f_t_df['is_friday'] = (f_t_df['encoded_day_of_week'] == 4).astype(int)

# Output: The f_t_df DataFrame now has several new columns like 'show_day',
# 'show_month', 'is_weekend', etc., filled with numerical values derived from the 'show_date'.
```
These new columns give our model more information about the specific timing of each show.

---

#### 2. Cyclical Encodings: Understanding Repeating Patterns

Imagine `day_of_week` goes from Monday (0) to Sunday (6). A machine learning model might think that Sunday (6) is "much bigger" or "further away" from Monday (0) than Saturday (5) is. But in reality, Sunday is followed by Monday, creating a cycle.

**Why it's useful:**
*   To correctly represent cyclical patterns (like days of the week, months, or day of year) where the start and end values are close to each other.
*   This prevents the model from assigning too much importance to the numerical distance between cyclical values.

We use sine and cosine transformations to achieve this. It's like plotting the days of the week on a circle instead of a straight line.

```python
import numpy as np # Used for mathematical operations like sin and cos

# Cyclical encodings for day of week
# 'dow_sin' and 'dow_cos' represent 'encoded_day_of_week' on a circle
f_t_df['dow_sin'] = np.sin(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)
f_t_df['dow_cos'] = np.cos(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)

# Cyclical encodings for month
# 'month_sin' and 'month_cos' represent 'show_month' on a circle
f_t_df['month_sin'] = np.sin(2 * np.pi * f_t_df['show_month'] / 12)
f_t_df['month_cos'] = np.cos(2 * np.pi * f_t_df['show_month'] / 12)

# Cyclical encodings for day of year
# 'doy_sin' and 'doy_cos' represent 'day_of_year' on a circle (assuming 365 days)
f_t_df['doy_sin'] = np.sin(2 * np.pi * f_t_df['day_of_year'] / 365)
f_t_df['doy_cos'] = np.cos(2 * np.pi * f_t_df['day_of_year'] / 365)

# Output: For each original cyclical feature (like 'encoded_day_of_week'),
# two new features (sine and cosine transformed) are added. These help the model
# understand the "cyclical" nature of time.
```

---

#### 3. Lag and Rolling Features: Using Past Audience Counts

These are perhaps the most powerful features for time-series forecasting. They use *past* values of our target variable (`audience_count`) to predict *future* values.

**Why it's useful:**
*   **Lag Features**: "How many people came to this theater *yesterday*?" or "How many came *exactly one week ago*?". Past audience numbers are often the best predictors of future ones.
*   **Rolling Features**: "What was the *average audience* over the *last 7 days*?" or "How much did the audience *vary* (standard deviation) over the *last 30 days*?". These capture trends and volatility.

These features need `audience_count` data from previous days. Since we're making predictions for *each specific theater*, we calculate these features by grouping the data by `book_theater_id`.

**Example: Creating lag features**

```python
# First, sort the data by theater and then by date - this is crucial!
f_t_df = f_t_df.sort_values(['book_theater_id', 'show_date']).reset_index(drop=True)

# Create lag features (past audience counts)
# For each theater, 'shift' the 'audience_count' column by a specific number of days.
for lag in [1, 7, 14, 21, 28, 30]:
    f_t_df[f'audience_lag_{lag}'] = f_t_df.groupby('book_theater_id')['audience_count'].shift(lag)

# Output: New columns like 'audience_lag_1' (yesterday's audience),
# 'audience_lag_7' (audience 7 days ago), etc., are added to f_t_df.
# Rows where there's no historical data for a specific lag will have NaN values.
```

**Example: Creating rolling features**

Rolling features involve calculating statistics (like mean or standard deviation) over a "rolling window" of past data.

```python
# Helper functions to calculate rolling mean and standard deviation
# .shift(1) ensures we only use *past* data (excluding the current day)
def roll_mean(window, x):
    return x.shift(1).rolling(window, min_periods=1).mean()

def roll_std(window, x):
    return x.shift(1).rolling(window, min_periods=1).std()

# Create rolling features for different time windows
for window in [7, 14, 30]:
    # Calculate rolling mean for audience count over the specified window
    f_t_df[f'audience_roll_mean_{window}'] = (
        f_t_df.groupby('book_theater_id')['audience_count']
               .transform(lambda x: roll_mean(window, x))
    )
    # Calculate rolling standard deviation for audience count
    f_t_df[f'audience_roll_std_{window}'] = (
        f_t_df.groupby('book_theater_id')['audience_count']
               .transform(lambda x: roll_std(window, x))
    )

# Output: Columns like 'audience_roll_mean_7' (average audience over the last 7 days)
# and 'audience_roll_std_14' (audience variability over the last 14 days)
# are added to f_t_df. Again, early rows will have NaN where not enough history exists.
```
These `NaN` (Not a Number) values from lags and rolling features will need to be handled, usually by filling them with a default value (like 0) before feeding them to the model. We've done this in the project code during preprocessing by filling them with 0.

---

### The Time-Series Feature Engineering Process in Action

Our "Time-Series Feature Engineering" process takes the raw data and systematically adds all these temporal "spices" to it.

#### How it works (Step-by-Step Walkthrough):

1.  **Combined Data In**: Our initial DataFrame (`f_t_df`), which has `book_theater_id`, `show_date`, and `audience_count` (along with other theater metadata), is ready.
2.  **Date Components**: The system first extracts various components from the `show_date`, like day, month, year, day of the week, etc., and also adds indicators for weekends or specific days like Friday/Sunday.
3.  **Cyclical Encoding**: Next, it applies sine and cosine transformations to cyclical date components (like day of week, month, day of year) to capture their repeating nature.
4.  **Lag Features**: It then looks back at past `audience_count` values for *each individual theater* to create 'lag' features (e.g., audience 1 day ago, 7 days ago).
5.  **Rolling Features**: Finally, it calculates 'rolling' statistics (like mean and standard deviation) over recent periods of `audience_count` for *each theater* (e.g., average audience over the last 7 days).
6.  **Enriched Data Out**: The result is a much richer dataset (`f_t_df`) with many new features that explicitly capture temporal patterns and dependencies, making it much more informative for our machine learning models.

Here's a simple diagram to visualize this process:

```mermaid
sequenceDiagram
    Actor RawCombinedData as "Raw Combined Data (f_t_df)"
    Participant DateExtractor as "Date Feature Extractor"
    Participant CyclicalEncoder as "Cyclical Encoder"
    Participant LagRollingGenerator as "Lag & Rolling Feature Generator"
    Actor EnrichedData as "Enriched Data (f_t_df)"

    RawCombinedData->DateExtractor: Data with 'show_date'
    Note over DateExtractor: Extracts day, month, year, DoW, is_weekend, etc.
    DateExtractor->CyclicalEncoder: Data with new date components
    Note over CyclicalEncoder: Adds sin/cos transformations for cycles
    CyclicalEncoder->LagRollingGenerator: Data with all date features
    Note over LagRollingGenerator: Calculates 'audience_lag_X' and 'audience_roll_mean/std_X' for each theater
    LagRollingGenerator->EnrichedData: Data ready for ML!
```

#### Code Implementation

The code snippets shown above are the core of the feature engineering. They directly operate on our `f_t_df` DataFrame to add new columns. We've defined the types of columns for preprocessing in [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md), which guides how these newly created numerical features will be scaled later.

```python
# --- Feature Engineering Code (as seen in notebook cells 35, 36, 38, 39) ---

# 1. Date Related Features
f_t_df['show_date'] = pd.to_datetime(f_t_df['show_date'])
f_t_df['show_day'] = f_t_df['show_date'].dt.day
f_t_df['show_month'] = f_t_df['show_date'].dt.month
f_t_df['show_year'] = f_t_df['show_date'].dt.year
f_t_df['encoded_day_of_week'] = f_t_df['show_date'].dt.dayofweek
f_t_df['day_of_year'] = f_t_df['show_date'].dt.dayofyear
f_t_df['week_of_year'] = f_t_df['show_date'].dt.isocalendar().week
f_t_df['quarter'] = f_t_df['show_date'].dt.quarter
f_t_df['is_weekend'] = (f_t_df['encoded_day_of_week'] >= 5).astype(int)
f_t_df['is_sunday'] = (f_t_df['encoded_day_of_week'] == 6).astype(int)
f_t_df['is_friday'] = (f_t_df['encoded_day_of_week'] == 4).astype(int)

# 2. Cyclical Encodings
f_t_df['dow_sin'] = np.sin(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)
f_t_df['dow_cos'] = np.cos(2 * np.pi * f_t_df['encoded_day_of_week'] / 7)
f_t_df['month_sin'] = np.sin(2 * np.pi * f_t_df['show_month'] / 12)
f_t_df['month_cos'] = np.cos(2 * np.pi * f_t_df['show_month'] / 12)
f_t_df['doy_sin'] = np.sin(2 * np.pi * f_t_df['day_of_year'] / 365)
f_t_df['doy_cos'] = np.cos(2 * np.pi * f_t_df['day_of_year'] / 365)

# 3. Categorical Encoding (simplified - from notebook cell 36, maps categories to numbers)
# Assumes 'theater_type_dict' and 'theater_area_dict' are created from unique values
# For instance: theater_type_dict = {'Drama': 0, 'Other': 1, ...}
# f_t_df['encoded_theater_type'] = f_t_df['theater_type'].map(theater_type_dict)
# f_t_df['encoded_theater_area'] = f_t_df['theater_area'].map(theater_area_dict)

# 4. Lag and Rolling Features (from notebook cells 38, 39)
f_t_df = f_t_df.sort_values(['book_theater_id', 'show_date']).reset_index(drop=True)
for lag in [1, 7, 14, 21, 28, 30]:
    f_t_df[f'audience_lag_{lag}'] = f_t_df.groupby('book_theater_id')['audience_count'].shift(lag)

def roll_mean(window, x): # Helper function
    return x.shift(1).rolling(window, min_periods=1).mean()
def roll_std(window, x): # Helper function
    return x.shift(1).rolling(window, min_periods=1).std()

for window in [7, 14, 30]:
    f_t_df[f'audience_roll_mean_{window}'] = \
        f_t_df.groupby('book_theater_id')['audience_count'].transform(lambda x: roll_mean(window, x))
    f_t_df[f'audience_roll_std_{window}'] = \
        f_t_df.groupby('book_theater_id')['audience_count'].transform(lambda x: roll_std(window, x))

# Output: The f_t_df DataFrame now contains a much richer set of features,
# including various date components, cyclical representations, and past audience trends.
# These new features are essential for our models to learn complex temporal patterns.
```

Remember from [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md) that any missing values introduced by lags or rolling features (especially at the beginning of each theater's time series) are typically filled (e.g., with 0 or the median) before the data is scaled. This ensures our models always receive complete and clean data.

### Conclusion

In this chapter, we explored "Time-Series Feature Engineering," a crucial step in making our temporal data truly valuable for machine learning models. We learned how to extract descriptive date components, encode cyclical patterns, and generate powerful lag and rolling features from past `audience_count` values. By enriching our data this way, we enable our models to capture the subtle nuances of time and historical trends, leading to more accurate predictions.

Next, we'll delve into how we decide which model performs best, by understanding "Model Evaluation and Selection Strategy."

[Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md)

---

<div style="page-break-before: always;"></div>






















<a name="chapter4"></a>
# Chapter 4: Model Evaluation and Selection Strategy

Welcome back! In [Chapter 3: Time-Series Feature Engineering](03_time_series_feature_engineering_.md), we learned how to extract valuable insights from our time-series data, transforming raw dates and past audience counts into powerful features. Now that our data is beautifully prepared, it's time for the next crucial step: **"Model Evaluation and Selection Strategy."**

### Why is Model Evaluation and Selection So Important?

Imagine you're a chef tasked with creating the perfect dish for a prestigious competition. You wouldn't just pick any recipe or randomly throw ingredients together, right? You'd try different recipes, taste each one, tweak the seasonings, and carefully select the dish that performs the best and truly stands out.

In machine learning, "Model Evaluation and Selection Strategy" is exactly like that. We've cleaned and enhanced our "ingredients" (data) and now we need to decide **which recipe (machine learning model) will give us the most delicious (accurate) prediction** for our cinema audience forecasting challenge. There isn't a single "best" model for every problem; we need to systematically test different ones, measure how well they perform, and pick the champion.

Our main goal in this project is to predict the `audience_count` for future cinema shows. To do this reliably, we need a strategy to:
1.  **Train various models:** Try out different types of "recipes."
2.  **Assess their performance:** "Taste" each model's predictions.
3.  **Optimize their settings:** "Tweak" the recipe to make it even better.
4.  **Select the best one(s):** Choose the winning dish(es) for the competition.

Let's dive into how we achieve this!

### Key Concepts in Model Evaluation and Selection

To find our best model, we'll explore different types of models, learn how to measure their performance, and discover techniques to optimize them.

#### 1. Different Models are Like Different Tools

Just as a chef has a variety of knives and pans, a data scientist has a variety of machine learning models. Each has its strengths!

*   **Baseline Model: The `DummyRegressor`**: This is our simplest "guess." It always predicts the average audience count from the training data. It's like predicting everyone will come exactly at the average time. It's not smart, but it sets a **minimum acceptable performance** benchmark. If our complex models can't beat this, something is wrong!
    ```python
    from sklearn.dummy import DummyRegressor
    # Example: A dummy regressor that always predicts the mean
    dummy_model = DummyRegressor(strategy='mean')
    # Output: dummy_model is an object that knows how to make basic mean predictions.
    ```
    *Why it's useful:* It tells us how much better our advanced models truly are compared to just guessing.

*   **Simple Model: `LinearRegression`**: This model looks for straight-line relationships between features and the target. It's like assuming audience numbers grow steadily with things like `show_day`.
    ```python
    from sklearn.linear_model import LinearRegression
    linear_model = LinearRegression()
    # Output: linear_model is ready to find linear relationships in data.
    ```
    *Why it's useful:* It's fast and easy to understand, checking if basic relationships exist.

*   **Rule-Based Model: `DecisionTreeRegressor`**: This model learns a set of "if-then-else" rules from the data, much like making a flowchart. For example: "IF `is_weekend` is 1 AND `audience_lag_7` is high, THEN predict a high `audience_count`."
    ```python
    from sklearn.tree import DecisionTreeRegressor
    tree_model = DecisionTreeRegressor(random_state=42)
    # Output: tree_model is ready to learn decision rules.
    ```
    *Why it's useful:* Can capture non-linear patterns, but single trees can sometimes be too specific (overfit).

*   **Powerful "Team" Models (Gradient Boosting & Ensemble Methods)**: These are like a team of decision trees working together, often correcting each other's mistakes or combining their strengths. They are usually very powerful for forecasting tasks.
    *   **`LightGBM` (Light Gradient Boosting Machine):** A fast and efficient gradient boosting model.
    *   **`HistGradientBoosting` (Histogram-based Gradient Boosting):** Another efficient boosting model, especially good for large datasets.
    *   **`XGBoost` (Extreme Gradient Boosting):** A highly optimized and popular gradient boosting model.
    *   **`ExtraTreesRegressor` (Extremely Randomized Trees):** An ensemble that builds many decision trees, but adds more randomness to make them diverse.
    *   **`RandomForestRegressor`:** Another ensemble that builds many trees, each on a random subset of data, and averages their predictions.

    ```python
    from lightgbm import LGBMRegressor
    from xgboost import XGBRegressor
    from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor

    # Example: Initializing a LightGBM model
    lgbm_model = LGBMRegressor(random_state=42, n_jobs=-1)
    # Output: lgbm_model is ready to learn as a powerful team.
    ```
    *Why they're useful:* They combine the strengths of many simple models to achieve high accuracy and robustness.

#### 2. How to Measure "Goodness" (Evaluation Metrics)

Once a model makes predictions, we need to objectively "taste" its performance. We use **metrics** for this.

*   **R² (R-squared) Score**: This tells us how well our model's predictions explain the actual variations in `audience_count`.
    *   It ranges from 0 to 1 (or can be negative for very poor models).
    *   **Closer to 1 is better:** An R² of 1 means the model perfectly predicts all variations. An R² of 0 means the model explains none of the variation (no better than the mean).
    *   *Analogy:* How much of the "flavor" of the audience count our model truly captures.

*   **MAE (Mean Absolute Error)**: This measures the average absolute difference between our model's predictions and the actual `audience_count`.
    *   It's expressed in the same units as our target (`audience_count`).
    *   **Lower is better:** An MAE of 10 means, on average, our predictions are off by 10 people.
    *   *Analogy:* How consistently accurate our "taste buds" (model) are.

#### 3. Making Models Even Better (Hyperparameter Tuning - HPT)

Models have settings called **hyperparameters** that we choose *before* training. These are not learned from the data itself but significantly impact how the model learns.

*   **Example Hyperparameters for Trees:**
    *   `n_estimators`: How many individual trees are in the "team."
    *   `max_depth`: How deep each decision tree can grow.
    *   `learning_rate`: How much each new tree's "correction" impacts the overall prediction.

*   **The Challenge**: Finding the best combination of these hyperparameters can be tricky. Trying every possible combination is too slow.
*   **The Solution: `RandomizedSearchCV`**: This is a smart technique that explores a *random sample* of hyperparameter combinations. It's much more efficient than trying everything.
    ```python
    from sklearn.model_selection import RandomizedSearchCV
    # RandomizedSearchCV helps find the best hyperparameters.
    # Output: This object will search for optimal settings.
    ```

*   **Special Considerations for Time-Series Data: `TimeSeriesSplit`**: In time-series forecasting, we must always train on past data and predict the future. We can't randomly shuffle the data, or our model might "see the future" during training!
    *   `TimeSeriesSplit` ensures that our data is split into training and validation sets chronologically. For example, it might train on data from January-March and validate on April, then train on January-April and validate on May, and so on.
    ```python
    from sklearn.model_selection import TimeSeriesSplit
    tscv = TimeSeriesSplit(n_splits=2) # Example: 2 splits for demonstration
    # Output: tscv is an object that knows how to split time-series data chronologically.
    ```
    *Why it's crucial:* It simulates how our model will actually perform in the real world when predicting future, unseen data.

#### 4. Avoiding Pitfalls: Overfitting

A common problem is **overfitting**: when a model learns the training data *too* well, it essentially memorizes it, but then performs poorly on new, unseen data.

*   **Analogy:** A chef who perfectly memorizes one recipe but can't adapt to new ingredients or different cooking conditions.
*   **How to spot it:** We look for a **large gap** between the model's performance (R² or MAE) on the **training set** (the data it learned from) and the **test set** (new data it hasn't seen). If the training R² is very high (e.g., 0.99) but the test R² is much lower (e.g., 0.50), the model is likely overfitting.
*   **How HPT helps:** By carefully tuning hyperparameters like `max_depth` (limiting how complex a tree can be), we can often find a balance where the model performs well on both training and test data, reducing overfitting.

---

### The Model Evaluation and Selection Process in Action

Our strategy ties all these concepts together. Here's a step-by-step walkthrough of how we evaluate and select our models:

1.  **Prepare Data**: Our feature-engineered data (`f_t_df` from [Chapter 3: Time-Series Feature Engineering](03_time_series_feature_engineering_.md)) is split into `X_train`, `y_train`, `X_test`, and `y_test`. We ensure that dynamic features (lags, rolling means) have their missing values (at the start of the time series) filled (e.g., with 0), as discussed in [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md).
2.  **Define Preprocessing Pipeline**: We create a `ColumnTransformer` (as seen in [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md)) to handle scaling numerical features and one-hot encoding categorical features.
3.  **Baseline & Simple Models**: We start with `DummyRegressor`, `LinearRegression`, and `DecisionTreeRegressor` to understand our baseline performance and general data characteristics.
4.  **Advanced Model Training (with HPT)**:
    *   For each advanced model (LightGBM, HistGradientBoosting, XGBoost, Extra Trees, RandomForest), we set up a `Pipeline` that first preprocesses the data and then applies the model.
    *   We define a range of hyperparameters to search for using `RandomizedSearchCV`.
    *   We use `TimeSeriesSplit` to ensure that our hyperparameter tuning respects the temporal order of our data.
    *   `RandomizedSearchCV` fits the pipeline multiple times, trying different hyperparameter combinations, and selects the best set.
    *   The model is then re-trained with these best hyperparameters on the *entire* training set.
5.  **Evaluate Performance**: For every model (baseline, simple, and advanced/tuned), we make predictions on both the `X_train` and `X_test` sets. We calculate and compare R² and MAE for both.
6.  **Compare and Select**: We analyze the R² and MAE scores across all models. We look for models that:
    *   Have a high R² and low MAE on the **test set**.
    *   Have a **small generalization gap** (i.e., similar performance on training and test sets) to avoid overfitting.
    *   Are computationally efficient (especially important for iterative forecasting).
    *   This comparison helps us identify the most suitable models for our forecasting task, which we'll then use in an ensemble strategy.

#### Code Implementation & Walkthrough

First, let's briefly review how our data is prepared before model training. We'll reuse the `f_t_df` from [Chapter 3: Time-Series Feature Engineering](03_time_series_feature_engineering_.md), which now includes all our engineered features.

```python
# Assuming f_t_df is our DataFrame with all features from previous chapters
# It needs to be sorted by date for time-series splitting
f_t_df = f_t_df.sort_values('show_date').reset_index(drop=True)

# Separate features (X) and target (y)
target_col = "audience_count"
y_train = f_t_df[target_col].values
X_train = f_t_df.drop(columns=[target_col, 'ID', 'show_date', 'day_of_week', 'theater_type', 'theater_area'], errors="ignore")

# Define column types for preprocessing (from Chapter 1)
categorical_columns = ['book_theater_id'] # book_theater_id is now one-hot encoded
# Note: original 'day_of_week', 'theater_type', 'theater_area' columns are dropped and replaced by encoded/cyclical features.
# 'encoded_theater_type' and 'encoded_theater_area' are now treated as static numerical for scaling.
static_numerical_columns = ['latitude', 'longitude', 'show_day', 'show_month', 'show_year',
                            'encoded_day_of_week', 'day_of_year', 'week_of_year', 'quarter',
                            'is_weekend', 'is_sunday', 'is_friday', 'dow_sin', 'dow_cos',
                            'month_sin', 'month_cos', 'doy_sin', 'doy_cos',
                            'encoded_theater_type', 'encoded_theater_area']
dynamic_columns = ['audience_lag_1', 'audience_lag_7', 'audience_lag_14',
                   'audience_lag_21', 'audience_lag_28', 'audience_lag_30',
                   'audience_roll_mean_7', 'audience_roll_std_7',
                   'audience_roll_mean_14', 'audience_roll_std_14',
                   'audience_roll_mean_30', 'audience_roll_std_30']

# Fill NaN in dynamic columns with 0 (as discussed in Chapter 1 for lag/roll features)
for col in dynamic_columns:
    X_train[col] = X_train[col].fillna(0)

# Output: X_train and y_train are now ready, with appropriate columns and filled NaNs.
# X_train.shape = (214046, 36)
# y_train.shape = (214046,)
```

Next, we set up our preprocessor using `MinMaxScaler` for numerical features and `OneHotEncoder` for categorical features, as established in [Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md). We also prepare `TimeSeriesSplit` for robust evaluation.

```python
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error

# Initialize our scalers and encoder
ohe_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
static_scaler = MinMaxScaler()
dynamic_scaler = MinMaxScaler()

# Create the ColumnTransformer to apply different preprocessing to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num_static', static_scaler, static_numerical_columns),
        ('num_dynamic', dynamic_scaler, dynamic_columns),
        ('cat', ohe_encoder, categorical_columns)
    ],
    remainder='passthrough' # Keep other columns (like 'ID', 'show_date') if any as is
)

# Time Series Cross-Validation setup (for hyperparameter tuning on training data)
tscv = TimeSeriesSplit(n_splits=2) # Using 2 splits for faster demonstration in tutorial

# Output: 'preprocessor' and 'tscv' objects are configured for model pipelines.
```

Now, let's see how we would evaluate a model, starting with a baseline, then more complex ones.

#### The Model Evaluation Pipeline (Simplified)

We'll use a simplified pipeline for this tutorial to demonstrate the core steps.

```python
# Helper function to evaluate a model (for demonstration)
def evaluate_model(model_pipeline, model_name, X_train_data, y_train_data):
    print(f"\n--- Evaluating {model_name} ---")

    # For this chapter, we focus on *how* to evaluate.
    # In practice, X_test would be used for final validation, but HPT uses CV.

    # Fit the pipeline on the training data
    model_pipeline.fit(X_train_data, y_train_data)

    # Make predictions on the training data
    train_pred = model_pipeline.predict(X_train_data)

    # Calculate metrics
    train_r2 = r2_score(y_train_data, train_pred)
    train_mae = mean_absolute_error(y_train_data, train_pred)

    print(f"Train R²: {train_r2:.4f}")
    print(f"Train MAE: {train_mae:.2f}")

# Output: This function helps us get a quick performance check.
```

Let's test some simple models using this concept:

```python
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

# 1. Dummy Regressor (Our Baseline)
# This model will just predict the mean of the target.
dummy_regressor = DummyRegressor(strategy='mean')
dummy_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', dummy_regressor)
])
evaluate_model(dummy_pipeline, "Dummy Regressor", X_train, y_train)

# Output:
# --- Evaluating Dummy Regressor ---
# Train R²: 0.0000
# Train MAE: 24.76
# (This means it's not explaining any variance and is off by ~24 people on average)
```

```python
# 2. Linear Regression (A simple linear model)
linear_regressor = LinearRegression()
linear_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', linear_regressor)
])
evaluate_model(linear_pipeline, "Linear Regression", X_train, y_train)

# Output (example, actual values can vary):
# --- Evaluating Linear Regression ---
# Train R²: 0.5189
# Train MAE: 15.12
# (Better than dummy, but for complex time series, it might not be enough)
```

```python
# 3. Decision Tree Regressor (A non-linear model)
tree_regressor = DecisionTreeRegressor(random_state=42)
tree_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', tree_regressor)
])
evaluate_model(tree_pipeline, "Decision Tree Regressor", X_train, y_train)

# Output (example, actual values can vary):
# --- Evaluating Decision Tree Regressor ---
# Train R²: 1.0000
# Train MAE: 0.00
# (This almost perfect training score indicates severe overfitting! It's memorizing the data.)
```

The output for the `DecisionTreeRegressor` (R²=1.00, MAE=0.00 on training) is a classic sign of **overfitting**. The model has learned the training data perfectly, but it won't generalize well to new data. This is why Hyperparameter Tuning (HPT) is so important!

#### Hyperparameter Tuning with `RandomizedSearchCV` and `TimeSeriesSplit`

Now, let's see how we would apply HPT to a more advanced model like `LightGBM` to find the best settings and prevent overfitting.

```python
from lightgbm import LGBMRegressor
from sklearn.model_selection import RandomizedSearchCV

# Define the LightGBM model
lgbm_model = LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)

# Create a pipeline with preprocessing and LightGBM
lgbm_pipeline = Pipeline([
    ('preprocess', preprocessor), # Our preprocessor from Chapter 1
    ('lgbm', lgbm_model)
])

# Define a distribution of hyperparameters to search for
param_dist_lgbm = {
    'lgbm__n_estimators': [100, 200, 500],
    'lgbm__max_depth': [5, 8, 10],
    'lgbm__learning_rate': [0.01, 0.05, 0.1],
    'lgbm__reg_alpha': [0.0, 0.1, 0.5], # L1 regularization
    'lgbm__reg_lambda': [0.0, 0.1, 0.5] # L2 regularization
}

# Set up RandomizedSearchCV with TimeSeriesSplit
# It will try 10 different random combinations of parameters.
lgbm_tuned_search = RandomizedSearchCV(
    lgbm_pipeline,
    param_distributions=param_dist_lgbm,
    n_iter=10,        # Number of parameter settings to sample
    cv=tscv,          # Our TimeSeriesSplit for chronological validation
    scoring='r2',     # Use R² to determine the best model
    n_jobs=-1,        # Use all available CPU cores
    random_state=42,
    verbose=0
)

print("Starting LightGBM Hyperparameter Tuning (this may take a while)...")
lgbm_tuned_search.fit(X_train, y_train)
print("Hyperparameter Tuning Complete.")

# Output:
# Starting LightGBM Hyperparameter Tuning (this may take a while)...
# Hyperparameter Tuning Complete.
```

After the search completes, we can retrieve the best parameters found and evaluate the performance of the model using these optimized settings.

```python
# Get the best parameters and best R² score from the search
print(f"Best parameters found: {lgbm_tuned_search.best_params_}")
print(f"Best R² score from CV: {lgbm_tuned_search.best_score_:.4f}")

# Extract the best model (which is the fitted pipeline with best params)
best_lgbm_pipeline = lgbm_tuned_search.best_estimator_

# Evaluate the best LightGBM model
evaluate_model(best_lgbm_pipeline, "Tuned LightGBM Regressor", X_train, y_train)

# Output (example, actual values can vary after tuning):
# Best parameters found: {'lgbm__reg_lambda': 0.5, 'lgbm__reg_alpha': 0.1, 'lgbm__n_estimators': 500, 'lgbm__max_depth': 8, 'lgbm__learning_rate': 0.01}
# Best R² score from CV: 0.5344
#
# --- Evaluating Tuned LightGBM Regressor ---
# Train R²: 0.5585
# Train MAE: 14.58
# (Notice how the training R² is no longer 1.0, indicating less overfitting compared to the single Decision Tree.)
```

By systematically evaluating different models and applying hyperparameter tuning with `TimeSeriesSplit`, we can identify models that perform well and generalize effectively to unseen data, ready for our iterative prediction task.

### Under the Hood: How it Works

The model evaluation and selection process is a systematic workflow designed to rigorously test and optimize our machine learning "recipes."

#### How it works (Step-by-Step Walkthrough):

1.  **Data Ingestion & Split**: The full processed dataset (`f_t_df`) is divided into a training set (for learning) and a separate test set (for unbiased evaluation). This is done chronologically for time series, with the training set always preceding the test set.
2.  **Preprocessing**: Features from both training and test sets are transformed using the `preprocessor` (e.g., scaling numbers, encoding categories).
3.  **Model Iteration**:
    *   For each candidate model (e.g., LightGBM, Extra Trees):
        *   A model `Pipeline` is created, chaining the `preprocessor` and the model.
        *   **Hyperparameter Tuning (HPT)**: If applicable, `RandomizedSearchCV` is used with `TimeSeriesSplit` to try various model settings. `TimeSeriesSplit` divides the *training set* itself into smaller, chronological train/validation folds. The model is trained on the "train" part of each fold and evaluated on the "validation" part. The best-performing settings are selected.
        *   **Training**: The model (with its optimal hyperparameters from HPT) is then trained on the *entire* preprocessed training dataset.
        *   **Prediction**: The trained model makes predictions on both the preprocessed training set and the preprocessed test set.
        *   **Evaluation**: R² and MAE scores are calculated for both training and test predictions.
4.  **Performance Comparison**: All models' test R² and MAE scores are gathered and compared.
5.  **Selection**: Based on these metrics and the generalization gap (difference between train and test performance), the most robust and accurate models are chosen for the final forecasting step.

Here's a simple diagram to visualize this process for one model (like LightGBM with HPT):

```mermaid
sequenceDiagram
    Participant RawData as "Raw Data (f_t_df)"
    Participant DataSplitter as "Data Splitter"
    Participant Preprocessor as "Preprocessor"
    Participant HPT_Optimizer as "HPT Optimizer (RandomizedSearchCV)"
    Participant ModelTrainer as "Model Trainer"
    Participant Evaluator as "Evaluator"

    RawData->DataSplitter: "Full Data"
    DataSplitter->Preprocessor: "X_train, y_train, X_test"
    Preprocessor->HPT_Optimizer: "Preprocessed X_train, y_train"
    Note over HPT_Optimizer: "Uses TimeSeriesSplit for CV"
    HPT_Optimizer->ModelTrainer: "Best Hyperparameters"
    ModelTrainer->ModelTrainer: "Train Model on X_train, y_train"
    ModelTrainer->Evaluator: "Trained Model"
    Evaluator->Preprocessor: "Request Preprocessed X_test"
    Preprocessor->Evaluator: "Preprocessed X_test"
    Evaluator->Evaluator: "Predict on X_train, X_test"
    Evaluator-->"Done": "Report R², MAE (Train & Test)"
```

#### Code for Internal Implementation (Simplified)

While `RandomizedSearchCV` handles the internal fitting and predicting for HPT, here's a brief look at how we'd typically initialize a final model with the best parameters and prepare it for real predictions (similar to what `best_lgbm_pipeline` already does).

```python
# Assuming 'best_params_lgbm' holds the best hyperparameters found (e.g., from lgbm_tuned_search.best_params_)
# from lightgbm import LGBMRegressor

# Initialize the final LightGBM model with the best found hyperparameters
final_lgbm_model = LGBMRegressor(
    n_estimators=best_lgbm_pipeline.named_steps['lgbm'].n_estimators,
    max_depth=best_lgbm_pipeline.named_steps['lgbm'].max_depth,
    learning_rate=best_lgbm_pipeline.named_steps['lgbm'].learning_rate,
    reg_alpha=best_lgbm_pipeline.named_steps['lgbm'].reg_alpha,
    reg_lambda=best_lgbm_pipeline.named_steps['lgbm'].reg_lambda,
    random_state=42,
    n_jobs=-1,
    verbose=-1 # Suppress verbose output
)

# Create the final pipeline
final_lgbm_pipeline = Pipeline([
    ('preprocess', preprocessor), # Our preprocessor from Chapter 1
    ('lgbm', final_lgbm_model)
])

# Fit the final pipeline on the entire training data
# This is the model that we will use for actual future predictions.
final_lgbm_pipeline.fit(X_train, y_train)

# Output: final_lgbm_pipeline is now fully trained and ready to make predictions.
```
By going through this systematic process, we ensure that the models we select are not only accurate on historical data but also robust enough to make reliable forecasts for future cinema audience counts.

### Conclusion

In this chapter, we explored the critical process of "Model Evaluation and Selection Strategy." We learned about different types of machine learning models, essential evaluation metrics like R² and MAE, and powerful optimization techniques like Hyperparameter Tuning with `RandomizedSearchCV` and `TimeSeriesSplit`. Understanding how to systematically test, evaluate, and fine-tune our models is key to identifying the most effective "recipes" for our cinema audience forecasting challenge and avoiding pitfalls like overfitting.

Now that we have selected our best-performing models, the next step is to understand how to use them to make predictions day by day for future dates, continuously updating their knowledge base.

[Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md)

---

<div style="page-break-before: always;"></div>






















<a name="chapter5"></a>
# Chapter 5: Iterative Time-Series Prediction

Welcome to Chapter 5! In our last chapter, [Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md), we explored how to rigorously test different machine learning models and select the best ones for our cinema audience prediction task. Now that we have our champion model (or models!), it's time to put them to work and make actual forecasts for future dates.

### Why is Iterative Time-Series Prediction So Important?

Imagine you're asked to predict the `audience_count` for a specific cinema theater for the entire next month. You've trained a great model that uses features like "audience count yesterday" (a lag feature) or "average audience over the last 7 days" (a rolling feature), as we learned in [Time-Series Feature Engineering](03_time_series_feature_engineering_.md).

Now, think about what happens when you try to predict for a day early next month, say March 5th. You need the "audience count yesterday" (March 4th) to make that prediction. But March 4th is also in the future! You don't *actually* know the audience count for March 4th yet.

This is where **Iterative Time-Series Prediction** comes in. It solves this real-world challenge by simulating how forecasting works step-by-step. Instead of trying to predict the entire future at once, we predict **one day at a time**.

Here's the magic: For each day we predict, the model uses all the *actual historical data* available up to that point, and then it also incorporates the *predictions it just made for previous future days*. It's like a rolling forecast, where each new prediction immediately becomes part of the "history" for the next day's prediction. This approach ensures two crucial things:

1.  **No Peeking into the Future**: We never use information from a future date to predict a present or past date.
2.  **Leveraging Latest Information**: Our predictions for later dates are always as informed as possible, benefiting from earlier predictions.

Let's dive into how this clever "rolling forecast" works.

### Key Concepts: The Daily Prediction Cycle

The core idea behind iterative time-series prediction is a continuous cycle of: **Look at History → Predict → Update History**.

#### 1. Our "History Book" (`history_dict`)

As we learned in [Historical Data Management for Prediction](02_historical_data_management_for_prediction_.md), we maintain a `history_dict`. This is a Python dictionary that stores a list of past `audience_count` values for *every single cinema theater*. It's like a running logbook for each theater.

*   **Initial State**: Before we start predicting, `history_dict` is filled with all the *actual* historical `audience_count` values from our training data.
*   **Dynamic Updates**: As soon as we make a prediction for a theater for a specific day, that *predicted* `audience_count` is immediately added to the end of that theater's list in the `history_dict`.

#### 2. Features Based on "Current" History

When we make a prediction for a future date, say March 5th:
*   Features like `show_day`, `is_weekend`, `latitude`, `theater_type` (which are known for March 5th) are used directly.
*   Features like `audience_lag_1` (yesterday's audience) or `audience_roll_mean_7` (average over the last 7 days) are calculated using the `audience_count` values currently stored in the `history_dict` for that specific theater. This `history_dict` now includes any predictions we've already made for March 4th, March 3rd, and so on.

#### 3. The Iterative Loop

The process repeats day after day. When we move from predicting March 5th to March 6th, the `history_dict` already contains the predicted audience count for March 5th. So, when the model calculates `audience_lag_1` for March 6th, it will use March 5th's predicted value. This allows the model to propagate its forecasts forward in time.

### Solving the Use Case: Predicting for the Next 7 Weeks

Let's walk through how we'd predict the `audience_count` for all theaters for all 53 days in our test period (March 1st to April 22nd, 2024), day by day.

**Step-by-Step Walkthrough:**

1.  **Prepare Prediction Dates**: We create a list of all future dates we need to predict, ordered chronologically.
2.  **Initialize `history_dict`**: Our `history_dict` is populated with all the *actual* historical `audience_count` data available up to the last day of our training set.
3.  **Loop Through Each Prediction Date**:
    *   For the `current_date` (e.g., March 1st):
        *   **Identify Theaters**: Find all unique `book_theater_id`s that need predictions for `current_date`.
        *   **For Each Theater**:
            *   **Extract Static Features**: Gather the `current_date`'s static features (like `show_day`, `is_weekend`, `latitude`, `theater_type`). These are known regardless of past predictions.
            *   **Calculate Dynamic Features**: Look up the `audience_count` history for this specific `theater_id` in `history_dict`. Use this history to calculate `audience_lag_X` and `audience_roll_mean/std_X` features for `current_date`.
            *   **Preprocess Features**: Scale numerical features and one-hot encode categorical features for the `current_date`'s data, using the preprocessors fitted on the training data (from [Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md)).
            *   **Make Prediction**: Feed the prepared features for this theater and `current_date` into our best-performing machine learning model (selected in [Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md)). The model outputs a `predicted_audience_count`.
            *   **Update History**: Immediately add the `predicted_audience_count` to the `history_dict` for that `theater_id`. This is the crucial iterative step!
        *   Store all predictions for the `current_date`.
    *   Move to the next `current_date` (e.g., March 2nd), and repeat the entire process. The `history_dict` is now updated with March 1st's predictions, allowing March 2nd's lag/rolling features to be correctly calculated.

This cycle continues until predictions have been made for all future dates.

### Under the Hood: How it Works

Let's visualize the daily prediction cycle for a single theater:

```mermaid
sequenceDiagram
    Participant ForecastSystem as "Forecast System"
    Participant DailyLoop as "Daily Prediction Loop"
    Participant TheaterHistory as "history_dict"
    Participant FeatureEngineer as "Feature Engineer"
    Participant MLModel as "ML Model"

    ForecastSystem->DailyLoop: "Predict for Current Date"
    DailyLoop->TheaterHistory: "Get history for Theater X"
    Note over TheaterHistory: [..., actual_day_N-1, pred_day_N]
    TheaterHistory-->DailyLoop: [historical_counts_X]
    DailyLoop->FeatureEngineer: "Calculate lag/roll features from [historical_counts_X]"
    FeatureEngineer-->DailyLoop: [calculated_features_X]
    DailyLoop->MLModel: "Predict audience with [calculated_features_X] + static features"
    MLModel-->DailyLoop: predicted_audience_X
    DailyLoop->TheaterHistory: "Add predicted_audience_X to history for Theater X"
    Note over TheaterHistory: history_dict['TheaterX'] is now [..., pred_day_N, predicted_audience_X]
    DailyLoop-->ForecastSystem: "Prediction for Theater X done"
```

The `Daily Prediction Loop` then repeats this for every theater for the `Current Date`, and then moves to the next `Current Date`, continuously building up its "history" of predictions.

#### Code for the Prediction Loop (Simplified)

Here's a simplified version of the `make_predictions_simple` function used in our project, illustrating the core iterative logic. Note how it processes day-by-day and updates the `current_history` with new predictions.

```python
import numpy as np # Used for numerical operations
import pandas as pd # Used for data manipulation
import datetime as dt # Used for date handling

# (Assume ohe_encoder, static_scaler, dynamic_scaler are already fitted from Chapter 1)
# (Assume historical_dict is initialized with actual data from Chapter 2)
# (Assume X_test_categorical, X_test_static are pre-transformed static/categorical features for future dates)
# (Assume dynamic_columns is a list of lag/rolling feature names)

def make_predictions_simple(model, test_dataframe, history_dictionary, X_test_cat, X_test_stat, dyn_scaler, dynamic_columns):
    # Make a copy of history_dictionary so the original is untouched
    current_history = {k: v.copy() for k, v in history_dictionary.items()}

    # Sort test data by date to process day-by-day
    test_dataframe = test_dataframe.sort_values(['show_date', 'book_theater_id']).reset_index(drop=True)
    all_unique_dates = sorted(test_dataframe['show_date'].unique())

    # To store all final predictions
    all_predictions = np.zeros(len(test_dataframe)) 

    for current_date in all_unique_dates:
        # Select rows for the current date
        date_mask = test_dataframe['show_date'] == current_date
        row_indices = np.where(date_mask)[0]
        theaters_today = test_dataframe.loc[row_indices, 'book_theater_id'].values
        num_theaters = len(theaters_today)

        # Prepare matrix for dynamic (lag/roll) features for today's predictions
        # These will be filled by looking up current_history
        lag_feature_matrix = np.zeros((num_theaters, len(dynamic_columns)))

        for i, theater_id in enumerate(theaters_today):
            theater_history = current_history.get(theater_id, [])
            history_length = len(theater_history)

            # --- Calculate dynamic features using `theater_history` ---
            # (Simplified for tutorial, in reality, all lags and rolling stats from Chapter 3 are computed)
            if history_length >= 1:
                lag_feature_matrix[i, 0] = theater_history[-1] # audience_lag_1
            # ... and so on for audience_lag_7, audience_roll_mean_7, etc.
            # If history is too short for a lag, it remains 0 (as initialized)

        # --- Scale dynamic features using the pre-fitted scaler ---
        X_dynamic_scaled = dyn_scaler.transform(lag_feature_matrix)

        # --- Combine all features for prediction ---
        X_cat_today = X_test_cat[row_indices]
        X_stat_today = X_test_stat[row_indices]
        X_today_combined = np.hstack([X_cat_today, X_stat_today, X_dynamic_scaled])

        # --- Make predictions ---
        predictions_today = model.predict(X_today_combined)
        predictions_today = np.maximum(predictions_today, 0) # Ensure no negative predictions
        all_predictions[row_indices] = predictions_today # Store predictions

        # --- Dynamically update history for the next day's predictions ---
        for i, theater_id in enumerate(theaters_today):
            if theater_id not in current_history:
                current_history[theater_id] = []
            current_history[theater_id].append(predictions_today[i]) # Add the new prediction

    return all_predictions

# Example usage (assuming model_2_lightgbm, t_df, etc., are defined):
# predictions_model_2 = make_predictions_simple(
#     model=model_2_lightgbm,
#     model_name="LightGBM",
#     test_dataframe=t_df.copy(),
#     history_dictionary=history_dict,
#     X_test_cat=X_test_categorical,
#     X_test_stat=X_test_static,
#     dyn_scaler=dynamic_scaler,
#     dyn_cols=dynamic_columns
# )
# print("Predictions made using LightGBM and dynamic history.")
```
**Output Description:**
After calling `make_predictions_simple`, `predictions_model_2` will be a NumPy array containing the predicted `audience_count` for each `book_theater_id` for every day in the future period (March 1st to April 22nd). Each day's prediction for a theater will have incorporated the previous day's predicted `audience_count` for that same theater into its feature calculations.

### Conclusion

In this chapter, we explored "Iterative Time-Series Prediction," a powerful technique for forecasting future values one step at a time. We learned how to dynamically update our `history_dict` with new predictions, allowing our models to always use the most current information (whether actual or predicted) for calculating time-dependent features like lags and rolling statistics. This iterative approach is crucial for building realistic and robust time-series forecasting systems.

Next, we'll combine the strengths of multiple models using an "Ensemble Modeling Strategy" to achieve even more accurate and stable predictions.

[Ensemble Modeling Strategy](06_ensemble_modeling_strategy_.md)

---


<div style="page-break-before: always;"></div>






















<a name="chapter6"></a>
# Chapter 6: Ensemble Modeling Strategy

Welcome to the final chapter of our machine learning capstone project! In [Chapter 5: Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md), we learned how to use our best-performing model to make predictions day by day, dynamically updating its history. Now, we're going to take our predictions to the next level by combining the strengths of multiple models through an **"Ensemble Modeling Strategy."**

### Why is Ensemble Modeling So Important?

Imagine you're trying to predict the `audience_count` for future cinema shows, and you've found a few excellent models (like LightGBM, Extra Trees, etc.). Each of these models is like a highly skilled expert. One expert might be really good at spotting daily patterns, another at identifying weekly trends, and a third at handling unusual spikes or drops in audience.

If you rely on just one expert (a single model), you might miss out on valuable insights that the others could provide. What if that one expert makes a mistake?

This is where "Ensemble Modeling" comes in. It's like building a **"team of experts"** to make a collective decision. Instead of picking just one best model, we combine the predictions from several diverse models. The goal is to leverage the unique strengths of each model, smooth out individual errors, and ultimately achieve a **more robust and accurate overall prediction**. It's the wisdom of the crowd applied to machine learning!

### Key Concepts in Ensemble Modeling

To build our "team of experts," we need to understand a few core ideas:

#### 1. Diverse Models: Our "Team of Experts"

The success of an ensemble often relies on using models that are diverse in how they learn. We've already identified several powerful models in [Chapter 4: Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md) that performed well individually:

*   **Extra Trees (ET)**: This model builds many decision trees but introduces more randomness to make them very different from each other. It's good at exploring various data relationships.
*   **LightGBM**: An efficient gradient boosting model that builds trees sequentially, with each new tree trying to correct the errors of the previous ones. It's known for speed and accuracy.
*   **HistGradientBoosting (HGB)**: Similar to LightGBM, it's another gradient boosting model optimized for performance, especially on large datasets.
*   **XGBoost (XGB)**: A very popular and highly optimized gradient boosting model that's often a top performer in many machine learning competitions.

Each of these models has slightly different strengths and weaknesses, making them good candidates for an ensemble.

#### 2. Combining Predictions: "Collective Decision-Making"

Once each model makes its prediction, we need a way to combine them into a single, final forecast. For beginners, a very simple and effective method is **averaging**:

*   **Simple Averaging**: We just add up the predictions from all our chosen models and divide by the number of models. For example, if Model A predicts 50, Model B predicts 55, and Model C predicts 48, the ensemble prediction would be (50 + 55 + 48) / 3 = 51.

**Why averaging works**: It helps to balance out the individual biases or errors of each model. If one model is slightly off on a particular day, the average with other models might pull the final prediction closer to the true value.

#### 3. Robustness and Accuracy: The "Informed Collective Decision"

By combining these diverse models, we aim for:
*   **Improved Accuracy**: The ensemble often outperforms any single model because it captures a wider range of patterns in the data.
*   **Increased Robustness**: The ensemble is less sensitive to outliers or quirks that might confuse a single model. It's like having multiple checkpoints to ensure a more reliable result.

### Solving the Use Case: Ensemble Forecasting for the Next 7 Weeks

Let's walk through how we apply this "Ensemble Modeling Strategy" to predict the `audience_count` for all theaters for all future days in our test period (March 1st to April 22nd, 2024), day by day.

**Step-by-Step Walkthrough:**

1.  **Prepare Prediction Dates & Initialize History**: Just like in [Chapter 5: Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md), we prepare our future dates and initialize the `history_dict` with all actual historical data.
2.  **Loop Through Each Prediction Date (Daily Cycle)**:
    *   For the `current_date` (e.g., March 1st):
        *   **For Each Theater**:
            *   **Calculate Features**: Extract static features and dynamically calculate lag and rolling features using the `history_dict` (which includes previous days' predictions from this ensemble process). This is done just as explained in [Chapter 3: Time-Series Feature Engineering](03_time_series_feature_engineering_.md) and [Chapter 5: Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md).
            *   **Preprocess Features**: Transform these features using the preprocessors fitted on the training data ([Chapter 1: Data Preprocessing Pipeline](01_data_preprocessing_pipeline_.md)).
            *   **Individual Model Predictions**: Feed the prepared features for this theater and `current_date` into *each of our selected ensemble models* (Extra Trees, LightGBM, HistGradientBoosting, XGBoost). Each model will output its own `predicted_audience_count`.
            *   **Combine Predictions**: Take the predictions from all individual models and calculate their average. This average is our **ensemble's `final_predicted_audience_count`** for this theater and `current_date`.
            *   **Update History**: Immediately add this `final_predicted_audience_count` to the `history_dict` for that `theater_id`. This is crucial because the *next* day's predictions will use this ensemble's collective forecast as part of its history.
        *   Store all ensemble predictions for the `current_date`.
    *   Move to the next `current_date` (e.g., March 2nd), and repeat the entire process. The `history_dict` is now updated with March 1st's *ensemble* predictions.

This iterative ensemble process continues until predictions have been made for all future dates. The output will be a single set of `audience_count` predictions, representing the combined wisdom of our model team.

### Under the Hood: How it Works

Let's visualize how the daily prediction cycle works within an ensemble for a single theater. Instead of one "ML Model," we now have a "Team of ML Models."

```mermaid
sequenceDiagram
    participant ForecastSystem as "Forecast System"
    participant DailyLoop as "Daily Prediction Loop"
    participant TheaterHistory as "history_dict"
    participant FeatureEngineer as "Feature Engineer"
    participant ModelET as "Extra Trees Model"
    participant ModelLGB as "LightGBM Model"
    participant ModelHGB as "HistGB Model"
    participant ModelXGB as "XGBoost Model"
    participant Ensemble as "Ensemble Combiner"

    ForecastSystem->DailyLoop: "Predict for Current Date"
    DailyLoop->TheaterHistory: "Get history for Theater X"
    Note over TheaterHistory: [..., actual_day_N-1, ens_pred_day_N]
    TheaterHistory-->DailyLoop: [historical_counts_X]
    DailyLoop->FeatureEngineer: "Calculate lag/roll features from [historical_counts_X]"
    FeatureEngineer-->DailyLoop: [prepared_features_X]

    DailyLoop->ModelET: "Predict using [prepared_features_X]"
    ModelET-->DailyLoop: pred_ET_X

    DailyLoop->ModelLGB: "Predict using [prepared_features_X]"
    ModelLGB-->DailyLoop: pred_LGB_X

    DailyLoop->ModelHGB: "Predict using [prepared_features_X]"
    ModelHGB-->DailyLoop: pred_HGB_X

    DailyLoop->ModelXGB: "Predict using [prepared_features_X]"
    ModelXGB-->DailyLoop: pred_XGB_X

    DailyLoop->Ensemble: "Average (pred_ET_X, pred_LGB_X, pred_HGB_X, pred_XGB_X)"
    Ensemble-->DailyLoop: final_ensemble_pred_X

    DailyLoop->TheaterHistory: "Add final_ensemble_pred_X to history for Theater X"
    Note over TheaterHistory: history_dict['TheaterX'] is now [..., ens_pred_day_N, final_ensemble_pred_X]
    DailyLoop-->ForecastSystem: "Ensemble Prediction for Theater X done"
```

#### Code for the Ensemble Prediction Loop (Simplified)

We'll use the `make_predictions_simple` function from [Chapter 5: Iterative Time-Series Prediction](05_iterative_time_series_prediction_.md) as a base, but modify it to make predictions with all our models and then average them.

First, let's assume our individual models (Extra Trees, LightGBM, HistGradientBoosting, XGBoost) have already been trained and configured as described in [Chapter 4: Model Evaluation and Selection Strategy](04_model_evaluation_and_selection_strategy_.md) and the project notebook.

```python
# Assume model_1_extratrees, model_2_lightgbm, model_3_histgb, model_4_xgboost are already trained.
# These models were trained on X_train_combined (preprocessed features).

# Also assume t_df, history_dict, X_train_combined, y_train, X_test_categorical,
# X_test_static, dynamic_scaler, and dynamic_columns are defined from previous cells/chapters.

# The make_predictions_simple function would be adapted as follows:
def make_ensemble_predictions(models, model_names, test_dataframe, history_dictionary, 
                              X_test_cat, X_test_stat, dyn_scaler, dyn_cols):
    
    current_history = {k: v.copy() for k, v in history_dictionary.items()}
    test_dataframe = test_dataframe.sort_values(['show_date', 'book_theater_id']).reset_index(drop=True)
    all_unique_dates = sorted(test_dataframe['show_date'].unique())
    all_ensemble_predictions = np.zeros(len(test_dataframe))

    for current_date in all_unique_dates:
        date_mask = test_dataframe['show_date'] == current_date
        row_indices = np.where(date_mask)[0]
        theaters_today = test_dataframe.loc[row_indices, 'book_theater_id'].values
        num_theaters = len(theaters_today)

        # Prepare dynamic features for today
        lag_feature_matrix = np.zeros((num_theaters, len(dyn_cols)))
        for i, theater_id in enumerate(theaters_today):
            theater_history = current_history.get(theater_id, [])
            history_length = len(theater_history)
            
            # Simplified: calculate audience_lag_1 and other dynamic features
            if history_length >= 1:
                lag_feature_matrix[i, 0] = theater_history[-1] # audience_lag_1
            # ... (add logic for other lag/rolling features as in make_predictions_simple) ...
            
        X_dynamic_scaled = dyn_scaler.transform(lag_feature_matrix)

        # Combine all features for prediction
        X_cat_today = X_test_cat[row_indices]
        X_stat_today = X_test_stat[row_indices]
        X_today_combined = np.hstack([X_cat_today, X_stat_today, X_dynamic_scaled])

        # --- Make predictions from ALL individual models ---
        individual_predictions = []
        for model in models:
            preds = model.predict(X_today_combined)
            individual_predictions.append(preds)
        
        # --- Combine predictions (simple averaging) ---
        ensemble_predictions_today = np.mean(individual_predictions, axis=0)
        ensemble_predictions_today = np.maximum(ensemble_predictions_today, 0) # Ensure no negative predictions
        
        all_ensemble_predictions[row_indices] = ensemble_predictions_today

        # --- Dynamically update history with ENSEMBLE's prediction ---
        for i, theater_id in enumerate(theaters_today):
            if theater_id not in current_history:
                current_history[theater_id] = []
            current_history[theater_id].append(ensemble_predictions_today[i])

    return all_ensemble_predictions

# List of our trained models
ensemble_models = [
    model_1_extratrees,
    model_2_lightgbm,
    model_3_histgb,
    model_4_xgboost
]
ensemble_model_names = ["ExtraTrees", "LightGBM", "HistGradientBoosting", "XGBoost"]

# Example usage:
# print("Starting Ensemble Predictions...")
# final_ensemble_predictions = make_ensemble_predictions(
#     models=ensemble_models,
#     model_names=ensemble_model_names,
#     test_dataframe=t_df.copy(),
#     history_dictionary=history_dict,
#     X_test_cat=X_test_categorical,
#     X_test_stat=X_test_static,
#     dyn_scaler=dynamic_scaler,
#     dyn_cols=dynamic_columns
# )
# print("Ensemble Predictions Complete.")

# Output:
# Starting Ensemble Predictions...
# Ensemble Predictions Complete.
```

**Output Description:**
After calling `make_ensemble_predictions`, `final_ensemble_predictions` will be a NumPy array containing the predicted `audience_count` for each `book_theater_id` for every day in the future period. Each day's prediction for a theater will have incorporated the previous day's *ensemble* predicted `audience_count` for that same theater into its feature calculations, effectively creating a powerful, robust, and iteratively updated forecast.

Finally, these predictions would be formatted and saved into a submission file, as demonstrated in the project's code.

### Conclusion

In this chapter, we explored the powerful "Ensemble Modeling Strategy" to enhance our cinema audience predictions. We learned how to combine the unique strengths of multiple diverse models (Extra Trees, LightGBM, HistGradientBoosting, and XGBoost) through simple averaging, creating a more accurate and robust collective forecast. By integrating this ensemble approach into our iterative prediction pipeline, we ensure that our final predictions benefit from a "team of experts" that continually learns and adapts to new information.

This concludes our journey through the `machine-learning-capstone-project-2025` tutorial. We've covered everything from preparing raw data and engineering clever time-series features to evaluating models, making iterative predictions, and finally, combining models for superior performance. You now have a solid understanding of the key concepts and techniques used to build a robust time-series forecasting system!

---




























<div style="page-break-before: always;"></div>

PART - 2 
# Tutorial: machine-learning-capstone-project-2025

This project aims to **predict daily cinema audience counts** by *integrating various raw data sources* into a clean, unified dataset. It then *generates powerful time-series features* like audience trends from previous days and weeks. This data undergoes a *standardized cleaning and transformation process* to be model-ready. The core of the system is an *iterative forecasting engine* that predicts audience counts day by day, using its own past predictions to inform future ones. Finally, a *model experimentation workbench* is used to test, tune, and optimize various machine learning algorithms to achieve the best possible prediction accuracy.


## Visual Overview

```mermaid
flowchart TD
    A0["Data Integration Hub
"]
    A1["Time-Series Feature Factory
"]
    A2["Preprocessing & Transformation Blueprint
"]
    A3["Iterative Forecasting Engine
"]
    A4["Model Experimentation Workbench
"]
    A0 -- "Feeds Unified Data" --> A1
    A0 -- "Initial Cleaning" --> A2
    A1 -- "Provides Engineered Features" --> A2
    A2 -- "Defines Data Prep" --> A4
    A4 -- "Deploys Trained Models" --> A3
    A3 -- "Re-calculates Features" --> A1
    A3 -- "Applies Fitted Transforms" --> A2
```

## Chapters

1. [Data Integration Hub
](01_data_integration_hub_.md)
2. [Preprocessing & Transformation Blueprint
](02_preprocessing___transformation_blueprint_.md)
3. [Time-Series Feature Factory
](03_time_series_feature_factory_.md)
4. [Model Experimentation Workbench
](04_model_experimentation_workbench_.md)
5. [Iterative Forecasting Engine
](05_iterative_forecasting_engine_.md)

---
















# Chapter 1: Data Integration Hub

Welcome to the first chapter of your Machine Learning Capstone Project! Imagine you're a detective trying to solve a mystery, but all your clues are scattered in different folders, written in different styles, and some pieces are missing. That's often what data looks like before you start a machine learning project!

In this project, our goal is to forecast how many people will visit different cinemas on various dates. This is a real-world challenge where predicting audience numbers can help cinemas plan better, from staffing to snack inventory.

The problem we face right away is that the information we need comes from many different places. We have one file with booking details, another with theater locations, and yet another with calendar information. To make accurate predictions, we need to bring all these pieces of the puzzle together into one comprehensive view. This is where the **Data Integration Hub** comes in.

## What is a Data Integration Hub?

Think of a "Data Integration Hub" as your project's central command center for all data. It's a special place or process where you gather *all* your raw data sources and combine them into one unified, clean, and complete dataset.

Why is this so important? Machine learning models are like smart students: they learn best when they have all the relevant information presented clearly in one organized textbook, rather than sifting through many different scattered notes. Our Data Integration Hub's job is to create that "master textbook" for our models.

## Building Our Data Integration Hub: Step-by-Step

Let's walk through how we build this hub for our cinema audience forecasting project.

### Step 1: Gathering All the Raw Data

Our journey begins by collecting all the individual pieces of information. These often come in separate files, like CSVs (Comma Separated Values). In our case, we have several:

*   `booknow_booking.csv`: Contains booking records from the "BookNow" system.
*   `booknow_theaters.csv`: Details about theaters using the "BookNow" system (e.g., location, type).
*   `date_info.csv`: Calendar-related information for each date (e.g., day of the week, holidays).
*   ...and several others!

We use a special library called `pandas` in Python to easily read these files into something called a `DataFrame`, which is like a spreadsheet in Python.

```python
import pandas as pd
import os # For listing files

# Define paths to our data files
csv_path = {
    "booknow_booking.csv": "/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_booking/booknow_booking.csv",
    "booknow_theaters.csv": "/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_theaters/booknow_theaters.csv",
    "date_info.csv": "/kaggle/input/Cinema_Audience_Forecasting_challenge/date_info/date_info.csv",
    "booknow_visits.csv": "/kaggle/input/Cinema_Audience_Forecasting_challenge/booknow_visits/booknow_visits.csv",
    # ... other files listed here ...
}

# Load the essential dataframes
df_bookings = pd.read_csv(csv_path['booknow_booking.csv'])
df_theaters = pd.read_csv(csv_path['booknow_theaters.csv'])
df_date_info = pd.read_csv(csv_path['date_info.csv'])
df_audience = pd.read_csv(csv_path['booknow_visits.csv']) # This has our target: audience_count

print(f"Loaded Bookings data: {df_bookings.shape}")
print(f"Loaded Theaters data: {df_theaters.shape}")
print(f"Loaded Date Info data: {df_date_info.shape}")
print(f"Loaded Audience data: {df_audience.shape}")
```

This code loads four important CSV files into separate `DataFrame` objects (`df_bookings`, `df_theaters`, `df_date_info`, `df_audience`). Each `DataFrame` now holds a specific type of information, ready to be combined.

### Step 2: Creating the "Skeleton" DataFrame

Now that we have all our pieces, the next crucial step is to create a "skeleton" for our main data table. Imagine you have a calendar and a list of all the movie theaters. Our skeleton will be a table that lists *every single theater* for *every single day* within our project's timeframe.

Why do this? Because even if a theater had zero audience on a particular day (or was closed), we still need a row for that theater and date combination. This ensures our final dataset is comprehensive and "nothing is missing," as the concept description states.

```python
import datetime as dt

# Define our date range
start_date = dt.date(2023, 1, 1)
end_date = dt.date(2024, 2, 28) # End of our training data period
dates = pd.date_range(start=start_date, end=end_date)

# Create a list of all theater IDs
all_theater_ids = [f"book_{i:05d}" for i in range(1, 830)] # Assuming 829 theaters

# Create all possible combinations of theater ID and date
skeleton_df = pd.DataFrame(
    [(theater, date) for date in dates for theater in all_theater_ids],
    columns=["book_theater_id", "show_date"]
)

skeleton_df["show_date"] = pd.to_datetime(skeleton_df["show_date"]) # Convert date column
print(f"Skeleton DataFrame created with {skeleton_df.shape[0]} rows.")
skeleton_df.head()
```

This code snippet generates a `skeleton_df` with a row for every combination of a theater ID and a date from January 1, 2023, to February 28, 2024. This `DataFrame` is currently "empty" in terms of details, but its structure is perfect for holding all our integrated data.

### Step 3: Attaching the "Meat" (Merging External Metadata)

With our `skeleton_df` in place, it's time to "flesh it out" by attaching all the relevant information from our other data sources. This process is called "merging" or "joining" dataframes. We're essentially looking up information in one table and adding it to matching rows in another.

Let's merge the calendar information (`df_date_info`), the actual audience counts (`df_audience`), and the theater details (`df_theaters`) onto our `skeleton_df`.

```python
# Ensure 'show_date' in date_info is also datetime
df_date_info['show_date'] = pd.to_datetime(df_date_info['show_date'])

# Merge date information (e.g., day of week)
integrated_df = skeleton_df.merge(df_date_info, on='show_date', how='left')

# Merge actual audience counts (our target variable)
# 'how=left' ensures we keep all theater-date combinations from skeleton_df
integrated_df = integrated_df.merge(df_audience, on=['book_theater_id', 'show_date'], how='left')

# Merge theater details (e.g., type, area, location)
integrated_df = integrated_df.merge(df_theaters, on='book_theater_id', how='left')

print(f"Integrated DataFrame shape after merges: {integrated_df.shape}")
integrated_df.head()
```

After these merges, our `integrated_df` now contains rows for every theater-date combination, along with calendar information, theater specifics, and (where available) the `audience_count`. Notice the `how='left'` argument in `merge`. This is crucial: it means "keep all rows from the left (our `integrated_df` or `skeleton_df`) and add matching information from the right." If there's no match (e.g., no audience count for a specific date), it will fill with `NaN` (Not a Number), which we'll handle next.

### Step 4: Initial Cleanup (Filling Gaps)

Even after merging, some columns might have missing values (`NaN`). For example, `audience_count` will be missing for future dates we want to predict or for historical dates where no data was recorded. Other metadata, like `latitude` or `theater_area`, might also have gaps. A key part of data integration is ensuring these initial gaps are handled so our dataset is truly "clean."

```python
# Fill missing geographical coordinates with the median value
# Median is a good choice to avoid being skewed by extreme values
integrated_df['latitude'] = integrated_df['latitude'].fillna(integrated_df['latitude'].median())
integrated_df['longitude'] = integrated_df['longitude'].fillna(integrated_df['longitude'].median())

# Fill missing categorical theater information with 'unknown'
integrated_df['theater_area'] = integrated_df['theater_area'].fillna('unknown_area')
integrated_df['theater_type'] = integrated_df['theater_type'].fillna('unknown_type')

# For audience_count, fill NaN with 0 for now. This indicates no audience.
# We will filter these out later for training, but keep for a complete timeline.
integrated_df['audience_count'] = integrated_df['audience_count'].fillna(0)

print("Missing values after initial cleanup:")
print(integrated_df.isnull().sum()[integrated_df.isnull().sum() > 0])
```

This code block fills numerical missing values (like `latitude`, `longitude`) with their respective median values, which is a common strategy to maintain data distribution. For categorical columns (`theater_area`, `theater_type`), we fill `NaN` with a new category 'unknown_area' or 'unknown_type'. Finally, for `audience_count`, we replace `NaN` with `0`, meaning no audience was recorded.

## How the Data Integration Hub Works (Under the Hood)

Let's visualize the entire data integration process, from raw files to our complete dataset.

```mermaid
sequenceDiagram
    participant User
    participant Raw_CSVs as "Raw CSV Files"
    participant Pandas_Script as "Python Script (Pandas)"
    participant Skeleton_Table as "Skeleton DataFrame"
    participant Unified_Table as "Unified Dataset"

    User->>Pandas_Script: "Initiate Data Integration"
    Pandas_Script->>Raw_CSVs: "1. Load multiple CSVs"
    Raw_CSVs-->>Pandas_Script: "Individual DataFrames (e.g., df_theaters, df_date_info)"

    Pandas_Script->>Skeleton_Table: "2. Create all Theater-Date combinations"
    Skeleton_Table-->>Pandas_Script: "Empty Grid (skeleton_df)"
    Note over Pandas_Script: "Ensures every possible event is accounted for"

    Pandas_Script->>Unified_Table: "3. Merge df_date_info onto Skeleton"
    Pandas_Script->>Unified_Table: "   Merge df_audience onto Unified"
    Pandas_Script->>Unified_Table: "   Merge df_theaters onto Unified"
    Note over Unified_Table: "Attaching details to the grid"

    Pandas_Script->>Unified_Table: "4. Fill initial missing values (NaN)"
    Unified_Table-->>Pandas_Script: "Clean, Integrated Dataset (integrated_df)"
    Pandas_Script->>User: "Data is now ready for analysis!"
```

As you can see, our Python script acts as the central orchestrator. It first loads all the separate data pieces. Then, it constructs a complete calendar-like grid (the "skeleton") to ensure we have a spot for every possible observation. Finally, it systematically combines all the individual data points onto this skeleton, filling in gaps along the way to produce a single, comprehensive dataset.

The core tools used here are the `pd.read_csv()` function to load data, `pd.date_range()` and list comprehensions to create the skeleton, and the `DataFrame.merge()` method to combine dataframes. The `DataFrame.fillna()` method helps us clean up any initial missing values.

## Conclusion

In this chapter, you've learned about the **Data Integration Hub**, a critical first step in any machine learning project. We started with scattered data sources and successfully brought them together into one unified, clean dataset (`integrated_df`). This `integrated_df` is like our fully assembled puzzle board, ready for us to start drawing the actual picture.

Having a complete and organized dataset is fundamental. Without it, our machine learning models wouldn't have all the necessary information to learn from, leading to poor predictions.

Now that our data is integrated and cleaned, the next exciting step is to prepare it for our machine learning models by transforming it into useful features. We'll explore this in the next chapter: [Preprocessing & Transformation Blueprint](02_preprocessing___transformation_blueprint_.md).

---




































# Chapter 2: Preprocessing & Transformation Blueprint

Welcome back! In [Chapter 1: Data Integration Hub](01_data_integration_hub_.md), we successfully gathered all our scattered clues and assembled them into one organized, unified dataset, `integrated_df`. Think of it as having all your raw ingredients neatly laid out on the kitchen counter. But are they ready to be cooked? Not quite!

Imagine you're baking a cake. You wouldn't just throw raw eggs, flour, and sugar straight into the oven, right? You first need to crack the eggs, measure the flour, and mix them according to the recipe. In machine learning, our "raw ingredients" (the data in `integrated_df`) also need to be prepared and shaped before our machine learning "oven" (the model) can use them effectively.

This is where the **Preprocessing & Transformation Blueprint** comes in. It's like having a standardized recipe for preparing all your ingredients. This blueprint defines a clear, repeatable set of steps to clean, scale, and format your data, ensuring it's in the perfect state for any machine learning model.

## Why Do We Need a Blueprint?

Our `integrated_df` contains various types of information:
*   **Numerical data**: Like `latitude`, `longitude`, `audience_count`. Some might be missing, and they are all on different scales.
*   **Categorical data**: Like `theater_type` (e.g., 'multiplex', 'single_screen') and `theater_area` (e.g., 'north', 'south'). These are text, but machine learning models only understand numbers!

If we feed this raw, mixed data directly to a model, it would get confused.
*   **Missing values (NaNs)** will cause errors.
*   **Numbers on different scales** (e.g., `latitude` is around 30-40, while `audience_count` can be hundreds or thousands) might make the model think features with larger numbers are more important, even if they're not.
*   **Text data** is simply incomprehensible to mathematical models.

Our blueprint solves these problems by providing standard instructions to:
1.  **Handle Missing Values**: Fill in any gaps (NaNs) in a smart way.
2.  **Scale Numerical Features**: Bring all numbers to a similar, small range.
3.  **Encode Categorical Features**: Convert text categories into numerical formats.

Let's break down each step.

## 1. Handling Missing Values: Filling the Gaps

Even after our initial cleanup in Chapter 1, some columns might still have missing data points (`NaN`). Machine learning models don't like `NaN` values, just like a chef can't cook with half-missing ingredients.

The way we handle missing values depends on the type of data:

*   **For Geographical Coordinates (Numerical)**: Columns like `latitude` and `longitude` are numbers that represent locations. If a theater's exact coordinates are missing, a good strategy is to fill them with the **median** value of all other theaters. The median is the middle value, which is less affected by extreme high or low values compared to the average (mean).
    ```python
    # Example: Filling missing latitude and longitude
    # We use the median from the *entire* dataset for consistency
    median_latitude = f_t_df['latitude'].median()
    median_longitude = f_t_df['longitude'].median()

    f_t_df['latitude'] = f_t_df['latitude'].fillna(median_latitude)
    f_t_df['longitude'] = f_t_df['longitude'].fillna(median_longitude)

    print("Missing geographical coordinates filled!")
    ```
    *Explanation*: We calculate the median for `latitude` and `longitude` across our entire dataset. Then, we use these median values to fill any `NaN`s in their respective columns. This keeps our data consistent and ensures no missing location data breaks our model.

*   **For Theater Categories (Categorical)**: Columns like `theater_area` or `theater_type` are text categories. If a theater's area or type is missing, we can fill it with a special 'unknown' category. This way, the model knows that this piece of information was not available, treating 'unknown' as just another category.
    ```python
    # Example: Filling missing theater categories
    f_t_df['theater_area'] = f_t_df['theater_area'].fillna('unknown_area')
    f_t_df['theater_type'] = f_t_df['theater_type'].fillna('unknown_type')

    print("Missing categorical data filled with 'unknown' labels!")
    ```
    *Explanation*: Any missing values in `theater_area` or `theater_type` are now replaced with new descriptive strings like `'unknown_area'` or `'unknown_type'`. This prevents errors and allows the model to learn from these "unknown" cases.

## 2. Scaling Numerical Features: Making Numbers Play Fair

Imagine trying to compare the height of a person (measured in feet) to the weight of an apple (measured in grams). The numbers would be vastly different, making direct comparison difficult. Our machine learning models face a similar challenge when features like `latitude` (numbers like 30.5) and `audience_count` (numbers like 1500) are used together.

**Scaling** means transforming numerical features so they fit within a specific, common range (e.g., between 0 and 1). This ensures that no single feature dominates the model's learning process just because it has naturally larger values.

A popular tool for this is `MinMaxScaler`. It takes all your numbers, finds the smallest (minimum) and largest (maximum) values, and then rescales everything so the minimum becomes 0, the maximum becomes 1, and all other numbers fall in between.

```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Let's use some example numerical columns from our integrated_df
numerical_features = ['latitude', 'longitude', 'day_of_year'] # Simplified for brevity

# Create a scaler
scaler = MinMaxScaler()

# Fit the scaler to our data and transform it
# (We'll use a copy of our full dataframe for demonstration)
scaled_data = scaler.fit_transform(f_t_df[numerical_features])

# The output is a NumPy array, let's put it back into a DataFrame to see
scaled_df = pd.DataFrame(scaled_data, columns=numerical_features)

print("Original Latitude (first 5 rows):\n", f_t_df['latitude'].head())
print("\nScaled Latitude (first 5 rows):\n", scaled_df['latitude'].head())
# Output will show numbers between 0 and 1
```
*Explanation*: We select our numerical columns and create a `MinMaxScaler`. The `fit_transform()` method first "learns" the min/max values from our data and then applies the scaling. Notice how the original `latitude` values (e.g., 34.05) are now transformed into small numbers between 0 and 1 (e.g., 0.54). All numerical features will now be on this same playing field.

## 3. Encoding Categorical Features: Turning Text into Numbers

Machine learning models are built on mathematics, so they can only process numerical inputs. This means text-based features like `theater_type` ('multiplex', 'single_screen') or `day_of_week` ('Monday', 'Tuesday') need to be converted into numbers.

**Encoding** is the process of turning these text categories into numerical representations. One of the most common and effective methods is **One-Hot Encoding** using `OneHotEncoder`.

Here's how it works:
If you have a column `theater_type` with categories 'multiplex', 'single_screen', and 'unknown_type', One-Hot Encoding creates *new* columns for each unique category. For each row, it places a `1` in the column corresponding to that row's category and `0`s in all other new columns.

| `theater_type` | `theater_type_multiplex` | `theater_type_single_screen` | `theater_type_unknown_type` |
| :------------- | :----------------------- | :--------------------------- | :-------------------------- |
| multiplex      | 1                        | 0                            | 0                           |
| single_screen  | 0                        | 1                            | 0                           |
| unknown_type   | 0                        | 0                            | 1                           |

This way, the model gets numerical input without assuming any false order or relationship between categories.

```python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Let's use some example categorical columns
categorical_features = ['theater_type', 'theater_area'] # Simplified for brevity

# Create an encoder
# handle_unknown='ignore' is crucial: it prevents errors if new, unseen categories appear in test data
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Fit the encoder to our data and transform it
encoded_data = encoder.fit_transform(f_t_df[categorical_features])

# The output is a NumPy array, let's look at its shape
print("Shape of encoded data:", encoded_data.shape)
# Output might be something like (453463, 100), meaning 100 new columns created
print("Number of new features created for categorical data:", encoded_data.shape[1])
# We can also see the new column names
# print(encoder.get_feature_names_out(categorical_features))
```
*Explanation*: We apply `OneHotEncoder` to our categorical features. It learns all unique categories and then transforms these columns into new numerical columns (a matrix of 0s and 1s). `sparse_output=False` makes the output a dense array, which is easier to work with for beginners.

## The Preprocessing & Transformation Blueprint in Action

To apply all these steps efficiently, especially to different types of columns, we use a powerful tool from `scikit-learn` called `ColumnTransformer`. It's like a central coordinator that applies specific transformations (like scaling or encoding) to specific sets of columns in your data.

Here’s how we'll build our blueprint using `ColumnTransformer`:

```python
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import pandas as pd # Assuming f_t_df is already loaded as in Chapter 1

# 1. Identify your numerical and categorical columns
# (These lists are simplified for demonstration; in actual code, they are longer)
numerical_features = ['latitude', 'longitude', 'show_day', 'show_month']
categorical_features = ['book_theater_id', 'day_of_week', 'theater_type', 'theater_area']

# 2. Define the transformers for each type of feature
numeric_transformer = MinMaxScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# 3. Create the ColumnTransformer (our blueprint!)
preprocessor = ColumnTransformer(
    transformers=[
        ('num_pipeline', numeric_transformer, numerical_features),
        ('cat_pipeline', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep any other columns untouched
)

print("Preprocessing & Transformation Blueprint is ready!")
# We now have a 'preprocessor' object that can transform our data
```
*Explanation*:
*   We define `numerical_features` and `categorical_features` lists, containing the names of columns to be treated.
*   `numeric_transformer` is our `MinMaxScaler`.
*   `categorical_transformer` is our `OneHotEncoder`.
*   The `ColumnTransformer` is then set up:
    *   `('num_pipeline', numeric_transformer, numerical_features)`: Apply `numeric_transformer` to `numerical_features`.
    *   `('cat_pipeline', categorical_transformer, categorical_features)`: Apply `categorical_transformer` to `categorical_features`.
    *   `remainder='passthrough'`: Any columns *not* explicitly listed in `numerical_features` or `categorical_features` will be kept as they are.

Once this `preprocessor` blueprint is defined, you can `fit` it to your training data (to learn min/max values for scaling and unique categories for encoding) and then `transform` both your training and new data using these learned rules.

```python
# Assuming f_t_df from Chapter 1 is available
# And numerical_features, categorical_features are defined as above

# Fit the preprocessor to our entire dataset (or just the training part)
# This step 'learns' the necessary rules (min/max, unique categories)
preprocessor.fit(f_t_df)

# Now, transform the data using the learned rules
transformed_data = preprocessor.transform(f_t_df)

print(f"Original data shape: {f_t_df.shape}")
print(f"Transformed data shape: {transformed_data.shape}")
# The transformed data will have many more columns due to One-Hot Encoding
```
*Explanation*:
*   `preprocessor.fit(f_t_df)`: The blueprint "learns" from the entire dataset (`f_t_df`). For `MinMaxScaler`, it finds the min and max for each numerical column. For `OneHotEncoder`, it identifies all unique categories in the categorical columns.
*   `preprocessor.transform(f_t_df)`: The blueprint now applies these learned transformations to the data. Numerical columns are scaled, and categorical columns are replaced by their one-hot encoded versions.
*   The output `transformed_data` is a NumPy array that is entirely numerical and scaled, ready for a machine learning model! Its shape will likely have more columns because `OneHotEncoder` expands categorical features into many new columns.

## How the Preprocessing Blueprint Works (Under the Hood)

Let's visualize how `ColumnTransformer` orchestrates these preprocessing steps:

```mermaid
sequenceDiagram
    participant Raw_Data as "Raw Data (integrated_df)"
    participant Column_Transformer as "ColumnTransformer"
    participant MinMaxScaler as "MinMaxScaler"
    participant OneHotEncoder as "OneHotEncoder"
    participant Model_Ready_Data as "Model-Ready Data"

    Raw_Data->>Column_Transformer: Send data for preprocessing
    Note over Column_Transformer: Identifies numerical and categorical columns

    Column_Transformer->>MinMaxScaler: Sends numerical columns
    MinMaxScaler-->>Column_Transformer: Returns scaled numerical data (0-1 range)

    Column_Transformer->>OneHotEncoder: Sends categorical columns
    OneHotEncoder-->>Column_Transformer: Returns one-hot encoded numerical data (0s and 1s)

    Column_Transformer->>Model_Ready_Data: Combines all transformed features
    Note over Model_Ready_Data: All features are now numerical and scaled
```

As you can see, `ColumnTransformer` acts as a central hub. It intelligently directs different parts of your raw data to the right transformation tools (`MinMaxScaler` for numbers, `OneHotEncoder` for categories). Once each tool has done its job, `ColumnTransformer` collects all the processed pieces and combines them into one final, model-ready dataset.

Here’s a summary of the core transformations in our blueprint:

| Transformation              | What it does                                                 | Why it's useful                                                              | Tool / Strategy                      |
| :-------------------------- | :----------------------------------------------------------- | :--------------------------------------------------------------------------- | :----------------------------------- |
| **Handling Missing Values** | Fills `NaN`s in numerical and categorical data.              | Prevents model errors; provides complete input for learning.                 | `fillna()` with median or 'unknown'  |
| **Scaling Numerical Features** | Changes numerical feature values to a common, small range (e.g., 0 to 1). | Ensures no single feature's large values dominate the model's learning.      | `MinMaxScaler`                       |
| **Encoding Categorical Features** | Converts text-based categories into numerical representations (0s and 1s). | Allows machine learning models (which understand numbers) to process text data. | `OneHotEncoder`                      |

## Conclusion

In this chapter, you've learned about the **Preprocessing & Transformation Blueprint**. We've established a standard set of steps to prepare our data, moving from the raw, integrated data to a format that machine learning models can readily understand. We've tackled missing values, scaled numerical features, and encoded categorical data into numbers.

With our data now thoroughly cleaned, scaled, and encoded, it's in excellent shape for machine learning. But we can still extract even more valuable information! In the next chapter, we'll dive into creating powerful features specifically designed for our time-series forecasting problem. Get ready for [Chapter 3: Time-Series Feature Factory](03_time_series_feature_factory_.md)!

---





































# Chapter 3: Time-Series Feature Factory

Welcome back, aspiring data scientist! In [Chapter 1: Data Integration Hub](01_data_integration_hub_.md), we bravely gathered all our scattered data pieces into one organized `integrated_df`. Then, in [Chapter 2: Preprocessing & Transformation Blueprint](02_preprocessing___transformation_blueprint_.md), we refined these ingredients by handling missing values, scaling numbers, and converting text into numbers. Our data, now represented by `f_t_df` (a more "feature-rich" version of `integrated_df` as we add more to it), is clean and ready.

But for our cinema audience forecasting challenge, "clean" and "ready" isn't quite enough. We're dealing with *time-series* data – data where the order and timing of events matter a lot. Just like predicting tomorrow's weather isn't just about today's temperature, but also yesterday's, and the day before, predicting cinema audience needs to consider the flow of time.

This is where our **Time-Series Feature Factory** comes in. Imagine it as a special kitchen dedicated to preparing ingredients that capture the "story" of our data over time. It doesn't just look at what happened on one day; it looks at patterns, trends, and past events to predict the future. This module will help us extract hidden patterns and valuable insights from our `show_date` column, transforming simple dates into powerful predictive features.

## Why Do We Need a Time-Series Feature Factory?

Our goal is to forecast `audience_count`. Think about it:
*   Does the `day_of_week` matter? (Weekend vs. weekday). Yes!
*   Does the `month` matter? (Holiday season vs. off-season). Yes!
*   Does the audience `yesterday` affect the audience `today`? Definitely!
*   What about the `average audience over the last 7 days`? That's a strong indicator too!

Standard preprocessing doesn't capture these time-dependent relationships directly. The Time-Series Feature Factory focuses on creating features that explicitly tell our machine learning model about:
1.  **Date Components**: Breaking down a date into its parts.
2.  **Cyclical Nature of Time**: Representing repeating patterns (like days of the week, months of the year).
3.  **Past Events (Lag Features)**: What happened a certain number of periods ago.
4.  **Recent Trends (Rolling Statistics)**: Averages or variations over recent periods.

Let's see how we can build these powerful features.

## 1. Extracting Date Components

Dates often contain a wealth of information. We can extract simple components directly from our `show_date` column.

```python
import pandas as pd
import numpy as np # For cyclical features later

# Assuming f_t_df is our integrated and preprocessed DataFrame
# Make sure 'show_date' is in datetime format
f_t_df['show_date'] = pd.to_datetime(f_t_df['show_date'])

# Extract basic date features
f_t_df['show_day'] = f_t_df['show_date'].dt.day
f_t_df['show_month'] = f_t_df['show_date'].dt.month
f_t_df['show_year'] = f_t_df['show_date'].dt.year
f_t_df['day_of_week_num'] = f_t_df['show_date'].dt.dayofweek # Monday=0, Sunday=6

# Create weekend indicators
f_t_df['is_weekend'] = (f_t_df['day_of_week_num'] >= 5).astype(int) # Saturday (5) and Sunday (6)
f_t_df['is_sunday'] = (f_t_df['day_of_week_num'] == 6).astype(int)
f_t_df['is_friday'] = (f_t_df['day_of_week_num'] == 4).astype(int)

print("New date components added:")
print(f_t_df[['show_date', 'show_day', 'is_weekend']].head())
```
*Explanation*: We use the `.dt` accessor in pandas to easily pull out parts of our date column. We get the day, month, year, and a numerical representation of the day of the week. We also create simple "yes/no" (0/1) features like `is_weekend`, `is_sunday`, and `is_friday` because these days often have unique audience patterns.

## 2. Creating Cyclical Representations

The day of the week (Monday-Sunday) or month of the year (January-December) are cyclical. Monday follows Sunday, and January follows December. If we just use numbers (0-6 for days, 1-12 for months), the model might think December (12) is much "further" from January (1) than November (11) is, which isn't true cyclically.

To capture this, we use sine and cosine transformations. This creates two new features for each cyclical component, placing them on a circle, so the start and end values are close together.

```python
# Cyclical encodings for day of week (7 days in a week)
f_t_df['dow_sin'] = np.sin(2 * np.pi * f_t_df['day_of_week_num'] / 7)
f_t_df['dow_cos'] = np.cos(2 * np.pi * f_t_df['day_of_week_num'] / 7)

# Cyclical encodings for month (12 months in a year)
f_t_df['month_sin'] = np.sin(2 * np.pi * f_t_df['show_month'] / 12)
f_t_df['month_cos'] = np.cos(2 * np.pi * f_t_df['show_month'] / 12)

print("\nNew cyclical features added:")
print(f_t_df[['day_of_week_num', 'dow_sin', 'dow_cos']].head())
```
*Explanation*: For `day_of_week_num`, we divide the day number by 7 (total days) and multiply by `2 * pi` to get an angle. Then we apply `np.sin` and `np.cos`. This effectively turns a linear scale (0, 1, ..., 6) into points on a circle, where 0 and 6 are numerically distinct but geometrically close, just like Monday and Sunday. The same logic applies to months.

## 3. Computing Lag Features: Looking to the Past

"Lag" features are simply values from previous time steps. For forecasting, knowing the `audience_count` from yesterday, last week, or even last month can be incredibly predictive of today's audience.

For our cinema project, we need to create lag features *for each specific theater*. An audience count at `book_00001` yesterday doesn't tell us much about `book_00002` today.

```python
# First, ensure our data is sorted by theater and then by date
f_t_df = f_t_df.sort_values(['book_theater_id', 'show_date']).reset_index(drop=True)

# Create lag features for 'audience_count'
# We'll use lags of 1 (yesterday), 7 (last week), 14 (two weeks ago), and 30 (last month)
for lag in [1, 7, 14, 30]:
    f_t_df[f'audience_lag_{lag}'] = f_t_df.groupby('book_theater_id')['audience_count'].shift(lag)

# Fill any NaN values created by shift (at the start of each theater's history) with 0
# This assumes if there's no previous data, the audience was 0 or unknown.
for lag in [1, 7, 14, 30]:
    f_t_df[f'audience_lag_{lag}'] = f_t_df[f'audience_lag_{lag}'].fillna(0)

print("\nNew lag features added (first 5 rows with lag values):")
print(f_t_df[['show_date', 'book_theater_id', 'audience_count', 'audience_lag_1', 'audience_lag_7']].head(10))
```
*Explanation*:
*   We first sort our `f_t_df` by `book_theater_id` and then `show_date`. This is crucial so `shift()` correctly looks at *previous dates for the same theater*.
*   `groupby('book_theater_id')` ensures that the `shift()` operation is performed independently for each theater.
*   `.shift(lag)` moves the `audience_count` value down by `lag` rows. So, `shift(1)` gives yesterday's value, `shift(7)` gives last week's value, etc.
*   `fillna(0)` handles the `NaN` values that appear at the beginning of each theater's data (because there's no historical data for those first few `lag` days).

## 4. Computing Rolling Statistics: Capturing Trends

While lag features tell us a specific point in the past, "rolling statistics" capture trends over a recent period. For example, knowing the average audience over the *last 7 days* can reveal if a cinema is generally busy or slow. The standard deviation over a period can tell us how consistent the audience numbers are.

Again, these statistics should be computed *per theater*.

```python
# Create rolling features (mean and standard deviation)
# We'll calculate over windows of 7 (last week), 14 (last two weeks), and 30 (last month)
for window in [7, 14, 30]:
    # Calculate rolling mean. We shift by 1 first to avoid data leakage (using today's data to predict today)
    f_t_df[f'audience_roll_mean_{window}'] = (
        f_t_df.groupby('book_theater_id')['audience_count']
              .transform(lambda x: x.shift(1).rolling(window, min_periods=1).mean())
    )
    # Calculate rolling standard deviation.
    f_t_df[f'audience_roll_std_{window}'] = (
        f_t_df.groupby('book_theater_id')['audience_count']
              .transform(lambda x: x.shift(1).rolling(window, min_periods=1).std())
    )

# Fill any NaN values created by rolling statistics (at the start of each theater's history) with 0
# Standard deviation might also be NaN if there's only one data point in the window.
for window in [7, 14, 30]:
    f_t_df[f'audience_roll_mean_{window}'] = f_t_df[f'audience_roll_mean_{window}'].fillna(0)
    f_t_df[f'audience_roll_std_{window}'] = f_t_df[f'audience_roll_std_{window}'].fillna(0)

print("\nNew rolling features added (first 5 rows with rolling values):")
print(f_t_df[['show_date', 'book_theater_id', 'audience_count', 'audience_roll_mean_7', 'audience_roll_std_7']].head(10))
```
*Explanation*:
*   We use `groupby('book_theater_id')` and `transform()` to apply the rolling calculation for each theater independently.
*   `x.shift(1)` is critical: it shifts the `audience_count` by one day *before* calculating the rolling window. This ensures we are only using *past* data to calculate the mean/std for the *current* day, preventing "data leakage" (where our model accidentally learns from information it wouldn't have at prediction time).
*   `.rolling(window, min_periods=1)` calculates the rolling statistic. `min_periods=1` means it will calculate a statistic even if there's only one data point available in the window (e.g., the first day of a 7-day window might only have 1 day's data).
*   `mean()` and `std()` compute the average and standard deviation, respectively.
*   `fillna(0)` handles `NaN` values for the initial days of each theater where a full rolling window isn't yet available.

## How the Time-Series Feature Factory Works (Under the Hood)

Let's visualize the process of generating these time-aware features:

```mermaid
sequenceDiagram
    participant Initial_Data as "Integrated Data (f_t_df)"
    participant Feature_Factory as "Time-Series Feature Factory"
    participant Date_Extractor as "Date Extractor"
    participant Cyclical_Transformer as "Cyclical Transformer"
    participant Lag_Calculator as "Lag Calculator"
    participant Rolling_Calculator as "Rolling Stats Calculator"
    participant Enriched_Data as "Enriched Data"

    Initial_Data->>Feature_Factory: Send 'show_date' and 'audience_count'
    Note over Feature_Factory: Orchestrates feature generation

    Feature_Factory->>Date_Extractor: Extract components from 'show_date'
    Date_Extractor-->>Feature_Factory: Returns 'show_day', 'is_weekend', etc.

    Feature_Factory->>Cyclical_Transformer: Transform 'day_of_week_num', 'show_month'
    Cyclical_Transformer-->>Feature_Factory: Returns 'dow_sin', 'month_cos', etc.

    Feature_Factory->>Lag_Calculator: Calculate past 'audience_count' for each theater
    Lag_Calculator-->>Feature_Factory: Returns 'audience_lag_1', 'audience_lag_7', etc.

    Feature_Factory->>Rolling_Calculator: Calculate recent trends (mean/std) for each theater
    Rolling_Calculator-->>Feature_Factory: Returns 'audience_roll_mean_7', 'audience_roll_std_7', etc.

    Feature_Factory->>Enriched_Data: Combines all new features with existing data
    Note over Enriched_Data: Data is now highly predictive for time-series models
```

The Time-Series Feature Factory takes our existing `f_t_df` and systematically adds these new, powerful features. It ensures that operations like lag and rolling statistics are performed correctly for each theater, preserving the individual time-series of each entity.

Here’s a summary of the features generated in this factory:

| Feature Type               | What it captures                                       | Why it's useful                                                            | Python Tool/Method                 |
| :------------------------- | :----------------------------------------------------- | :------------------------------------------------------------------------- | :--------------------------------- |
| **Date Components**        | Day, month, year, day of week, weekend flags.          | Identifies daily, weekly, monthly, yearly patterns in audience.            | `DataFrame.dt` accessor            |
| **Cyclical Features**      | Circular representation of day of week, month of year. | Prevents artificial linear relationships for cyclical data (e.g., Dec-Jan). | `np.sin()`, `np.cos()`             |
| **Lag Features**           | Audience count from previous days (e.g., yesterday, last week). | Direct historical influence, strong predictor of immediate future.       | `groupby().shift()`                |
| **Rolling Statistics**     | Mean/Standard Deviation of audience over recent periods. | Captures recent trends, stability, or volatility in audience numbers.      | `groupby().rolling().mean()`, `std()` |

## Conclusion

In this chapter, you've mastered the **Time-Series Feature Factory**. We've transformed simple date and audience data into a rich set of predictive features, including various date components, cyclical representations, historical "lag" values, and recent "rolling" trends. These features are crucial for any time-series forecasting problem, enabling our models to understand and leverage the temporal dynamics of the data.

Our `f_t_df` is now packed with information, not just about what's happening now, but also about the context of time and how events unfolded in the past. This makes our dataset exceptionally well-prepared for our machine learning models.

Next, we'll take this feature-rich dataset and put it to work in the [Chapter 4: Model Experimentation Workbench](04_model_experimentation_workbench_.md), where we'll train, evaluate, and fine-tune various machine learning models to make our actual audience predictions.

---











































# Chapter 4: Model Experimentation Workbench

Welcome back, future data scientists! In our journey so far, we've gone from raw, scattered information to a rich, time-aware dataset. In [Chapter 1: Data Integration Hub](01_data_integration_hub_.md), we assembled our data. Then, in [Chapter 2: Preprocessing & Transformation Blueprint](02_preprocessing___transformation_blueprint_.md), we cleaned, scaled, and encoded it. And just recently, in [Chapter 3: Time-Series Feature Factory](03_time_series_feature_factory_.md), we engineered powerful time-based features crucial for forecasting cinema audience. Our `f_t_df` is now a treasure chest of information, perfectly prepared.

Now what? We have all these fantastic features, but how do we turn them into actual predictions? This is where the magic of machine learning models comes in! But choosing the *best* model isn't like picking a ready-made tool; it's more like being a scientist in a laboratory. You need to try different approaches, carefully measure their results, and fine-tune them until you get the best possible outcome.

This entire process of trying, testing, and perfecting different machine learning models is what we call the **Model Experimentation Workbench**. Think of it as your project's advanced laboratory. Here, you'll put various machine learning "recipes" (algorithms) to the test, evaluate how well they predict, and make adjustments to get the most accurate forecasts for our cinema audience challenge.

## Why Do We Need a Model Experimentation Workbench?

Our goal is to predict the `audience_count`. But there isn't just one machine learning model that magically works best for every problem. Some models are good with simple linear relationships, others excel at complex patterns.
For our cinema audience forecasting, we need a systematic way to:

1.  **Try Different Algorithms**: Is a simple Linear Regression enough, or do we need a more complex model like a Decision Tree or Gradient Boosting?
2.  **Ensure Consistency**: How do we make sure that every time we test a new model, it uses the exact same data preparation steps we defined in [Chapter 2](02_preprocessing___transformation_blueprint_.md)?
3.  **Measure Success**: How do we objectively tell if one model is "better" than another? We need specific metrics.
4.  **Optimize Models**: Even with a good model, it often has "knobs and dials" (hyperparameters) that need to be adjusted for peak performance.

The Model Experimentation Workbench gives us the tools and methods to answer these questions and systematically find the best predictive model.

## Key Components of Our Workbench

Our workbench consists of three main ideas:
1.  **Machine Learning Pipelines**: Combining data preparation and modeling into one consistent flow.
2.  **Model Evaluation Metrics**: Quantifying how well our models perform.
3.  **Hyperparameter Tuning**: Optimizing model "knobs and dials" for the best results.

Let's explore each one.

### 1. Machine Learning Pipelines: Your Consistent "Recipe"

Imagine you have a complex data preprocessing sequence (scaling numbers, encoding categories, adding time-series features). Every time you want to train a new machine learning model, you would have to run all these steps first. This can be repetitive and error-prone.

A **Machine Learning Pipeline** from `scikit-learn` is like a complete, automated assembly line for your data. It links all your preprocessing steps and your chosen machine learning model into one single, coherent object.

Our pipeline will look something like this:
`Raw Data` -> `Preprocessing Blueprint (ColumnTransformer)` -> `Machine Learning Model` -> `Predictions`

This ensures that whenever you train or make predictions with a pipeline, the data always goes through the *exact same* sequence of transformations before reaching the model.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeRegressor # Our first simple model

# --- Recall from Chapter 2: Our Preprocessing Blueprint ---
# (Simplified feature lists for this example)
numerical_features = ['latitude', 'longitude', 'show_day']
categorical_features = ['book_theater_id', 'theater_type']

# Define transformers
numeric_transformer = MinMaxScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Create the ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough'
)

# --- Now, create the full Pipeline ---
model = DecisionTreeRegressor(random_state=42) # Our chosen ML model

full_pipeline = Pipeline([
    ('preprocess', preprocessor), # Step 1: Apply all preprocessing
    ('model', model)             # Step 2: Train the Decision Tree model
])

print("Our full ML pipeline is ready!")
# You can now use full_pipeline.fit(X, y) and full_pipeline.predict(X_new)
```
*Explanation*: We combine our `preprocessor` (which takes care of scaling and encoding, as learned in [Chapter 2](02_preprocessing___transformation_blueprint_.md)) with a `DecisionTreeRegressor` into a `Pipeline`. Now, when we `fit` this `full_pipeline` to our `X_train` data, it automatically preprocesses `X_train` *then* trains the `DecisionTreeRegressor`. Similarly, when we `predict`, it preprocesses the new data *then* uses the trained model to make predictions.

### 2. Model Evaluation Metrics: How Good are Our Predictions?

After training a model, we need to know how well it performed. Just saying "it's good" isn't enough; we need objective numbers. This is where evaluation metrics come in. For our regression task (predicting a number like `audience_count`), two common metrics are:

*   **R² (R-squared)**: This metric tells us how much of the variation in our target variable (`audience_count`) can be explained by our model.
    *   A value of `1.0` means the model perfectly explains all the variance (perfect prediction).
    *   A value of `0.0` means the model explains none of the variance (it's no better than simply predicting the average).
    *   Negative values mean the model is worse than simply predicting the average, indicating a very poor fit.
    *   *Analogy*: Imagine trying to predict how many apples are in a basket. R² tells you how much of the actual number of apples your prediction accounts for.

*   **MAE (Mean Absolute Error)**: This metric tells us the average absolute difference between our model's predictions and the actual values.
    *   A lower MAE means predictions are, on average, closer to the actual values.
    *   It's easy to understand: if MAE is 10, it means, on average, our predictions are off by 10 audience members.
    *   *Analogy*: If you predict 50 apples and there are 55, your error is 5. If you predict 40 and there are 35, your error is 5. MAE averages these absolute errors.

```python
from sklearn.metrics import r2_score, mean_absolute_error
import numpy as np

# Dummy actual values (y_true) and model predictions (y_pred)
y_true = np.array([100, 150, 200, 120, 180]) # Actual audience counts
y_pred = np.array([105, 145, 190, 130, 175]) # Model's predictions

# Calculate R²
r2 = r2_score(y_true, y_pred)
print(f"R-squared (R²): {r2:.4f}") # Higher is better, closer to 1

# Calculate MAE
mae = mean_absolute_error(y_true, y_pred)
print(f"Mean Absolute Error (MAE): {mae:.2f}") # Lower is better, closer to 0
```
*Explanation*: We use `r2_score` and `mean_absolute_error` functions from `sklearn.metrics`. These functions take the true values (`y_true`) and the predicted values (`y_pred`) and return the respective scores.

### 3. Hyperparameter Tuning with RandomizedSearchCV: Fine-tuning Your Model

Machine learning models have two types of parameters:
*   **Learned parameters**: These are learned automatically from the data during training (e.g., the weights in a Linear Regression).
*   **Hyperparameters**: These are settings you choose *before* training the model (e.g., `max_depth` in a Decision Tree, `n_estimators` in a Random Forest). They control *how* the model learns.

Choosing the right hyperparameters can significantly impact a model's performance. It's like having a complicated audio system: the "volume" and "bass" knobs (hyperparameters) need to be adjusted just right for the best sound. Trying every single combination of hyperparameters is called "Grid Search," but it can be very slow for many parameters.

**RandomizedSearchCV** is a smarter way to tune hyperparameters. Instead of trying every combination, it tries a fixed number of *random* combinations from a specified range. This is often much faster and can still find very good hyperparameter settings.

```python
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor
import pandas as pd
import numpy as np

# --- Reusing the preprocessor from earlier for the pipeline ---
# (Assume 'preprocessor' is already defined from Section 1)

# Create a sample dataset for demonstration
# In a real scenario, this would be your X_train and y_train
X_dummy = pd.DataFrame({
    'latitude': np.random.rand(100) * 10 + 30,
    'longitude': np.random.rand(100) * 10 + 70,
    'show_day': np.random.randint(1, 31, 100),
    'book_theater_id': [f'book_{i:05d}' for i in np.random.randint(1, 10, 100)],
    'theater_type': np.random.choice(['multiplex', 'single_screen'], 100)
})
y_dummy = pd.Series(np.random.rand(100) * 200)

# Create the pipeline with a DecisionTreeRegressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('decision_tree', dt_model) # The model we want to tune
])

# Define the search space for hyperparameters
# The prefix 'decision_tree__' refers to the 'decision_tree' step in the pipeline
param_dist = {
    'decision_tree__max_depth': [5, 10, 15, 20, None], # Max depth of the tree
    'decision_tree__min_samples_split': [2, 10, 20]    # Min samples required to split a node
}

# For time-series data, regular cross-validation can be problematic
# because it might use future data to predict the past.
# TimeSeriesSplit ensures that validation sets always come *after* training sets.
tscv = TimeSeriesSplit(n_splits=2) # Uses 2 splits for demonstration

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=dt_pipeline,        # Our pipeline
    param_distributions=param_dist, # The hyperparameters to search
    n_iter=5,                     # Number of random combinations to try (reduced for quick example)
    cv=tscv,                      # The cross-validation strategy
    scoring='r2',                 # Metric to optimize (maximize R²)
    random_state=42,
    n_jobs=-1,                    # Use all available CPU cores
    verbose=0                     # Set to 1 or 2 for more output
)

# Run the search (this can take a while!)
print("Starting RandomizedSearchCV...")
random_search.fit(X_dummy, y_dummy) # Fit to your training data (X_train, y_train)
print("RandomizedSearchCV complete!")

# Get the best parameters and best score
print(f"\nBest hyperparameters found: {random_search.best_params_}")
print(f"Best R² score from cross-validation: {random_search.best_score_:.4f}")
```
*Explanation*:
1.  We define `dt_pipeline` which includes our `preprocessor` and `DecisionTreeRegressor`.
2.  `param_dist` specifies the range of hyperparameters we want to explore for the `decision_tree` step in our pipeline. Notice the `decision_tree__` prefix.
3.  `TimeSeriesSplit` is crucial for our time-series problem. It makes sure that during cross-validation, the model is always trained on older data and tested on newer data, mimicking real-world forecasting.
4.  `RandomizedSearchCV` then intelligently samples `n_iter` combinations of these hyperparameters, trains and evaluates the `dt_pipeline` with each, and reports the best combination and its corresponding R² score.

## How the Model Experimentation Workbench Works (Under the Hood)

Let's visualize the flow when you use the Model Experimentation Workbench to find the best settings for your model:

```mermaid
sequenceDiagram
    participant User
    participant TrainingData as "Training Data (X_train, y_train)"
    participant PreprocessingBlueprint as "Preprocessing Blueprint (ColumnTransformer)"
    participant ML_Pipeline as "ML Pipeline (Preprocessing + Model)"
    participant RandomizedSearch as "RandomizedSearchCV"
    participant PerformanceMetrics as "Performance Metrics (R², MAE)"
    participant BestModel as "Best Tuned Model"

    User->>ML_Pipeline: "Define pipeline with model placeholder"
    User->>RandomizedSearch: "Define hyperparameter search space"
    User->>RandomizedSearch: "Initiate search with Training Data"

    loop N random combinations
        RandomizedSearch->>ML_Pipeline: "Get pipeline with current hyperparameters"
        ML_Pipeline->>TrainingData: "Fit Preprocessing Blueprint"
        TrainingData-->>PreprocessingBlueprint: Transformed data
        PreprocessingBlueprint-->>ML_Pipeline: Transformed data
        ML_Pipeline->>ML_Pipeline: "Train model on transformed data"
        ML_Pipeline->>ML_Pipeline: "Make predictions"
        ML_Pipeline-->>PerformanceMetrics: Predictions & True values
        PerformanceMetrics-->>RandomizedSearch: R², MAE scores
    end

    RandomizedSearch->>BestModel: "Select hyperparameters with best score"
    BestModel-->>User: "Best Model and Parameters"
    Note over User: "Now ready for final training and forecasting!"
```

The workbench orchestrates the entire process. It takes your raw training data, passes it through the predefined preprocessing steps using the `ColumnTransformer` within the `Pipeline`. Then, it systematically tries different model "knobs and dials" (hyperparameters) as instructed by `RandomizedSearchCV`. For each trial, it measures the model's performance using metrics like R² and MAE. Finally, it tells you which combination of hyperparameters yielded the best results, giving you your optimally tuned model.

Here’s a summary of the core tools and their roles in our workbench:

| Tool / Concept            | What it does                                                 | Why it's useful                                                                 |
| :------------------------ | :----------------------------------------------------------- | :------------------------------------------------------------------------------ |
| **`Pipeline`**            | Chains multiple data processing steps and a model into one object. | Ensures consistent data flow from raw input to prediction; simplifies workflow. |
| **R² Score**              | Measures how well the model explains variance in the target. | Gives an intuitive understanding of model fit (0-1, higher is better).         |
| **MAE (Mean Absolute Error)** | Measures the average absolute difference between predictions and actuals. | Provides a clear, interpretable error value in the original units.              |
| **`RandomizedSearchCV`**  | Efficiently searches for the best hyperparameters by sampling combinations. | Optimizes model performance without exhaustively trying every possibility.      |
| **`TimeSeriesSplit`**     | Ensures correct cross-validation for time-series data.     | Prevents data leakage by always training on past data and validating on future. |

## Conclusion

In this chapter, you've stepped into the **Model Experimentation Workbench**! You've learned how to streamline the process of building and evaluating machine learning models using `Pipelines`, how to quantify their performance with `R²` and `MAE`, and how to fine-tune them for optimal results using `RandomizedSearchCV` with appropriate `TimeSeriesSplit` cross-validation. This robust setup allows us to systematically test various algorithms and their configurations to find the best one for our cinema audience forecasting.

With our best models identified and tuned, we're now ready for the final, crucial step: using them to make predictions into the future, iterating as new data becomes available. This is what we'll explore in the next exciting chapter: [Chapter 5: Iterative Forecasting Engine](05_iterative_forecasting_engine_.md).

---































# Chapter 5: Iterative Forecasting Engine

Welcome to the final chapter of your Machine Learning Capstone Project! In our journey, we've come a long way. In [Chapter 1: Data Integration Hub](01_data_integration_hub_.md), we bravely gathered all our scattered data. Then, in [Chapter 2: Preprocessing & Transformation Blueprint](02_preprocessing___transformation_blueprint_.md), we cleaned and prepared it. [Chapter 3: Time-Series Feature Factory](03_time_series_feature_factory_.md) taught us how to create powerful time-based features like `lag` and `rolling averages`. Finally, in [Chapter 4: Model Experimentation Workbench](04_model_experimentation_workbench_.md), we trained and fine-tuned our machine learning models, finding the best one to predict cinema audience numbers.

Now, we face the ultimate challenge: using our best model to make predictions for *future* dates. This isn't a simple "predict all at once" task. Why? Because many of our best features, like "yesterday's audience" (`audience_lag_1`) or "the average audience over the last 7 days" (`audience_roll_mean_7`), depend on what happened *just before* the day we are predicting.

If we're predicting for March 1st, 2024, we have the actual audience count for February 29th, 2024. But if we want to predict for March 2nd, 2024, we *don't* have the actual audience count for March 1st yet! We only have our *prediction* for March 1st. This means we need a smart way to generate predictions one day at a time, using our *own previous predictions* as part of the "history" for the next day's forecast.

This is precisely what the **Iterative Forecasting Engine** does. It's the core prediction mechanism that allows our system to forecast into the future, day by day, building a chain of predictions.

## What is an Iterative Forecasting Engine?

Imagine you're a seasoned weather forecaster. To predict tomorrow's weather, you look at today's observed weather. But to predict the weather five days from now, you might use your prediction for day one, then your prediction for day two (which used day one's prediction), and so on. You don't just guess all five days at once; you build your forecast step by step.

Our Iterative Forecasting Engine works exactly like that for cinema audience prediction:

*   **Iterative**: It operates in a loop, predicting one day at a time for all theaters.
*   **Forecasting**: It uses our trained machine learning model to make these predictions.
*   **Engine**: It's the dynamic system that keeps track of the "history" – both actual past data and its own predictions – and uses this evolving history to create features for future forecasts.

The key idea is that for each day it forecasts, it *updates its memory* with the new predictions it just made. This updated memory then becomes the "history" for calculating features for the *next* day's predictions.

## How the Iterative Forecasting Engine Works (Day by Day)

Let's walk through how our engine makes predictions for a range of future dates:

1.  **Start with Known History**: Before predicting anything new, the engine first loads all the *actual historical audience data* we have up until the last known date (e.g., February 29, 2024). This is our initial "memory."
2.  **Loop Through Each Future Day**: Starting from the first day we want to predict (e.g., March 1, 2024), the engine enters a loop.
3.  **For Each Day, For Each Theater**:
    *   It looks at the *current date* for all theaters.
    *   It calculates dynamic features (like `audience_lag_1`, `audience_roll_mean_7`) for *each theater on this date*.
    *   **Crucially**: These dynamic features are calculated using the "history" it has. This history includes actual past data, *and any predictions it made for previous days in the current forecasting run*.
4.  **Make Predictions**: It feeds all the calculated features (static, preprocessed, and the newly calculated dynamic features) into our best-trained machine learning model. The model then predicts the `audience_count` for all theaters on the current date.
5.  **Update History**: The predicted `audience_count` for each theater on the current date is immediately added to that theater's "history."
6.  **Move to Next Day**: The loop continues to the next day, now with the updated history that includes our latest predictions.

This process continues until predictions have been made for all desired future dates.

## Building Our Iterative Forecasting Engine: The `making_predictions` Function

We've already done much of the hard work in previous chapters! The `making_predictions` function (from the provided code) encapsulates this entire iterative process. Let's look at its key parts:

### Step 1: Initializing History and Test Data

First, we need to set up our actual historical data and the structure for our future predictions.

```python
import numpy as np
import pandas as pd
import datetime as dt

# Assume `f_t_df` is our integrated, feature-engineered DataFrame (from Chapter 3)
# Assume `df8` is the sample submission template

# This dictionary will hold the historical audience counts for each theater
# It's built from actual past data before forecasting starts.
history_dict = {}
all_theaters = f_t_df['book_theater_id'].unique()

for theater_id in all_theaters:
    theater_data = f_t_df[f_t_df['book_theater_id'] == theater_id]
    theater_data = theater_data.sort_values('show_date')
    audience_list = theater_data['audience_count'].tolist()
    history_dict[theater_id] = audience_list

print(f"Initialized history for {len(history_dict)} theaters.")
# Example output: Initialized history for 829 theaters.
# Example: book_00001 has 423 days of history (actual past data).
```
*Explanation*: We create a `history_dict` where each theater has a list of its `audience_count` values from the *actual past*. This is the starting point of our engine's "memory."

Next, we prepare the empty "skeleton" for the dates we want to predict. This `t_df` will contain all the future dates and theater IDs, along with their static features.

```python
# Define the range of future dates for prediction
start_date_forecast = dt.date(2024, 3, 1)
end_date_forecast = dt.date(2024, 4, 22)
all_forecast_dates = pd.date_range(start=start_date_forecast, end=end_date_forecast)

# Create a DataFrame for all future theater-date combinations
combinations = []
for date in all_forecast_dates:
    for booking_id in all_theaters: # Use all_theaters from history_dict setup
        combinations.append((booking_id, date))

t_df = pd.DataFrame(combinations, columns=["book_theater_id", "show_date"])
t_df["show_date"] = pd.to_datetime(t_df["show_date"])
t_df["ID"] = t_df["book_theater_id"] + "_" + t_df["show_date"].dt.strftime("%Y-%m-%d")

print(f"Prepared future prediction DataFrame with {t_df.shape[0]} rows.")
# Example output: Prepared future prediction DataFrame with 44766 rows.
```
*Explanation*: `t_df` now holds the structure for all the predictions we need to make. This dataframe will be filled with `audience_count` values by our engine. We also make sure static features (like `latitude`, `theater_type`, `show_day`, `dow_sin`, etc., as per [Chapter 3](03_time_series_feature_factory_.md)) are added and preprocessed for `t_df` *before* the iterative loop begins, as these don't change daily.

### Step 2: The Daily Prediction Loop

This is the heart of the Iterative Forecasting Engine. We loop through each day in our prediction period.

```python
# Assume `ohe_encoder`, `static_scaler`, `dynamic_scaler` are fitted preprocessors
# Assume `model_2_lightgbm` is our best-trained model (from Chapter 4)
# Assume `X_test_categorical`, `X_test_static` are pre-transformed features for t_df

def making_predictions_simplified(model, test_dataframe, history_dict, X_test_cat, X_test_stat, dyn_scaler):
    current_history = {tid: hist.copy() for tid, hist in history_dict.items()}
    test_dataframe = test_dataframe.sort_values(['show_date', 'book_theater_id']).reset_index(drop=True)
    all_unique_dates = sorted(test_dataframe['show_date'].unique())
    all_predictions = np.zeros(len(test_dataframe))

    for day_idx, current_date in enumerate(all_unique_dates):
        # 1. Identify rows for the current date
        date_mask = test_dataframe['show_date'] == current_date
        row_indices = np.where(date_mask)[0]
        theaters_today = test_dataframe.loc[row_indices, 'book_theater_id'].values
        num_theaters = len(theaters_today)

        # Prepare to store dynamic features for today
        dynamic_features_today = np.zeros((num_theaters, 12)) # 12 dynamic features

        # 2. Calculate dynamic features for each theater based on `current_history`
        for i, theater_id in enumerate(theaters_today):
            theater_history = current_history.get(theater_id, [])
            history_length = len(theater_history)

            # Example: Calculate audience_lag_1 (yesterday's audience)
            lag_1 = theater_history[-1] if history_length >= 1 else 0
            dynamic_features_today[i, 0] = lag_1 # Store lag_1 in the first column

            # Example: Calculate audience_roll_mean_7 (average last 7 days)
            if history_length >= 1:
                last_7_days = theater_history[-7:]
                roll_mean_7 = np.mean(last_7_days)
            else:
                roll_mean_7 = 0
            dynamic_features_today[i, 6] = roll_mean_7 # Store roll_mean_7

            # ... (similar logic for other 10 lag/rolling features) ...

        # 3. Scale the dynamically calculated features
        X_dynamic_scaled = dyn_scaler.transform(dynamic_features_today)

        # 4. Combine all features for today's prediction
        X_cat_today = X_test_cat[row_indices]
        X_stat_today = X_test_stat[row_indices]
        X_today_combined = np.hstack([X_cat_today, X_stat_today, X_dynamic_scaled])

        # 5. Make predictions for the current day
        predictions_today = model.predict(X_today_combined)
        predictions_today = np.maximum(predictions_today, 0) # Ensure no negative predictions

        # 6. Store predictions
        all_predictions[row_indices] = predictions_today

        # 7. UPDATE `current_history` with today's predictions for the next iteration
        for i, theater_id in enumerate(theaters_today):
            current_history.setdefault(theater_id, []).append(predictions_today[i])

    return all_predictions

print("Iterative forecasting function defined!")
```
*Explanation*:
1.  **`current_history`**: A copy of our initial historical data is made. This `current_history` will be dynamically updated as we make predictions.
2.  **`for day_idx, current_date in enumerate(all_unique_dates):`**: This loop iterates through each day we need to predict.
3.  **`dynamic_features_today = np.zeros(...)`**: For each day, we create a placeholder for the `lag` and `rolling` features.
4.  **`for i, theater_id in enumerate(theaters_today):`**: Inside the daily loop, we iterate through each theater. For *each* theater, we fetch its `theater_history` from `current_history`.
5.  **`lag_1 = theater_history[-1] if history_length >= 1 else 0`**: This line is key! It retrieves "yesterday's" audience count from the `theater_history`. If `current_date` is March 1st, it gets February 29th's *actual* audience. If `current_date` is March 2nd, it gets March 1st's *predicted* audience (because we already added it to `current_history` in the previous daily loop iteration).
6.  **`X_dynamic_scaled = dyn_scaler.transform(dynamic_features_today)`**: Once all dynamic features for *today* are calculated for all theaters, they are scaled using the `dynamic_scaler` (which was fitted on historical `dynamic_columns` in [Chapter 4](04_model_experimentation_workbench_.md)).
7.  **`X_today_combined = np.hstack([...])`**: All features (categorical, static, dynamic) for the current day are combined.
8.  **`predictions_today = model.predict(X_today_combined)`**: Our trained machine learning model makes predictions for all theaters on the `current_date`.
9.  **`current_history.setdefault(theater_id, []).append(predictions_today[i])`**: This is the crucial step for iteration! The `predictions_today` for each theater are added to their respective `current_history` lists. Now, when the loop moves to the *next day*, these predictions will be available as "past" data for calculating `lag` and `rolling` features.

### Step 3: Running the Engine and Generating Submission

Finally, we run our engine with a trained model and convert the predictions into the required submission format.

```python
# Assuming X_train_combined and y_train were used to train the model,
# and ohe_encoder, static_scaler, dynamic_scaler are fitted.

# Our best model from Chapter 4 was LightGBM (LGBMRegressor)
model_2_lightgbm = LGBMRegressor(
    n_estimators=500, max_depth=8, learning_rate=0.01,
    reg_alpha=0.1, reg_lambda=0.5, random_state=42, n_jobs=-1, verbose=-1
)
# We need to re-train the model on the full preprocessed training data X_train_combined, y_train
# (In a real scenario, this would be `model_2_lightgbm.fit(X_train_combined, y_train)`)
# For this tutorial, we will assume it is already fitted to illustrate prediction.

print("MODEL 2: LightGBM Forecasting...")
predictions_model_2 = making_predictions_simplified(
    model=model_2_lightgbm,
    test_dataframe=t_df.copy(), # Make a copy to avoid modifying the original t_df
    history_dictionary=history_dict,
    X_test_cat=X_test_categorical, # Pre-transformed categorical features for t_df
    X_test_stat=X_test_static,     # Pre-transformed static numerical features for t_df
    dyn_scaler=dynamic_scaler      # Fitted scaler for dynamic features
)

# Create a DataFrame for the submission file
submission_2 = pd.DataFrame()
submission_2['ID'] = t_df['ID']
submission_2['audience_count'] = np.round(predictions_model_2).astype(int)

# This function (from Chapter 4's notebook) formats and saves the submission
# def create_submission(df_template, df_predictions, id_col, target_col, out_path): ...
# final_sub = create_submission(df_template=df8, df_predictions=submission_2,
#                               id_col='ID', target_col='audience_count',
#                               out_path='submission.csv')

print("Forecasting complete! Predictions are ready.")
# The `predictions_model_2` array now contains all the daily audience forecasts.
```
*Explanation*: We call our `making_predictions_simplified` function with our trained `model_2_lightgbm`, the `t_df` containing future dates, the initial `history_dict`, and the pre-transformed static/categorical features. The function runs the day-by-day loop, and `predictions_model_2` will contain the final array of forecasted audience counts for every theater on every future date. These are then formatted into a submission file.

## How the Iterative Forecasting Engine Works (Under the Hood)

Let's visualize the dynamic, day-by-day process:

```mermaid
sequenceDiagram
    participant User
    participant Engine as "Iterative Forecasting Engine"
    participant InitialHistory as "Initial History (Actuals)"
    participant DynamicHistory as "Current History (Actuals + Predictions)"
    participant FeatureFactory as "Time-Series Feature Factory"
    participant Model as "Trained ML Model"
    participant AllPredictions as "Accumulated Predictions"

    User->>Engine: "Start forecasting for [D1, ..., Dn]"
    Engine->>InitialHistory: "Load all known actuals"
    InitialHistory-->>DynamicHistory: "Current History"

    loop For each Day (D_current) from D1 to Dn
        Engine->>FeatureFactory: "Request dynamic features for D_current"
        FeatureFactory->>DynamicHistory: "Retrieve relevant past data (lags, rolls)"
        DynamicHistory-->>FeatureFactory: "Past (Actuals / Previous Predictions)"
        FeatureFactory-->>Engine: "Return calculated dynamic features"

        Engine->>Model: "Predict audience for D_current (with static + dynamic features)"
        Model-->>Engine: "Return predictions P_current"

        Engine->>DynamicHistory: "Add P_current to history for next day's use"
        Engine->>AllPredictions: "Store P_current"
    end

    Engine-->>User: "Return final [P1, ..., Pn]"
```

The Iterative Forecasting Engine acts as the central coordinator. It initializes its memory with all available actual historical data. Then, day by day, it asks the `Time-Series Feature Factory` to calculate features using its up-to-date `Current History`. These features, along with static information, are fed into the `Trained ML Model` to get predictions for the current day. Crucially, these new predictions are immediately absorbed back into the `Dynamic History`, making them available for calculating features for the *next* day's forecast. This continuous feedback loop is what makes the forecasting iterative and powerful.

Here’s a summary of the engine's operational flow:

| Step                            | What it does                                                                 | Why it's crucial                                                                     | Key Components Involved                                   |
| :------------------------------ | :--------------------------------------------------------------------------- | :----------------------------------------------------------------------------------- | :-------------------------------------------------------- |
| **Initialize History**          | Loads all actual historical data for each theater.                           | Provides the ground truth for the first few predictions' feature calculations.         | `history_dict`                                            |
| **Prepare Future Data Skeleton** | Creates a DataFrame with all theater-date combinations for future periods.   | Sets up the structure to store predictions and provides static/preprocessed features. | `t_df`, `X_test_categorical`, `X_test_static`             |
| **Iterate Day by Day**          | Loops through each future day, from earliest to latest.                      | Enables dynamic feature calculation where today's prediction informs tomorrow's features.| Main `for` loop in `making_predictions`                  |
| **Calculate Dynamic Features**  | For each theater on the current day, computes lag and rolling statistics.      | Incorporates temporal dependencies and trends using the *latest available* history. | `current_history`, `theater_history`, `dynamic_features_today` |
| **Make Predictions**            | Uses the trained ML model with all processed features.                       | Generates the actual `audience_count` forecast for the current day.                  | `trained_model` (e.g., `model_2_lightgbm`)                |
| **Update History**              | Adds the newly made predictions to each theater's `current_history`.         | Ensures that future forecasts are built on a continuous chain of (actual or predicted) past events. | `current_history.append()`                                |

## Conclusion

In this chapter, you've conquered the **Iterative Forecasting Engine**, the pinnacle of our machine learning project! You've learned how to move beyond simple, one-shot predictions to a sophisticated, day-by-day forecasting mechanism that intelligently updates its own "memory" with previous predictions. This allows our models to correctly calculate dynamic features like `lag` and `rolling statistics` for every future day, producing a continuous and coherent forecast for cinema audience numbers.

You now understand the entire lifecycle of a machine learning project, from raw data to dynamic, iterative predictions. This project has equipped you with the skills to tackle complex time-series forecasting challenges in the real world. Congratulations on completing your Machine Learning Capstone Project!

---






























